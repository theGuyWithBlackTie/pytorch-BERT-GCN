{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3472, 0.4268, 0.2569, 0.5298, 0.6082, 0.8769, 0.6337, 0.6792, 0.1333,\n",
       "         0.0773]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.floor(x).type(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor([False, True, False])\n",
    "b = x._indices()[:,c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.LongTensor([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(i, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1, 2, 3, 4, 5, 6], shape=[3, 2])\n",
    "b = tf.constant([7, 8, 9, 10, 11, 12], shape=[2, 3])\n",
    "c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[ 27,  30,  33],\n",
       "       [ 61,  68,  75],\n",
       "       [ 95, 106, 117]], dtype=int32)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9,), dtype=int32, numpy=array([ 27,  30,  33,  61,  68,  75,  95, 106, 117], dtype=int32)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(c, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nonzero_elems = 5\n",
    "noise_shape = [num_nonzero_elems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tf.random.uniform(noise_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.floor(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = 0.7\n",
    "random_tensor = keep_prob\n",
    "temp = tf.random.uniform(noise_shape)\n",
    "print(temp)\n",
    "random_tensor += temp\n",
    "print(random_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile = pd.read_csv('../data/full_context_PeerRead.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>right_citated_text</th>\n",
       "      <th>left_citated_text</th>\n",
       "      <th>source_abstract</th>\n",
       "      <th>source_author</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_title</th>\n",
       "      <th>source_venue</th>\n",
       "      <th>source_year</th>\n",
       "      <th>target_id</th>\n",
       "      <th>target_author</th>\n",
       "      <th>target_abstract</th>\n",
       "      <th>target_year</th>\n",
       "      <th>target_title</th>\n",
       "      <th>target_venue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>Deep Neural Networks (DNNs) are powerful model...</td>\n",
       "      <td>ilya sutskever;oriol vinyals;quoc v le</td>\n",
       "      <td>1409.3215v1</td>\n",
       "      <td>Sequence to Sequence Learning with Neural Netw...</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>Modeling crisp logical regularities is crucial...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Data Recombination for Neural Semantic Parsing</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>Syntactic parsing is a fundamental problem in ...</td>\n",
       "      <td>oriol vinyals;lukasz kaiser;terry koo;slav pet...</td>\n",
       "      <td>1412.7449v1</td>\n",
       "      <td>Grammar as a Foreign Language</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>Modeling crisp logical regularities is crucial...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Data Recombination for Neural Semantic Parsing</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>Reproducibility. All code, data, and experimen...</td>\n",
       "      <td>We introduce a new neural architecture to lear...</td>\n",
       "      <td>oriol vinyals;meire fortunato;navdeep jaitly</td>\n",
       "      <td>1506.03134v1</td>\n",
       "      <td>Pointer Networks</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>Modeling crisp logical regularities is crucial...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Data Recombination for Neural Semantic Parsing</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>st like CWS and POS tagging, automatic prosody...</td>\n",
       "      <td>The recently introduced continuous Skip-gram m...</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>Distributed Representations of Words and Phras...</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1511.00360v1</td>\n",
       "      <td>chuang ding;lei xie;jie yan;weini zhang;yang liu</td>\n",
       "      <td>Prosody affects the naturalness and intelligib...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Automatic Prosody Prediction for Chinese Speec...</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>We begin by considering a document as the set ...</td>\n",
       "      <td>The recently introduced continuous Skip-gram m...</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>Distributed Representations of Words and Phras...</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>We investigate the pertinence of methods from ...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Does the Geometry of Word Embeddings Help Docu...</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>to create a sentence embedding. Second, from e...</td>\n",
       "      <td>For each word in the sentence we calculate var...</td>\n",
       "      <td>In this paper we compare different types of re...</td>\n",
       "      <td>junyoung chung;caglar gulcehre;kyunghyun cho;y...</td>\n",
       "      <td>1412.3555v1</td>\n",
       "      <td>Empirical Evaluation of Gated Recurrent Neural...</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1708.05582v1</td>\n",
       "      <td>sushant hiray;venkatesh duppada</td>\n",
       "      <td>This paper presents models for detecting agree...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Agree to Disagree: Improving Disagreement Dete...</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>for accelerating training. The network is opti...</td>\n",
       "      <td>For each Q-R pair we extract two sets of featu...</td>\n",
       "      <td>Training Deep Neural Networks is complicated b...</td>\n",
       "      <td>sergey ioffe;christian szegedy</td>\n",
       "      <td>1502.03167v1</td>\n",
       "      <td>Batch Normalization: Accelerating Deep Network...</td>\n",
       "      <td>ICML</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1708.05582v1</td>\n",
       "      <td>sushant hiray;venkatesh duppada</td>\n",
       "      <td>This paper presents models for detecting agree...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Agree to Disagree: Improving Disagreement Dete...</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>optimizer with learning rate of 0.001. We have...</td>\n",
       "      <td>For each Q-R pair we extract two sets of featu...</td>\n",
       "      <td>We introduce Adam, an algorithm for first-orde...</td>\n",
       "      <td>diederik p kingma;jimmy ba</td>\n",
       "      <td>1412.6980v1</td>\n",
       "      <td>Adam: A Method for Stochastic Optimization</td>\n",
       "      <td>iclr</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1708.05582v1</td>\n",
       "      <td>sushant hiray;venkatesh duppada</td>\n",
       "      <td>This paper presents models for detecting agree...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Agree to Disagree: Improving Disagreement Dete...</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>, and convolutional neural networks  have show...</td>\n",
       "      <td>Modeling textual or visual information with ve...</td>\n",
       "      <td>Deep Neural Networks (DNNs) are powerful model...</td>\n",
       "      <td>ilya sutskever;oriol vinyals;quoc v le</td>\n",
       "      <td>1409.3215v1</td>\n",
       "      <td>Sequence to Sequence Learning with Neural Netw...</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1606.01847v1</td>\n",
       "      <td>akira fukui;dong huk park;daylen yang;anna roh...</td>\n",
       "      <td>Modeling textual or visual information with ve...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Multimodal Compact Bilinear Pooling for Visual...</td>\n",
       "      <td>EMNLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>use concatenation and fully connected layers t...</td>\n",
       "      <td>In this paper, we propose to rely on ltimodal ...</td>\n",
       "      <td>We describe a very simple bag-of-words baselin...</td>\n",
       "      <td>bolei zhou;yuandong tian;sainbayar sukhbaatar;...</td>\n",
       "      <td>1512.02167v1</td>\n",
       "      <td>Simple Baseline for Visual Question Answering</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1606.01847v1</td>\n",
       "      <td>akira fukui;dong huk park;daylen yang;anna roh...</td>\n",
       "      <td>Modeling textual or visual information with ve...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Multimodal Compact Bilinear Pooling for Visual...</td>\n",
       "      <td>EMNLP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   right_citated_text  \\\n",
       "0   andsyntactic parsing .Because RNNs make very f...   \n",
       "1   .Because RNNs make very few domain-specific as...   \n",
       "2   ; in a Pointer Network,the only way to generat...   \n",
       "3   . Recently, nsur .  have shown superior perfor...   \n",
       "4   model trained on the Google News dataset3.In a...   \n",
       "..                                                ...   \n",
       "95  to create a sentence embedding. Second, from e...   \n",
       "96  for accelerating training. The network is opti...   \n",
       "97  optimizer with learning rate of 0.001. We have...   \n",
       "98  , and convolutional neural networks  have show...   \n",
       "99  use concatenation and fully connected layers t...   \n",
       "\n",
       "                                    left_citated_text  \\\n",
       "0   We conducted additional experiments on artific...   \n",
       "1   We conducted additional experiments on artific...   \n",
       "2   Reproducibility. All code, data, and experimen...   \n",
       "3   st like CWS and POS tagging, automatic prosody...   \n",
       "4   We begin by considering a document as the set ...   \n",
       "..                                                ...   \n",
       "95  For each word in the sentence we calculate var...   \n",
       "96  For each Q-R pair we extract two sets of featu...   \n",
       "97  For each Q-R pair we extract two sets of featu...   \n",
       "98  Modeling textual or visual information with ve...   \n",
       "99  In this paper, we propose to rely on ltimodal ...   \n",
       "\n",
       "                                      source_abstract  \\\n",
       "0   Deep Neural Networks (DNNs) are powerful model...   \n",
       "1   Syntactic parsing is a fundamental problem in ...   \n",
       "2   We introduce a new neural architecture to lear...   \n",
       "3   The recently introduced continuous Skip-gram m...   \n",
       "4   The recently introduced continuous Skip-gram m...   \n",
       "..                                                ...   \n",
       "95  In this paper we compare different types of re...   \n",
       "96  Training Deep Neural Networks is complicated b...   \n",
       "97  We introduce Adam, an algorithm for first-orde...   \n",
       "98  Deep Neural Networks (DNNs) are powerful model...   \n",
       "99  We describe a very simple bag-of-words baselin...   \n",
       "\n",
       "                                        source_author     source_id  \\\n",
       "0              ilya sutskever;oriol vinyals;quoc v le   1409.3215v1   \n",
       "1   oriol vinyals;lukasz kaiser;terry koo;slav pet...   1412.7449v1   \n",
       "2        oriol vinyals;meire fortunato;navdeep jaitly  1506.03134v1   \n",
       "3   tomas mikolov;ilya sutskever;kai chen 0010;gre...   1310.4546v1   \n",
       "4   tomas mikolov;ilya sutskever;kai chen 0010;gre...   1310.4546v1   \n",
       "..                                                ...           ...   \n",
       "95  junyoung chung;caglar gulcehre;kyunghyun cho;y...   1412.3555v1   \n",
       "96                     sergey ioffe;christian szegedy  1502.03167v1   \n",
       "97                         diederik p kingma;jimmy ba   1412.6980v1   \n",
       "98             ilya sutskever;oriol vinyals;quoc v le   1409.3215v1   \n",
       "99  bolei zhou;yuandong tian;sainbayar sukhbaatar;...  1512.02167v1   \n",
       "\n",
       "                                         source_title source_venue  \\\n",
       "0   Sequence to Sequence Learning with Neural Netw...         NIPS   \n",
       "1                       Grammar as a Foreign Language         NIPS   \n",
       "2                                    Pointer Networks         NIPS   \n",
       "3   Distributed Representations of Words and Phras...         NIPS   \n",
       "4   Distributed Representations of Words and Phras...         NIPS   \n",
       "..                                                ...          ...   \n",
       "95  Empirical Evaluation of Gated Recurrent Neural...        arxiv   \n",
       "96  Batch Normalization: Accelerating Deep Network...         ICML   \n",
       "97         Adam: A Method for Stochastic Optimization         iclr   \n",
       "98  Sequence to Sequence Learning with Neural Netw...         NIPS   \n",
       "99      Simple Baseline for Visual Question Answering        arxiv   \n",
       "\n",
       "    source_year     target_id  \\\n",
       "0        2014.0  1606.03622v1   \n",
       "1        2014.0  1606.03622v1   \n",
       "2        2015.0  1606.03622v1   \n",
       "3        2013.0  1511.00360v1   \n",
       "4        2013.0  1705.10900v1   \n",
       "..          ...           ...   \n",
       "95       2014.0  1708.05582v1   \n",
       "96       2015.0  1708.05582v1   \n",
       "97       2014.0  1708.05582v1   \n",
       "98       2014.0  1606.01847v1   \n",
       "99       2015.0  1606.01847v1   \n",
       "\n",
       "                                        target_author  \\\n",
       "0                               robin jia;percy liang   \n",
       "1                               robin jia;percy liang   \n",
       "2                               robin jia;percy liang   \n",
       "3    chuang ding;lei xie;jie yan;weini zhang;yang liu   \n",
       "4   paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "..                                                ...   \n",
       "95                    sushant hiray;venkatesh duppada   \n",
       "96                    sushant hiray;venkatesh duppada   \n",
       "97                    sushant hiray;venkatesh duppada   \n",
       "98  akira fukui;dong huk park;daylen yang;anna roh...   \n",
       "99  akira fukui;dong huk park;daylen yang;anna roh...   \n",
       "\n",
       "                                      target_abstract  target_year  \\\n",
       "0   Modeling crisp logical regularities is crucial...         2016   \n",
       "1   Modeling crisp logical regularities is crucial...         2016   \n",
       "2   Modeling crisp logical regularities is crucial...         2016   \n",
       "3   Prosody affects the naturalness and intelligib...         2015   \n",
       "4   We investigate the pertinence of methods from ...         2017   \n",
       "..                                                ...          ...   \n",
       "95  This paper presents models for detecting agree...         2017   \n",
       "96  This paper presents models for detecting agree...         2017   \n",
       "97  This paper presents models for detecting agree...         2017   \n",
       "98  Modeling textual or visual information with ve...         2016   \n",
       "99  Modeling textual or visual information with ve...         2016   \n",
       "\n",
       "                                         target_title target_venue  \n",
       "0      Data Recombination for Neural Semantic Parsing          ACL  \n",
       "1      Data Recombination for Neural Semantic Parsing          ACL  \n",
       "2      Data Recombination for Neural Semantic Parsing          ACL  \n",
       "3   Automatic Prosody Prediction for Chinese Speec...        arxiv  \n",
       "4   Does the Geometry of Word Embeddings Help Docu...        arxiv  \n",
       "..                                                ...          ...  \n",
       "95  Agree to Disagree: Improving Disagreement Dete...        arxiv  \n",
       "96  Agree to Disagree: Improving Disagreement Dete...        arxiv  \n",
       "97  Agree to Disagree: Improving Disagreement Dete...        arxiv  \n",
       "98  Multimodal Compact Bilinear Pooling for Visual...        EMNLP  \n",
       "99  Multimodal Compact Bilinear Pooling for Visual...        EMNLP  \n",
       "\n",
       "[100 rows x 14 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFile.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['left_citated_text', 'right_citated_text', 'target_id', 'source_id', 'target_year','target_author', 'source_author']\n",
    "frequency = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataFile[column]\n",
    "source_cut_data = df[['target_id', 'source_id']].drop_duplicates(subset=['target_id', 'source_id'])\n",
    "source_cut = source_cut_data.source_id.value_counts()[(source_cut_data.source_id.value_counts() >= frequency)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id = np.sort(source_cut.keys())\n",
    "source_id\n",
    "df = df.loc[df['source_id'].isin(source_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0705.4485v1', '0806.4686v1', '0812.4952v1', '0902.1284v1',\n",
       "       '0902.2206v1', '0907.1815v1', '0911.5708v1', '0912.0071v1',\n",
       "       '0912.3995v1', '1006.2588v1', '1010.3091v1', '1010.5511v1',\n",
       "       '1103.0398v1', '1104.4803v1', '1105.1033v1', '1105.5379v1',\n",
       "       '1106.2436v1', '1106.4574v1', '1107.4080v1', '1107.4557v1',\n",
       "       '1109.3701v1', '1109.3843v1', '1109.5647v1', '1109.6841v1',\n",
       "       '1112.6209v1', '1201.0292v1', '1201.0490v1', '1202.6258v1',\n",
       "       '1204.0136v1', '1204.2742v1', '1204.3514v1', '1204.3968v1',\n",
       "       '1204.6703v1', '1205.2661v1', '1206.1106v1', '1206.1270v1',\n",
       "       '1206.2944v1', '1206.3255v1', '1206.4657v1', '1206.4683v1',\n",
       "       '1206.6230v1', '1206.6380v1', '1206.6389v1', '1206.6392v1',\n",
       "       '1206.6398v1', '1206.6400v1', '1206.6417v1', '1206.6418v1',\n",
       "       '1206.6423v1', '1206.6426v1', '1206.6430v1', '1206.6448v1',\n",
       "       '1206.6487v1', '1207.4404v1', '1207.4747v1', '1209.3352v1',\n",
       "       '1210.5644v1', '1211.3966v1', '1211.5063v1', '1211.5590v1',\n",
       "       '1211.7012v1', '1212.0901v1', '1212.1824v1', '1212.4777v1',\n",
       "       '1212.5701v1', '1301.3224v1', '1301.3485v1', '1301.3584v1',\n",
       "       '1301.3666v1', '1301.3781v1', '1302.0723v1', '1302.4389v1',\n",
       "       '1305.0445v1', '1305.2982v1', '1305.3120v1', '1305.6663v1',\n",
       "       '1306.0160v1', '1306.0186v1', '1306.0543v1', '1306.0940v1',\n",
       "       '1306.1091v1', '1306.2119v1', '1306.3888v1', '1307.0032v1',\n",
       "       '1307.1493v1', '1307.1662v1', '1307.5101v1', '1307.7973v1',\n",
       "       '1308.0850v1', '1309.2375v1', '1310.4546v1', '1310.6343v1',\n",
       "       '1310.8499v1', '1311.1869v1', '1311.2495v1', '1311.4296v1',\n",
       "       '1312.3005v1', '1312.3393v1', '1312.4400v1', '1312.5602v1',\n",
       "       '1312.5851v1', '1312.6173v1', '1312.6184v1', '1312.6199v1',\n",
       "       '1312.6203v1', '1401.0514v1', '1401.3492v1', '1401.4082v1',\n",
       "       '1401.5390v1', '1402.0030v1', '1402.0119v1', '1402.0555v1',\n",
       "       '1402.1454v1', '1402.1869v1', '1402.3511v1', '1402.4102v1',\n",
       "       '1404.0736v1', '1404.2188v1', '1404.4641v1', '1405.3162v1',\n",
       "       '1405.4053v1', '1405.4273v1', '1405.5869v1', '1406.1078v1',\n",
       "       '1406.1822v1', '1406.2541v1', '1406.2572v1', '1406.2751v1',\n",
       "       '1406.3332v1', '1406.3676v1', '1406.5298v1', '1406.5679v1',\n",
       "       '1406.6247v1', '1407.0202v1', '1407.3068v1', '1408.5093v1',\n",
       "       '1408.5882v1', '1409.1458v1', '1409.2848v1', '1409.3215v1',\n",
       "       '1409.7495v1', '1410.0210v1', '1410.0759v1', '1410.1090v1',\n",
       "       '1410.1141v1', '1410.2455v1', '1410.8516v1', '1411.1147v1',\n",
       "       '1411.1792v1', '1411.4166v1', '1411.5654v1', '1411.6081v1',\n",
       "       '1412.0233v1', '1412.1058v1', '1412.1632v1', '1412.2007v1',\n",
       "       '1412.3555v1', '1412.4729v1', '1412.5068v1', '1412.5567v1',\n",
       "       '1412.6115v1', '1412.6550v1', '1412.6553v1', '1412.6564v1',\n",
       "       '1412.6568v1', '1412.6575v1', '1412.6583v1', '1412.6604v1',\n",
       "       '1412.6632v1', '1412.6651v1', '1412.6806v1', '1412.6980v1',\n",
       "       '1412.7024v1', '1412.7062v1', '1412.7449v1', '1412.7580v1',\n",
       "       '1412.7753v1', '1412.7755v1', '1501.02598v1', '1501.03796v1',\n",
       "       '1502.02367v1', '1502.02551v1', '1502.02761v1', '1502.02791v1',\n",
       "       '1502.03044v1', '1502.03167v1', '1502.03492v1', '1502.03508v1',\n",
       "       '1502.04390v1', '1502.04623v1', '1502.04681v1', '1502.05477v1',\n",
       "       '1503.00075v1', '1503.00185v1', '1503.01007v1', '1503.01070v1',\n",
       "       '1503.01838v1', '1503.02364v1', '1503.03167v1', '1503.03244v1',\n",
       "       '1503.03535v1', '1503.04069v1', '1503.04269v1', '1503.05671v1',\n",
       "       '1504.00548v1', '1504.04788v1', '1504.06580v1', '1504.06654v1',\n",
       "       '1505.00387v1', '1505.01809v1', '1505.02074v1', '1505.05008v1',\n",
       "       '1505.05612v1', '1505.05770v1', '1505.05899v1', '1505.08075v1',\n",
       "       '1506.00019v1', '1506.00333v1', '1506.01057v1', '1506.01066v1',\n",
       "       '1506.01070v1', '1506.01094v1', '1506.01900v1', '1506.02075v1',\n",
       "       '1506.02078v1', '1506.02142v1', '1506.02216v1', '1506.02438v1',\n",
       "       '1506.02516v1', '1506.02617v1', '1506.02626v1', '1506.03099v1',\n",
       "       '1506.03134v1', '1506.03340v1', '1506.03487v1', '1506.04089v1',\n",
       "       '1506.05254v1', '1506.05865v1', '1506.05869v1', '1506.06158v1',\n",
       "       '1506.06579v1', '1506.06714v1', '1506.06726v1', '1506.06863v1',\n",
       "       '1506.07190v1', '1506.07285v1', '1506.07503v1', '1506.07512v1',\n",
       "       '1506.07650v1', '1506.08909v1', '1506.08941v1', '1507.00210v1',\n",
       "       '1507.01127v1', '1507.01526v1', '1507.01839v1', '1507.02672v1',\n",
       "       '1507.03641v1', '1507.04808v1', '1507.08750v1', '1508.00305v1',\n",
       "       '1508.00657v1', '1508.01745v1', '1508.02096v1', '1508.03720v1',\n",
       "       '1508.04025v1', '1508.04112v1', '1508.05326v1', '1508.05508v1',\n",
       "       '1508.06615v1', '1508.07909v1', '1509.00685v1', '1509.00838v1',\n",
       "       '1509.01240v1', '1509.01626v1', '1509.02208v1', '1509.04219v1',\n",
       "       '1509.06461v1', '1509.06569v1', '1509.06664v1', '1509.06812v1',\n",
       "       '1509.08062v1', '1509.08967v1', '1509.09292v1', '1510.00726v1',\n",
       "       '1510.01722v1', '1510.03055v1', '1510.04935v1', '1510.09142v1',\n",
       "       '1511.00363v1', '1511.00561v1', '1511.01432v1', '1511.03677v1',\n",
       "       '1511.03729v1', '1511.04108v1', '1511.05644v1', '1511.05952v1',\n",
       "       '1511.06279v1', '1511.06295v1', '1511.06335v1', '1511.06342v1',\n",
       "       '1511.06350v1', '1511.06392v1', '1511.06422v1', '1511.06434v1',\n",
       "       '1511.06464v1', '1511.06530v1', '1511.06581v1', '1511.06709v1',\n",
       "       '1511.06732v1', '1511.06931v1', '1511.07289v1', '1511.07401v1',\n",
       "       '1511.08130v1', '1511.08198v1', '1511.08228v1', '1511.08308v1',\n",
       "       '1512.00103v1', '1512.01274v1', '1512.02167v1', '1512.02393v1',\n",
       "       '1512.02433v1', '1512.02595v1', '1512.05193v1', '1512.08849v1',\n",
       "       '1512.09300v1', '1601.00770v1', '1601.01073v1', '1601.01085v1',\n",
       "       '1601.01272v1', '1601.01280v1', '1601.01705v1', '1601.03896v1',\n",
       "       '1601.04811v1', '1601.06733v1', '1601.06759v1', '1602.00367v1',\n",
       "       '1602.01783v1', '1602.01925v1', '1602.02410v1', '1602.02644v1',\n",
       "       '1602.02830v1', '1602.02867v1', '1602.03609v1', '1602.04621v1',\n",
       "       '1602.06023v1', '1602.07332v1', '1602.07776v1', '1602.07868v1',\n",
       "       '1603.00391v1', '1603.00448v1', '1603.00748v1', '1603.00810v1',\n",
       "       '1603.01312v1', '1603.01354v1', '1603.01360v1', '1603.01417v1',\n",
       "       '1603.01547v1', '1603.02199v1', '1603.04351v1', '1603.04467v1',\n",
       "       '1603.05106v1', '1603.05643v1', '1603.06021v1', '1603.06042v1',\n",
       "       '1603.06059v1', '1603.06075v1', '1603.06147v1', '1603.06155v1',\n",
       "       '1603.06160v1', '1603.06270v1', '1603.06393v1', '1603.06598v1',\n",
       "       '1603.06744v1', '1603.07252v1', '1603.07954v1', '1603.08023v1',\n",
       "       '1603.08575v1', '1603.09025v1', '1604.00788v1', '1604.01485v1',\n",
       "       '1604.02201v1', '1604.03640v1', '1604.03968v1', '1604.06045v1',\n",
       "       '1604.06778v1', '1605.02097v1', '1605.02276v1', '1605.02688v1',\n",
       "       '1605.03209v1', '1605.03481v1', '1605.03705v1', '1605.04238v1',\n",
       "       '1605.04569v1', '1605.05273v1', '1605.06069v1', '1605.06676v1',\n",
       "       '1605.07110v1', '1605.07146v1', '1605.07272v1', '1605.07277v1',\n",
       "       '1605.07683v1', '1605.07736v1', '1605.09128v1', '1605.09186v1',\n",
       "       '1605.09304v1', '1606.00061v1', '1606.00709v1', '1606.00776v1',\n",
       "       '1606.01305v1', '1606.01540v1', '1606.01541v1', '1606.01549v1',\n",
       "       '1606.01847v1', '1606.01868v1', '1606.01933v1', '1606.02006v1',\n",
       "       '1606.02245v1', '1606.02270v1', '1606.02447v1', '1606.02492v1',\n",
       "       '1606.02689v1', '1606.02858v1', '1606.02891v1', '1606.02892v1',\n",
       "       '1606.02960v1', '1606.03126v1', '1606.03498v1', '1606.03622v1',\n",
       "       '1606.03657v1', '1606.04080v1', '1606.04155v1', '1606.04164v1',\n",
       "       '1606.04596v1', '1606.04640v1', '1606.04671v1', '1606.04695v1',\n",
       "       '1606.05250v1', '1606.05328v1', '1606.06031v1', '1606.06160v1',\n",
       "       '1606.06357v1', '1606.06565v1', '1606.07356v1', '1606.07419v1',\n",
       "       '1606.07947v1', '1607.04423v1', '1607.07086v1', '1608.04428v1',\n",
       "       '1608.04631v1', '1608.06993v1', '1608.07905v1', '1609.00150v1',\n",
       "       '1609.01704v1', '1609.03145v1', '1609.03499v1', '1609.04243v1',\n",
       "       '1609.05140v1', '1609.05473v1', '1609.05518v1', '1609.06773v1',\n",
       "       '1609.07061v1', '1609.07843v1', '1609.08667v1', '1610.01108v1',\n",
       "       '1610.02413v1', '1610.03017v1', '1610.04286v1', '1610.05256v1',\n",
       "       '1610.09038v1', '1610.09996v1', '1610.10099v1', '1611.00020v1',\n",
       "       '1611.00179v1', '1611.01587v1', '1611.01603v1', '1611.01604v1',\n",
       "       '1611.01874v1', '1611.03530v1', '1611.08669v1', '1611.09268v1',\n",
       "       '1611.09830v1', '1612.00837v1', '1612.03969v1', '1701.02810v1',\n",
       "       '1701.06547v1', '1701.08734v1', '1702.05800v1', '1705.03122v1',\n",
       "       '1708.06131v1'], dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_citated_text</th>\n",
       "      <th>right_citated_text</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>target_year</th>\n",
       "      <th>target_author</th>\n",
       "      <th>source_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1409.3215v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>ilya sutskever;oriol vinyals;quoc v le</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1412.7449v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;lukasz kaiser;terry koo;slav pet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reproducibility. All code, data, and experimen...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1506.03134v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;meire fortunato;navdeep jaitly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>st like CWS and POS tagging, automatic prosody...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>1511.00360v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>chuang ding;lei xie;jie yan;weini zhang;yang liu</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We begin by considering a document as the set ...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16664</th>\n",
       "      <td>The final  argument tells LLAMA whether low pe...</td>\n",
       "      <td>. For each instance, 36 features weremeasured....</td>\n",
       "      <td>1306.1031v1</td>\n",
       "      <td>1306.5606v1</td>\n",
       "      <td>2013</td>\n",
       "      <td>lars kotthoff</td>\n",
       "      <td>barry hurley;lars kotthoff;yuri malitsky;barry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16665</th>\n",
       "      <td>This approach works reasonably well, but does ...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "      <td>1610.03946v1</td>\n",
       "      <td>1301.3781v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>jessica ficler;yoav goldberg</td>\n",
       "      <td>tomas mikolov;kai chen;greg corrado;jeffrey dean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16666</th>\n",
       "      <td>In addition to the symmetry and replacement si...</td>\n",
       "      <td>and nia Treebank7.When evaluating on the PTB, ...</td>\n",
       "      <td>1610.03946v1</td>\n",
       "      <td>1606.02529v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>jessica ficler;yoav goldberg</td>\n",
       "      <td>jessica ficler;yoav goldberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16667</th>\n",
       "      <td>Fig. 10 shows histograms of the execution time...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1603.02199v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>sergey levine;peter pastor;alex krizhevsky;dei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>As we can see from Eq. , the target of the lea...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1312.5602v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>volodymyr mnih;koray kavukcuoglu;david silver;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16669 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       left_citated_text  \\\n",
       "0      We conducted additional experiments on artific...   \n",
       "1      We conducted additional experiments on artific...   \n",
       "2      Reproducibility. All code, data, and experimen...   \n",
       "3      st like CWS and POS tagging, automatic prosody...   \n",
       "4      We begin by considering a document as the set ...   \n",
       "...                                                  ...   \n",
       "16664  The final  argument tells LLAMA whether low pe...   \n",
       "16665  This approach works reasonably well, but does ...   \n",
       "16666  In addition to the symmetry and replacement si...   \n",
       "16667  Fig. 10 shows histograms of the execution time...   \n",
       "16668  As we can see from Eq. , the target of the lea...   \n",
       "\n",
       "                                      right_citated_text     target_id  \\\n",
       "0      andsyntactic parsing .Because RNNs make very f...  1606.03622v1   \n",
       "1      .Because RNNs make very few domain-specific as...  1606.03622v1   \n",
       "2      ; in a Pointer Network,the only way to generat...  1606.03622v1   \n",
       "3      . Recently, nsur .  have shown superior perfor...  1511.00360v1   \n",
       "4      model trained on the Google News dataset3.In a...  1705.10900v1   \n",
       "...                                                  ...           ...   \n",
       "16664  . For each instance, 36 features weremeasured....   1306.1031v1   \n",
       "16665  on the POS sequences in PTB trainingset.The re...  1610.03946v1   \n",
       "16666  and nia Treebank7.When evaluating on the PTB, ...  1610.03946v1   \n",
       "16667  , but none of these methods can be applied dir...  1708.04033v1   \n",
       "16668  , we use multiple long short-term memory  laye...  1708.04033v1   \n",
       "\n",
       "          source_id  target_year  \\\n",
       "0       1409.3215v1         2016   \n",
       "1       1412.7449v1         2016   \n",
       "2      1506.03134v1         2016   \n",
       "3       1310.4546v1         2015   \n",
       "4       1310.4546v1         2017   \n",
       "...             ...          ...   \n",
       "16664   1306.5606v1         2013   \n",
       "16665   1301.3781v1         2016   \n",
       "16666  1606.02529v1         2016   \n",
       "16667  1603.02199v1         2017   \n",
       "16668   1312.5602v1         2017   \n",
       "\n",
       "                                           target_author  \\\n",
       "0                                  robin jia;percy liang   \n",
       "1                                  robin jia;percy liang   \n",
       "2                                  robin jia;percy liang   \n",
       "3       chuang ding;lei xie;jie yan;weini zhang;yang liu   \n",
       "4      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "...                                                  ...   \n",
       "16664                                      lars kotthoff   \n",
       "16665                       jessica ficler;yoav goldberg   \n",
       "16666                       jessica ficler;yoav goldberg   \n",
       "16667  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "16668  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "\n",
       "                                           source_author  \n",
       "0                 ilya sutskever;oriol vinyals;quoc v le  \n",
       "1      oriol vinyals;lukasz kaiser;terry koo;slav pet...  \n",
       "2           oriol vinyals;meire fortunato;navdeep jaitly  \n",
       "3      tomas mikolov;ilya sutskever;kai chen 0010;gre...  \n",
       "4      tomas mikolov;ilya sutskever;kai chen 0010;gre...  \n",
       "...                                                  ...  \n",
       "16664  barry hurley;lars kotthoff;yuri malitsky;barry...  \n",
       "16665   tomas mikolov;kai chen;greg corrado;jeffrey dean  \n",
       "16666                       jessica ficler;yoav goldberg  \n",
       "16667  sergey levine;peter pastor;alex krizhevsky;dei...  \n",
       "16668  volodymyr mnih;koray kavukcuoglu;david silver;...  \n",
       "\n",
       "[16669 rows x 7 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_citated_text</th>\n",
       "      <th>right_citated_text</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>target_year</th>\n",
       "      <th>target_author</th>\n",
       "      <th>source_author</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1409.3215v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>ilya sutskever;oriol vinyals;quoc v le</td>\n",
       "      <td>le, recurrent neural networks  have made swift...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1412.7449v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;lukasz kaiser;terry koo;slav pet...</td>\n",
       "      <td>networks  have made swift inroads intomany str...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reproducibility. All code, data, and experimen...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1506.03134v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;meire fortunato;navdeep jaitly</td>\n",
       "      <td>n-based copying can be seen as acombination of...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>st like CWS and POS tagging, automatic prosody...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>1511.00360v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>chuang ding;lei xie;jie yan;weini zhang;yang liu</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>sing neural networks from raw text in a fully ...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We begin by considering a document as the set ...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>s their usefulness for real-world tasks.As a f...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16662</th>\n",
       "      <td>With human annotation of data, significant int...</td>\n",
       "      <td>proposed a yesian EM framework for continuous-...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>1512.02393v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>tingting zhu;nic dunkley;joachim behar;david a...</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "      <td>ion of each expert annotator and the underlyin...</td>\n",
       "      <td>proposed a yesian EM framework for continuous-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16663</th>\n",
       "      <td>An effective probabilistic approach to aggrega...</td>\n",
       "      <td>. as is defined as the inverse of accuracy: It...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>1512.02393v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>tingting zhu;nic dunkley;joachim behar;david a...</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "      <td>reduce annotator inter- and intra-variability....</td>\n",
       "      <td>. as is defined as the inverse of accuracy: It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16665</th>\n",
       "      <td>This approach works reasonably well, but does ...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "      <td>1610.03946v1</td>\n",
       "      <td>1301.3781v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>jessica ficler;yoav goldberg</td>\n",
       "      <td>tomas mikolov;kai chen;greg corrado;jeffrey dean</td>\n",
       "      <td>ell in the CKY chart.3In both approaches,the P...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16667</th>\n",
       "      <td>Fig. 10 shows histograms of the execution time...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1603.02199v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>sergey levine;peter pastor;alex krizhevsky;dei...</td>\n",
       "      <td>l concept is shown in Fig. 1.Recent studies ha...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>As we can see from Eq. , the target of the lea...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1312.5602v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>volodymyr mnih;koray kavukcuoglu;david silver;...</td>\n",
       "      <td>work decision.Algorithm 2 shows the learning t...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12230 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       left_citated_text  \\\n",
       "0      We conducted additional experiments on artific...   \n",
       "1      We conducted additional experiments on artific...   \n",
       "2      Reproducibility. All code, data, and experimen...   \n",
       "3      st like CWS and POS tagging, automatic prosody...   \n",
       "4      We begin by considering a document as the set ...   \n",
       "...                                                  ...   \n",
       "16662  With human annotation of data, significant int...   \n",
       "16663  An effective probabilistic approach to aggrega...   \n",
       "16665  This approach works reasonably well, but does ...   \n",
       "16667  Fig. 10 shows histograms of the execution time...   \n",
       "16668  As we can see from Eq. , the target of the lea...   \n",
       "\n",
       "                                      right_citated_text     target_id  \\\n",
       "0      andsyntactic parsing .Because RNNs make very f...  1606.03622v1   \n",
       "1      .Because RNNs make very few domain-specific as...  1606.03622v1   \n",
       "2      ; in a Pointer Network,the only way to generat...  1606.03622v1   \n",
       "3      . Recently, nsur .  have shown superior perfor...  1511.00360v1   \n",
       "4      model trained on the Google News dataset3.In a...  1705.10900v1   \n",
       "...                                                  ...           ...   \n",
       "16662  proposed a yesian EM framework for continuous-...  1503.06619v1   \n",
       "16663  . as is defined as the inverse of accuracy: It...  1503.06619v1   \n",
       "16665  on the POS sequences in PTB trainingset.The re...  1610.03946v1   \n",
       "16667  , but none of these methods can be applied dir...  1708.04033v1   \n",
       "16668  , we use multiple long short-term memory  laye...  1708.04033v1   \n",
       "\n",
       "          source_id  target_year  \\\n",
       "0       1409.3215v1         2016   \n",
       "1       1412.7449v1         2016   \n",
       "2      1506.03134v1         2016   \n",
       "3       1310.4546v1         2015   \n",
       "4       1310.4546v1         2017   \n",
       "...             ...          ...   \n",
       "16662  1512.02393v1         2015   \n",
       "16663  1512.02393v1         2015   \n",
       "16665   1301.3781v1         2016   \n",
       "16667  1603.02199v1         2017   \n",
       "16668   1312.5602v1         2017   \n",
       "\n",
       "                                           target_author  \\\n",
       "0                                  robin jia;percy liang   \n",
       "1                                  robin jia;percy liang   \n",
       "2                                  robin jia;percy liang   \n",
       "3       chuang ding;lei xie;jie yan;weini zhang;yang liu   \n",
       "4      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "...                                                  ...   \n",
       "16662  tingting zhu;nic dunkley;joachim behar;david a...   \n",
       "16663  tingting zhu;nic dunkley;joachim behar;david a...   \n",
       "16665                       jessica ficler;yoav goldberg   \n",
       "16667  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "16668  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "\n",
       "                                           source_author  \\\n",
       "0                 ilya sutskever;oriol vinyals;quoc v le   \n",
       "1      oriol vinyals;lukasz kaiser;terry koo;slav pet...   \n",
       "2           oriol vinyals;meire fortunato;navdeep jaitly   \n",
       "3      tomas mikolov;ilya sutskever;kai chen 0010;gre...   \n",
       "4      tomas mikolov;ilya sutskever;kai chen 0010;gre...   \n",
       "...                                                  ...   \n",
       "16662                  changbo zhu;huan xu;shuicheng yan   \n",
       "16663                  changbo zhu;huan xu;shuicheng yan   \n",
       "16665   tomas mikolov;kai chen;greg corrado;jeffrey dean   \n",
       "16667  sergey levine;peter pastor;alex krizhevsky;dei...   \n",
       "16668  volodymyr mnih;koray kavukcuoglu;david silver;...   \n",
       "\n",
       "                                               #1 String  \\\n",
       "0      le, recurrent neural networks  have made swift...   \n",
       "1      networks  have made swift inroads intomany str...   \n",
       "2      n-based copying can be seen as acombination of...   \n",
       "3      sing neural networks from raw text in a fully ...   \n",
       "4      s their usefulness for real-world tasks.As a f...   \n",
       "...                                                  ...   \n",
       "16662  ion of each expert annotator and the underlyin...   \n",
       "16663  reduce annotator inter- and intra-variability....   \n",
       "16665  ell in the CKY chart.3In both approaches,the P...   \n",
       "16667  l concept is shown in Fig. 1.Recent studies ha...   \n",
       "16668  work decision.Algorithm 2 shows the learning t...   \n",
       "\n",
       "                                               #2 String  \n",
       "0      andsyntactic parsing .Because RNNs make very f...  \n",
       "1      .Because RNNs make very few domain-specific as...  \n",
       "2      ; in a Pointer Network,the only way to generat...  \n",
       "3      . Recently, nsur .  have shown superior perfor...  \n",
       "4      model trained on the Google News dataset3.In a...  \n",
       "...                                                  ...  \n",
       "16662  proposed a yesian EM framework for continuous-...  \n",
       "16663  . as is defined as the inverse of accuracy: It...  \n",
       "16665  on the POS sequences in PTB trainingset.The re...  \n",
       "16667  , but none of these methods can be applied dir...  \n",
       "16668  , we use multiple long short-term memory  laye...  \n",
       "\n",
       "[12230 rows x 9 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['#1 String'] = df['left_citated_text'].str[-128:]\n",
    "df['#2 String'] = df['right_citated_text'].str[:128]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ameters for the text representation model. We take Jsemi as the loss function, and train neural networks with the Adam algorithm': 0,\n",
       " 'ck-box attacks without a substitute model , based on the principle that adversarial examples can transfer among different models': 1,\n",
       " 'vel models .Outside of language modeling, improvements have been reported on part-of-speech tagging  andnamed entity recognition': 2,\n",
       " ' models such as Deep Convolutional GAN  , undary Equilibrium GAN  , and the combination of DCGAN with a Variational Autoencoder ': 3,\n",
       " 'by leveraging semantic denotations during structural search.Third, to train from weak supervision and directly maximize the expe': 4,\n",
       " 'introduced a simplified version of LSTMs called Gated Recurrent Units  with has one less gate, and consequently fewer parameters': 5,\n",
       " '. Architectures employed for NLP applications differ in that they typically involve temporal ratherthan spatial convolutions.t C': 6,\n",
       " 'explore the magnitude-based pruning in conjunction with retraining, and report promising compression results without accuracy lo': 7,\n",
       " ', which does not make use of it.Among systems approaches, AlphaGo  recently achieved astonishingresults through combined algorit': 8,\n",
       " 'e SOTA LSTM-aux model . Our model also achieves higher averaged POS tagging accuracy than the SOTA joint model Stack-propagation': 9,\n",
       " ' algorithm and report onsome experimental results.As already mentioned, our work is closely related to the previous works of and': 10,\n",
       " '.We first test variants of the architecture for rman and then verify our empirical findings for Czech .We conduct the experiment': 11,\n",
       " 'le tag-sequences T1 using a softmax:This objective function and its gradients can be efficiently computed by dynamic programming': 12,\n",
       " ', alloptimize the unconstrained objective form.These methods are typically preferable in the large scale setting, since they hav': 13,\n",
       " '. DRAW is a variational auto-encoder with generative model pp and encoder q, similar to the model in section 5.2, but with both ': 14,\n",
       " '] algorithm for FiGAR-A3C experiments. The async-rmsprop algorithm [nce the Atari 2600 games tend to be quite complex, jointly l': 15,\n",
       " ' trained a support vector regression  model on a co-occurrence statistics model derived from the skip-gram word embedding model ': 16,\n",
       " ', which integrates a loss function  which learns distinct feature representations for anaphoricity detection and antecedent rank': 17,\n",
       " ', and question answering h . ;  . ;  . .wever, in the field of sentiment analysis, the attention is applied to only aspect-based': 18,\n",
       " 'n in the backprop phase. Training techniques to overcome these effects are beyond the scope of this work eg, quantized gradients': 19,\n",
       " ' Convolutional Networks, while still achieving good performance. A soft or differentiable attention mechanisms have been used in': 20,\n",
       " 'ter performance.This paper proposes an extension of the work of bret and  2014 by investigating the impact of different factors.': 21,\n",
       " 'becoming popular choices in designingdeep models, and most current state-of-the-art results involve using one ofsuch activations': 22,\n",
       " 'riment, we selected the following four recent state-of-the-art models.Brief descriptions of these models are as follows:Word2vec': 23,\n",
       " 'have recently compiled a massive Cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k  examples from CNN': 24,\n",
       " ', we observed that this model was able to incorporate term-frequency inverse document frequency   and significantly outperformed': 25,\n",
       " 'g scalar βt∈ according to the previous decoder state st−1:It is then used to compute the time-dependent image context vector : .': 26,\n",
       " 'that 8-bit fixed point quantization is enough for a networkto achieve almost the same performance as 32-bit floating point count': 27,\n",
       " '. It was trained with the Caffe framework on the ImageNet 2012 dataset  . .demoOur first visualization method is straightforward': 28,\n",
       " '. CTC assumes that the network outputprobability scores, normalized at the frame level. It considers allpossible sequence of let': 29,\n",
       " 'em. They report that even with 1000-best lists, the gains of using the NMT rescorer often do not saturate. Syntactically ded NMT': 30,\n",
       " '. The gradients are computed via back-propagation while regularization is executed by a dropout on the hidden vectors before the': 31,\n",
       " ', and Rectified near Units  as nonlinearities. Figure 1  shows a schematic view of this architecture. In the most recent formula': 32,\n",
       " ', and instead trained word embeddings of dimension 200 from scratch.We optimized with Stochastic Gradient Descent and used a bat': 33,\n",
       " 'introduce a novel architecturethat enables the optimization of networks with virtually arbitrarydepth through the use of alearne': 34,\n",
       " ' in both subtasks. Detailed results for our system appear inSection 3.4.The way we built our embeddings is based on retrofitting': 35,\n",
       " 'and Lasagne.The substitute DNN is trained on ourmachine for 6 substitute epochs.ringeach of these 6 epochs, the model is trained': 36,\n",
       " 'and  is abstractive.Our work starts with the same framework as , where we use RNNs for both source and target, but we go beyond ': 37,\n",
       " 'on words w0,...,wn−1 and an image are projected into a joint embedding space before the next word in a caption, wn, is generated': 38,\n",
       " '.All the word vectors are stacked in a word embedding matrix L∈Rd×|V|, where d is the dimension of word vector and |V| is vocabu': 39,\n",
       " 'nsampling module to adjust for the smaller image size.We train the networks with a mini-batch size of 96 and optimize using ADAM': 40,\n",
       " 'and Theano, two popular GPU implementations.Comparison can be tricky because CPU and GPU implementations bydefinition cannot be ': 41,\n",
       " ' we compare CTB and SCTB with three benchmarks: Thompson mpling, Relative Upper Confidence und  and the ery Selection Algorithm ': 42,\n",
       " ', we havetting σi,j,t∼Uniform{−1,1} for i∈,j∈,t∈ be independent  variables, it follows thatThe last inequality follows from Tala': 43,\n",
       " 'ctions for sequential yesian optimization including probability of improvement , expected improvement  , upper confidence bound ': 44,\n",
       " ', and mini-batch size is 80. Given the training set, we first run the ‘fast_align’  in one direction,and use the translation tab': 45,\n",
       " ', and we follow the design of these experiments. For training efficiency, we didn’t follow the exact same dimensionality used. I': 46,\n",
       " 'to encode the passage and questions. For the passage, the hidden state is defined: hi=concat.Where contextual embeddings di of e': 47,\n",
       " '.For example, consider asking the question “who was born in Paris?” and requiringthe word embedding for Paris to effectively con': 48,\n",
       " 'which leverages a single neural network directly to transform the source sentence into the target sentence, has obtained state-o': 49,\n",
       " ' todefine sequence level reward functions based on language models andreconstruction errors to bootstrap MT with lessresources .': 50,\n",
       " ', which takes malware’s API sequence as input and generates an adversarial API sequence. The substitute RNN is trained on benign': 51,\n",
       " 'om hashing algorithms with a slightly sparser random matrix A. re we provide a JL-type lemma for the random hashing algorithm in': 52,\n",
       " 'parsersthat use local features that are fed into a neural-network classifier, sometimes coupled with a node composition function': 53,\n",
       " ', trained on the Ubuntu alogue Corpous training set, to identify commonly misspelled variants of each activity. The result is a ': 54,\n",
       " ', which consists of a convolution layer and a max-pooling over the entire sentence for each feature map. This type of models, wi': 55,\n",
       " '. Using 32 yields a 2.5% improvement over the MFapproximation. This may seem small until one considers that we only modifythe al': 56,\n",
       " 'arning models with differential privacy guarantees is difficult because the sensitivity of models is unknown for most interestin': 57,\n",
       " ', namely, the usage of an external memory, the application of different operators over values in the memory and the copying of s': 58,\n",
       " 'ed if the full gradient computation gk was replaced by a growing-batch estimation, the linear convergence rate can be preserved.': 59,\n",
       " 'd representation of words for representing their underlying concepts by taking into consideration the structure of the KB graph ': 60,\n",
       " 'are split into two groups, phrases Pxy and words not-in-phrases Wy.We use the regular encoder from RNNsearch, but add tags to hi': 61,\n",
       " ', convolutional neural networks  . ;  . , and recursive neural networks  . .re we adopt recurrent neural network with long short': 62,\n",
       " '.We want to emphasize again that our vision of semantic similarity is one of topic-based similarity instead of paraphrase-simila': 63,\n",
       " ' observations. This gives a powerful approach for future frame modeling.Recent frame prediction methods based on neural networks': 64,\n",
       " 'sualization quality, including α−norm monyan13Visualise , total variationhendran2015CVPR , jitterhendran2015CVPR , Gaussian blur': 65,\n",
       " 'full gradient several times, which will also restricts its application in large scale context. Another variation of SVRG is SAGA': 66,\n",
       " 'nged during training. A single randomly initializedembeddingis created for allunknown wordsby uniform sampling from .We use ADAM': 67,\n",
       " 'rce distributions. sferring knowledge between domains is through such learned subspaces or feature representations. For example,': 68,\n",
       " 'for word representations, where the various directionsin the vector space representing the words are shown to give rise to asurp': 69,\n",
       " 'without gradient clipping, with the default hyper-parameters of the AdamTrainer in theDynet toolkits.2We also use dropout  at le': 70,\n",
       " ' adjusted gradient is ▽θlog. The value Rt−bt is known as the advantage function.With regard to the advantage actor-critic method': 71,\n",
       " '. Traditional word embedding methods  model the syntactic context information of words. sed on them,  proposed timent-Specific W': 72,\n",
       " ',which  provide analogous statistical and computationalbenefits despite being unsupervised. The text embedding techniqueof  is a': 73,\n",
       " '. GRU is better in the cross-subject test and LSTM is better in the cross-view test.  the other hand, GRU is faster than LSTM bo': 74,\n",
       " 'ed to O . Concretely,  .  show that all algorithms satisfying the so-called RVU property , which include Optimistic rror Descent': 75,\n",
       " '. Second, neural abstract machines definecontinuous analogues of ring machines or other general-purposecomputational models by “': 76,\n",
       " ',which contains about 500K dialogs extracted from the Ubuntu channel on Internet Relay Chat .u   perform a study of problems in ': 77,\n",
       " 'for further details.The overall efficiency of the CCCP algorithm relies on our ability to solve problems  and . At first glance,': 78,\n",
       " '. Instead of operatingon a pre-specified grid, this allows one to search for hyperparametersin a given range. We used the follow': 79,\n",
       " 'eelman_finding_1990 see Figure 3-leftand its extensions are used to tackle sequence-related problems, such asmachine translation': 80,\n",
       " ', a stochastic gradient-based optimization algorithm.For every entity mention in set M from Dtest, we perform a top-down search ': 81,\n",
       " 'also reports that trained models often report the same answer to a question irrespective of the image, suggesting that they larg': 82,\n",
       " '. milar to the source-originated case, it can also be built from a pivot-target bilingual corpus using a pivot-to-source transla': 83,\n",
       " 'for a certain number of iterations with the learning rate, minibatch size, and number of iterations tuned with other hyperparame': 84,\n",
       " 'orkto approximate the critic and represent the intra-option policies and termination functions.We used the same configuration as': 85,\n",
       " 'her hyperparameter search. Each model was trained during 50 epochs using the adaptive variant of Stochastic Gradient DescentAdam': 86,\n",
       " 'implementation as a starting point and parameters as reported in  for simulated control tasks with a fully connected network wit': 87,\n",
       " ' for large-scale clustering and is included in widely used machine learning packages, such as Sofia-ML Sculley  and scikit-learn': 88,\n",
       " '.We evaluate our model on a second character-level language modeling task on themuch larger text8 dataset . This dataset is deri': 89,\n",
       " ', speech recognition , and languagemodeling . wever, success has not been shown forcross-lingual tasks such as machine translati': 90,\n",
       " 'architecture by incorporating category information of each document, to learn more comprehensive andenhanced word representation': 91,\n",
       " 'to improve the efficiency of IE by learning how to incrementally reconcile new information and help choose what to look for next': 92,\n",
       " 'units to control information flow. There are many kinds of gated RNNs, such as long short-term memory,  and gated recurrent unit': 93,\n",
       " ', RMSprop, Adam. The update rules of several common optimization algorithms are listed in Table 1.We are interested in finding a': 94,\n",
       " '. . introduced the idea of off-policy training with auxiliary control tasks, such as pixel control or feature control, which can': 95,\n",
       " '. This reveals that the two worlds of deep learning and domain adaptation are not compatible with each other in the two-step pip': 96,\n",
       " ',or research to perform speaker adaptation of a phoneme classifier based onTRAPS .There are also a few publications which invest': 97,\n",
       " 'opy-augmented, will be called soft Q-learning.Following  . 2015, modern implementations of Q-learning, and n-step Q-learning see': 98,\n",
       " 'models where givenan input sequence X and output sequence Y of length TY, we modeland minimize the loss l=−∑tlogpθ.This setting ': 99,\n",
       " 'in rman, Spanish, Italian, and Farsi as fouradditional inputs to the retrofitting-and-merging process.The results of this varian': 100,\n",
       " ', CCG parsing sra and Artzi joint syntactic andsemantic parsing nderson . ; Swayamdipta .  andeven abstract-meaning representati': 101,\n",
       " ', in which a new network is created for each new task, and lateral connections between networks allow the system to leverage pre': 102,\n",
       " 've learning   and the more recent margin-based or confidence-based active learning . Our algorithm builds on recent work in DBAL': 103,\n",
       " 'for all reinforcement learning experiments. It was shown to achieve state-of-the-art results on several challenging benchmarks M': 104,\n",
       " '. Apart from the discriminator and generator which are the same as DCGAN, we add an encoder which is the ”inverse” of the genera': 105,\n",
       " 'vectors of answers. This correlation turns out to be quite high  for both CNN+LSTM and ATT models and significant  for the MCB m': 106,\n",
       " '.Being new to reinforcement learning, our first step was to train a robust baseline model on Atari Breakout. For this we used a ': 107,\n",
       " ' no concordance cases, are shown in Experiments C1 and C2.tch Normalization tch Normalization  is a recent technique proposed by': 108,\n",
       " ', Lasagne  and Parmesan1.Training using a NVIDIA Titan X GPU took around 1.5 hours for TIMIT, 18 hours for Blizzard, less than 1': 109,\n",
       " '., n is large, researchers have recently proposed stochastic gradient descent  and its variants like SVRG  to solve it. ny works': 110,\n",
       " 'n mechanism on top of the CNN or LSTM modelto introduce extra source of information to guide the extraction of sentenceembedding': 111,\n",
       " 'mbrace big data ideas for robotics .For training in simulation, we use the Async Advantage Actor-Critic  framework introduced in': 112,\n",
       " 'onality reduction techniques are often applied to the original matrix to decrease the memory and computational requirements.SGNS': 113,\n",
       " 'is equivalent to CharWNN without character-level embeddings,i.e.,it uses word embeddings only.Additionally,in the same way as in': 114,\n",
       " 'lize any available unlabeled data.1 We note that while variational methods  have been applied to semi-supervised classification ': 115,\n",
       " '. Specifically,  . ;  .  uses an LSTM for the sentence encoder, while n .  uses a bag-of-words to represent sentences.We first p': 116,\n",
       " '.  were the first to present a graph-based neural network parser, employing an MLP with bidirectional LSTM inputs to score arcs ': 117,\n",
       " 'For planning algorithms that can be represented as a computation graph , such as linear quadratic regulator  and value iteration': 118,\n",
       " 'is unable to identify correlations between edge states and node states.Gated Graph ral Networks ,  . The message function used i': 119,\n",
       " 'and possibly intractable task. In this work we investigate if it is possible to replace these early layers, by simpler cascades ': 120,\n",
       " 'duces an output, a NAM is a form of transducer, namely avisibly-pushdown transducer ICALP:RS08 .ral stack machines like those in': 121,\n",
       " ', thus we begin by briefly outlining it. t W1 and W2 denote filter matrices  for pattern size n=2. newcitei15 generate a sequenc': 122,\n",
       " ', the norm of the gradient decays very slowly when adopting batch normalization with a proper initialization of γ, even after th': 123,\n",
       " 'version of ImageNet presented in  that will soon be released as a standard dataset. We train the network with the Adam algorithm': 124,\n",
       " 'umber of filters to extract local features of the sentence.In this work, we employ one-dimensional wide convolution described in': 125,\n",
       " 'ataset can be handled by ‘blind’ models which use language input only, or by simple concatenation of language and visionfeatures': 126,\n",
       " 'ication, we also experimented with five more baseline classifiers, all of which were implemented using the scikit Python package': 127,\n",
       " 'and  . . A GAN model consists of two networks, simultaneously trained in an adversarial manner. A generative model, referred as ': 128,\n",
       " 'pose , and phone recognition . TRBMs have been applied for transferring 2D and 3D point clouds , and polyphonic music generation': 129,\n",
       " 'his problem, it is possible to construct a sub-space by either sampling  . , generating a k-best list g .  or mode approximation': 130,\n",
       " 'studied normalization using weight reparameterizations. An early work by  mathematically analyzed a form of online normalization': 131,\n",
       " '.An output non-linearity similar to LSTM networks could alternatively be used to combat this issue.For optimization and Wikipedi': 132,\n",
       " 'that have been trained with the principles of GANs  .  to reconstruct images from hidden-layer feature representations within Ca': 133,\n",
       " ', a method for faster accuracy convergence), this is the first work targeted at LVCSR under noisy conditions.Our novel ACCAN tra': 134,\n",
       " ', question answering, predicting box-office revenues of movies based oncritic reviews  modeling text interestingness, and modeli': 135,\n",
       " 'nputs.Indirect supervision arises more generally in latent-variable models,which arises in machine translation ,semantic parsing': 136,\n",
       " '. Statistically, capturing correlations via a low-rank structure could help improve the sample complexity for recovery, and comp': 137,\n",
       " 'forimage generation. wever, their receptive field grows linearly and handling long-scale effectsis difficult. Our results are qu': 138,\n",
       " 'propose anothermodification to the ED training objective where the true tokenyj−1 in the training term logPr isreplaced by a sam': 139,\n",
       " '.We validate the learning rate for parameters m and b in Equation 4 to make the effective region of the sigmoid function of TAGM': 140,\n",
       " 's works dedicated to address the problem of huge overparametrization of deep models with structured matrices, e.g. the method of': 141,\n",
       " 'sons when trying to reduce computation viaconditional computation or to reduce interactions between parameters viasparse updates': 142,\n",
       " ' They find that conditioning on a smaller window of a pixels leads to betterresults with VAE, which is similar to our finding.ch': 143,\n",
       " 'rticles or when the dataset is small, W or TF-IDF is still the state-of-art representation compared to sent2vec or paragraph2vec': 144,\n",
       " 'target sentences are given simply as flat sequences of characters  or statistically, not linguistically, motivated subword units': 145,\n",
       " 'to use dynamic k-max pooling to stack multiple convolution layers, and gets insight from  to investigate variable-size filters i': 146,\n",
       " 'eve state-of-the-art performance for a wide range of NLP tasks, including many sequence tagging tasks  and  , dependency parsing': 147,\n",
       " 'can be applied. Instead of only using previous true label, using sample from previous predicted label distribution in model trai': 148,\n",
       " '.In particular, different types of attention models have been proposed to address this problem, including hierarchical , stacked': 149,\n",
       " '.This work has shown how to extend the modeling capabilitiesof recurrent neural networks by combining them with nonlinear state ': 150,\n",
       " ', but replace the decoder with a classifier. nce error types are not given in unsupervised training, our classifier does not inf': 151,\n",
       " '.wever, as in shown in Figure 6, there is no notable performance degradation for longer documents and questions contrary to our ': 152,\n",
       " '.Our first set of experiments compareall-reduce, elastic averaging, and push-gossiping at p=8 and p=16with an aggregate minibatc': 153,\n",
       " 'uring training.Crucially, in this prior work the training scheme relies heavily on the existence of large structured datasets:in': 154,\n",
       " 'atural language processing tasks such as language modeling , dependency parsing , sentence compression , and machine translation': 155,\n",
       " 'umulative gain  computed for top 10 tweets of each user. We call it , hereafter.In our experiments, we used Scikit-learn library': 156,\n",
       " 're f can be a logistic function, the sophisticated long short-term memory  unit , or the recently proposed gated recurrent unit ': 157,\n",
       " 'ices of their inputs. Our model follows the convolutional autoregressive structure introduced by ByteNet , WaveNet  and PixelCNN': 158,\n",
       " 'of depth 40 instead of 28,we achieve an accuracy of 81.76%. In comparison, the bestperformance achieved by a vanilla wide resnet': 159,\n",
       " '.Incorporation We consider two separate approaches to incorporating the retrieved values from the key-value memory, motivated by': 160,\n",
       " 'model with the released code1 and evaluate it with our internal English Gigaword test set and MSR-ATC test set.sed on ABS model,': 161,\n",
       " 'on nese Gigaword corpus  using default settings. The number of dimensions that we try are 50, 100, 150, 200, 250, and 300. We in': 162,\n",
       " 'ompare its performance with that of a state-of-the-art stochastic optimization algorithm, namely adaptive moment estimation Adam': 163,\n",
       " '.Thus, analyzing the attention weights of a trained model can give us valuable insight into the information that is retained ove': 164,\n",
       " '.  DUC 2002 corpus, we report several baselines such as Integer near Programming based approach , and graph based approaches suc': 165,\n",
       " 'handling more tasks than the five tasks used in our experiments; examples include entity detection and relation extraction as in': 166,\n",
       " 'recent years, reaching higher translation quality than statistical phrase-based machine translation  on many tasks. man analysis': 167,\n",
       " '. Recent work has also been exploring applying word embeddings to capture image semantics .Relational Databases:In the context o': 168,\n",
       " 'was used as an activation function for all architectures.Our DQN baseline consists of the encoding module followed by the transi': 169,\n",
       " 'imulated source domains to a real-world target domain . While a number of model-free RL algorithms have been proposed see, e.g.,': 170,\n",
       " ' sampled from the noisedistribution Pnw. We use smoothed unigram frequenciesexponentiating by 0.75 as the noise distribution Pnw': 171,\n",
       " 'propose to initialize the network with decorrelated features using orthonormal initialization xe . 2013 while normalizing the va': 172,\n",
       " 'on convolutional layers.AlexNet is trained by the hyper-parameters and data augmentation depicted in Caffe.GoogNet is trained by': 173,\n",
       " 'periments in this paper, we set d=50 and initialize v with wi’s word vector pretrained from large scale corpus, using methods in': 174,\n",
       " '. As in , only sentence pairs are required in the learning; the difference is that the embedding leveraged both monolingual data': 175,\n",
       " ', and Q-Prop , etc.. Secondly, systematic investigation into the problems such as how to augment the reward function for other i': 176,\n",
       " 'e in perplexity on the validation setbetween two consecutive epochs is less than 1%.We follow training protocols as described in': 177,\n",
       " 'is used to simulate the RL formulation of the AIM problem described in Section IV. For this purpose, the AIM state transition an': 178,\n",
       " 'assification krizhevsky2012imagenet ,semantic segmentation long2015fully , object detection girshick2014rich ,speech recognition': 179,\n",
       " 'y in developing deep learning methods, there are a number of open-source deep learning toolkits including Caffe from UC Berkeley': 180,\n",
       " '.  introduce an end-to-end sequence model to generate captions for videos.By applying attention mechanism  to visual recognition': 181,\n",
       " 'proposed spatio-temporal video prediction conditioned on actions and previous video frames with deep neural networks in Atari ga': 182,\n",
       " 'usedan unrolled model. As such, all of their approaches are subject tothis concern. Notably, all three limited the depth of trai': 183,\n",
       " 'port of SqueezeNet: Chainer  port of SqueezeNet: ras  port of SqueezeNet: Torch  port of SqueezeNet’s Fire Modules: We now turn ': 184,\n",
       " ' representing numeral sequences.We train the ’s continuous Skip-gram model using the neuralnetwork and source code introduced in': 185,\n",
       " 'ation algorithms whose learning rates are carefully chosen and other hyperparameters are set to the default values in Tensorflow': 186,\n",
       " 'and even game playing at human level .In NLP, distributional representations are pursued as a more flexible way to represent sem': 187,\n",
       " ',with a learning rate of 0.001 for TIMIT and Accent and 0. for the rest.We use a minibatch size of 128 for Blizzard and Accent a': 188,\n",
       " '.  . argue that memory is necessary in partially-observed tasks, and augment the state space to include memory states . . observ': 189,\n",
       " 'wever known to have difficulties instoring information for long time-spans . ng short-term memory and gated recurrent units GRU,': 190,\n",
       " 'on a corpus of 951M words ldiz . , 2,045,040 of which are unique. This corpus consists of rkish text extracted from several nati': 191,\n",
       " 'used bilingual or comparable corpus which is also expensive for many low-resource languages.newcitesogaard-EtAl:2015:ACL-IJCNLP ': 192,\n",
       " ' be viewed as a machine translation problem.Treating syntactic parsing as a string prediction task is defined and implemented in': 193,\n",
       " ', a variant of stochastic gradient descent that adaptively tunes the step size for each dimension. We run each model for 100-400': 194,\n",
       " '.Details on the training procedure and hyperparameter settings are provided in Appendix B.2.At test time we applied a network to': 195,\n",
       " ', given the current estimate of the parameters Wl, the conditional gradient of the dualof program  with respect to a training sa': 196,\n",
       " ', which aims to provide a coherent and detailed description, like telling stories for images/videos.nerating a full paragraph de': 197,\n",
       " ', Adagrad , Adadelta .Our idea is to modify the initial cost function by taking away the non-linearity of the activation functio': 198,\n",
       " '.In multimodal MT , where an image and its caption areon the input, we might expect the caption to be the primary source ofinfor': 199,\n",
       " '.Model Training For our deep kernel learning model, we used deep neural networks which produce C-dimensional top-level features.': 200,\n",
       " '.ture work might consider alternatives; we chose thebidirectional LSTM due to its effectiveness in many settings.Given the disco': 201,\n",
       " '.While it is possible to use unimodal representations to encode the source modality, it has been shown that using a coordinated ': 202,\n",
       " 'o tried to train sigmoid Rumelhart1986  network, but the initial loss never decreased.Finally, as proposed by Swietojanski et.al': 203,\n",
       " 'e predictions necessary in many time-critical applications and decision support systems e.g., ocean sensing , traffic monitoring': 204,\n",
       " 'ension, which is half of that in g2017Rethinking , and GRUs with much smaller size. Also, our models presented here use word2vec': 205,\n",
       " 'theoretically shows that1/ε2 communications rounds of single vectors are enough toobtain ε-quality for linear classifiers, with ': 206,\n",
       " '. Two important differenceswith the approach proposed here is that here we avoid back-propagation thanks to an inference steptha': 207,\n",
       " 'ugmenting the neural networks with additional computational modules such as ral GPU , ral ring chine , and stacks-augmented RNNs': 208,\n",
       " 'g operations, and they combine features through summation before passing them to subsequent layers. milar to our work, DenseNets': 209,\n",
       " ' recent line of research has directly tackled the optimization problem of neural networks and provided either certain guarantees': 210,\n",
       " '.Convolutional neural networks are inspired by the observation that for inputs like images , many low level operations are local': 211,\n",
       " 'without any additional fine-tuning: δatt=0.6, λreg=10−9,k=50. We train for 5 epochs without early stopping using AdaGrad chi . .': 212,\n",
       " ', because it can allow for easyfine-tuning. Our semi-supervised learning approach is related toSkip-Thought vectors , with two d': 213,\n",
       " ', speech recognition , machine translation , image captioning , predicting output of simple computer programs  and action recogn': 214,\n",
       " ', pi=⌊τ⌋−τ+1 if i=⌊τ⌋+1−K1, pi=τ−⌊τ⌋ if i=⌊τ⌋+2−K1, and pi=0 otherwise.This reduces to finding a predicted ^pθ∈ΔK2−K1 given mode': 215,\n",
       " '. wever, with neural sequence models, we cannot organize beams by their explicit coverage of the input. A simpler alternative is': 216,\n",
       " 'and  the authors use compression methods for speeding up CNN testing time.More recently, some works focus on compressing neural ': 217,\n",
       " '. Thepartition function in the softmax over tokens is estimated using importancesampling with a unigram distribution over tokens': 218,\n",
       " 'notes:The basic idea is that humans  arelimited to “local descent” optimization methods, that make smallchanges in the parameter': 219,\n",
       " '.In this framework the encoder maps input videos into abstract representations that precondition a caption-generating decoder.As': 220,\n",
       " 'eats computing an attention vector between the query and the context through multiple layers, typically referred to as multi-hop': 221,\n",
       " 'on.We constructed a sentence evaluation tool to automate evaluation on all the tasks mentioned in this paper. The tool uses Adam': 222,\n",
       " ', the er is used to predict the start and end positions of the answer phrase in the passage.First we process the output of tch-L': 223,\n",
       " 'Label Space sformation  , Feature-aware Implicit Label space Encoding  ,  rank Empirical risk minimization for lti-Label arning ': 224,\n",
       " ' thatvector be vimage. Model B tries to match the final RNN state withvimage. Finally, model C develops the multimodal skip-gram': 225,\n",
       " ', leading to powerful representations.wever, how to systematically incorporate a desired invariance into the learned representat': 226,\n",
       " 'to represent the semantic manifold Ms, and a function vec that maps a word to a K×1 vector in Ms. The main assumption behind thi': 227,\n",
       " '. It will be nice to compare these rigorously as well. The learned embeddings can use additional information such as typing , ha': 228,\n",
       " ' . . The only change we make is to use long short-term memoryunits  instead of gated recurrent units . rther, instead ofAdadelta': 229,\n",
       " 'is may be due to that word2vec provides more generic word representations, since it is trained on the large Google News dataset ': 230,\n",
       " 'ed along the frequency dimension,which can help build invariance against small spectral variation.In our deepest 10-layer CNN in': 231,\n",
       " 'orpus into training and validation sets by withholding a random selection of 20% of the dialogues from training. We use the Adam': 232,\n",
       " 'tion etc.Until recently, RNNs were considered very difficult to train because of the problem of exploding or vanishing gradients': 233,\n",
       " '. The Word2Vec model for each of the eight folds is trained from the collection of tweets pertaining to the seven events in the ': 234,\n",
       " 'ordeconvolution , are introduced in the literature. Finally, concatenate the feature maps of the downsampling layerswith the fea': 235,\n",
       " ' to zero. The initial forget gate bias for LSTM is set to 3. Gradients are clipped if the norm of the parameter vector exceeds 5': 236,\n",
       " 'ion.LMs have played a key role in traditional NLP tasks such as speech recognition , machine translation , or text summarization': 237,\n",
       " 'returned binary probabilities which do not lend themselves well to multi-label classification where each review has a variable n': 238,\n",
       " '.Specifically, we zero-pad the beginning of the sequence with k−1 elements, assuming the first input element is the beginning of': 239,\n",
       " 'ods that can scale to large knowledge bases. Tensor factorization e.g. ckel ., 2011, 2012 and neural-embedding-based models e.g.': 240,\n",
       " ', or policy gradient reinforcement learning  to work around the inapplicability of gradient-based learning to problems with disc': 241,\n",
       " ', Fig. 9 shows the tanh of a particular LSTM cell’s state called memory in This paper presented an approach using LSTM networks ': 242,\n",
       " '.Because the biomedical datasets are imbalanced, we use downsampling   . ;  and   to effectively train on balanced subsets of th': 243,\n",
       " 'and  .  demonstrated the effectiveness of globally normalized networks and training with beam search for part of speech tagging ': 244,\n",
       " '. We also evaluated thestrided convolutional version byThe main differences to Conv-CNN-C are the use of Gaussian noiseinstead o': 245,\n",
       " 'o scale up yesian inference, much progress has been made on developing online variational yes  . ; mno .  and online Monte Carlo': 246,\n",
       " ' the DCGAN and unrolled GAN images.Figure 24 compares all approaches when trained with 5× higher learning rate  . As observed in': 247,\n",
       " ' customer product reviews.Attention based methods have been successful in many application domains, such as image classification': 248,\n",
       " 'efinished this work discusses the idea of imitating cortical feedbackby introducing loops into neural networks.A Highway Network': 249,\n",
       " 'used in our experiments, tend to be co-hyponyms as shown in prior studies . The results presented in both Table 2 and 3 have bee': 250,\n",
       " 'on CIFAR-10 images. The inception score is used for GANs to measure samples quality anddiversity on the pretrained inception mod': 251,\n",
       " 'and property of RRF , we haveCombine above two inequalities, and choose optimal dp as ~O, we obtain the first inequality of the ': 252,\n",
       " 'and textual entailment  . .wever the linguistic structure used in applications haspredominantly been shallow, restricted to bile': 253,\n",
       " ' science training.We also presents results on the same task for a state-of-the-art artificial neural network  dialogue model see': 254,\n",
       " '. tharchitectures have been introduced with the explicit goal of trainingdeeper models.There are, however, some surprising findi': 255,\n",
       " 'uiding the order in which the different configurations are tried, from sophisticated ones such as Sequential yesian Optimization': 256,\n",
       " 'irs:f=n∑i=1tanhThey find that this composition slightly outperforms addition, but underperforms it on smaller training datasets.': 257,\n",
       " ', use linear matrix factorization methods for speeding up convolutions and obtain a 200% speed-up gain with almost no loss in cl': 258,\n",
       " '. We found that using a learning rate of around 1e−4  improved convergence and helped avoid a common local optimum, mapping all ': 259,\n",
       " 's applied after the fire9 module.Note the lack of fully-connected layers in SqueezeNet; this design choice was inspired by the N': 260,\n",
       " 'icated on the existence ofgroups, and aim to guarantee that certain groups are notunequally favored or mistreated. In this vein,': 261,\n",
       " ', there have also been proposed models that learn text representations by leveraging structured linguistic resources.For instanc': 262,\n",
       " ' target these issues and enable the training of deeper networks. wever, stacking more than a dozen layers still lead to a hard t': 263,\n",
       " '. In SGNS, each word wi is represented by two dense, low-dimensional vectors: a word vector  and context vector . These embeddin': 264,\n",
       " 'er to the state-of-the-art which arengrams TF-IDF for these data sets and significantly surpass convolutional modelspresented in': 265,\n",
       " 'propagation as described above.The architecture is sketched in Figure 4.All three models were implemented in the Caffe framework': 266,\n",
       " 'called MNIST-scale. It consists of 28×28 gray-scale images, where each digit is randomly scaled by a factor s∈U without making a': 267,\n",
       " ' Several techniques for producingdifferentially-private machine learning models modify trained models by perturbing coefficients': 268,\n",
       " 'ocabulary per minibatch.The second design choice was the number of LSTM layers. We used a three layer LSTM as it worked well for': 269,\n",
       " '. This method is used to learn high-quality word vector representations. The input is usually the index of the word in a diction': 270,\n",
       " '. The function of hidden cells and gates are defined as follows.where i,f,o and c refers to the input gate, forget gate, output ': 271,\n",
       " 'achieves the best results by integrating attention mechanism  on this task. Although we believe incorporating such powerful mech': 272,\n",
       " 'gates, the number of parameters is reduced.As an alternative to the LSTM, the Gated Recurrent Unit  architecture was proposed in': 273,\n",
       " 'm, it works well for user-generated data.Recent approach of using ConvNets on text classification mainly works at the word-level': 274,\n",
       " 'net consisting of a local template matching operation followed by a piecewise linear activation function also known as a xOut NN': 275,\n",
       " 'IST 2003 ,2004  2005  and 2006  datasets as our test sets.For English-rman, to compare with theresults reported by previous work': 276,\n",
       " 'ncoder, the size of the hidden layer  is 2048. For the coverage model, we set the size of coverage vectors to 50.We use Adadelta': 277,\n",
       " 'dels, using a joint attention/translation model makes joint learning of alignment and translation possible .th hierarchical RNNs': 278,\n",
       " ' onthe other hand, their improvements are about the same.The HIGGS dataset has 11 million samples with 28 dimensions.The first 2': 279,\n",
       " '. The decoder uses a single maxout  output layer with the feed-forward attention model .The En-De Hiero system uses rules which ': 280,\n",
       " 'ing,and 2) the need for a manually selected global learning rate. Afterderiving our method we noticed several similarities to  .': 281,\n",
       " '.Details are as follows:4 layer LSTM models with 1,000 hidden cells for each layer.tch size is set to 128.arning rate is set to ': 282,\n",
       " '. They show that performance can thus be improved bymore delicate text models.Others have suggested using extra-linguistic featu': 283,\n",
       " 'who demonstrate a larger model with greedy sequence inference performs comparably to beam search.In contrast to translation, we ': 284,\n",
       " 'working directly on themolecular graph representation. The approach thus more directly readsin the topology of the molecular mod': 285,\n",
       " ', instead of a single branching point in our network that creates forked paths to only specialize to individual tasks, we contin': 286,\n",
       " 's their usefulness for real-world tasks.As a first step we compute word vectors for each document in our corpus using a word2vec': 287,\n",
       " ', .Figure 7 shows two examples of multiple alignment, demonstrating alternative syntactic parsings of the ambiguous sentence fru': 288,\n",
       " 's are simply learned by passing the dot product of regional CNN feature and question embedding to a fully connected layer.In FDA': 289,\n",
       " 'coder and the predictor are parameterized by single-layer neural networks. A three-layer neural network with batch normalization': 290,\n",
       " '.We apply the PV-DBOW variant that learns an embedding for a document by optimizing the prediction of its constituent words.Thes': 291,\n",
       " 'm and max-pooling over contextual embeddings of vectors for each named entity.Our model architecture was inspired by glsplptrnet': 292,\n",
       " ', memory , and the use of parse structure .wever, it falls short of the promise laid out above. The sentences in SNLI are derive': 293,\n",
       " '. The word embedding learns how to do a dense representation of the input word as a vector in a continuous space of the specifie': 294,\n",
       " 'xample for classification in DBLP:journals/corr/G14 ; DBLP:journals/corr/ZophL16  based on Reinforcement learning techniques, in': 295,\n",
       " '.This problem arises because standard CNNs are discriminative models. This work provides a solution to improve instability of CN': 296,\n",
       " 'and security cao2015towards ; adversarial:ccs16 ; wu2016methodology ; perdisci2006misleading ; fredrikson2015model ; fredrikson2': 297,\n",
       " 'for generative language models that have been conditioned on various contexts, e.g.foreign language text for machine translation': 298,\n",
       " 'model trained on the full text of over 1 million biomedical papers taken from the Open Access subset of bMed. Because the corpus': 299,\n",
       " ' SVRGfoR. The distributed SVRG in  cannot be guaranteed to converge because it is similar to the version of SCOPE with c=0.EASGD': 300,\n",
       " '.In addition, the number of matching entities is also used to score each article.The top M articles based on these scores are se': 301,\n",
       " 'earch. Besides its success in traditional settings of machine translation, that is one-to-one translation between two languages,': 302,\n",
       " 'ises multiple utterances.Modelling such long-range dependencies with an RLM is difficult and is still considered an open problem': 303,\n",
       " 'forcement learning approaches to develop the computing models, where the former one is based on TensorFlow  computational graphs': 304,\n",
       " '. The attention mechanism has enabled a -directional RNN with GRU to achieve even better performance in machine translation , , ': 305,\n",
       " 'to minimize unweighted sequence cross-entropy.5 We perform 10 runs with different random initialization of the network and up to': 306,\n",
       " 'stability . We expect that CP-TPM can decompose the whole convolution layers in contrast to previous CP-decomposition approaches': 307,\n",
       " 're is actually noadvantage to asymmetry. In the third setting, where an asymmetrichash is indeed needed, the hashes suggested by': 308,\n",
       " '. ch modelsstill fall under the first approach, however, in contrast to previousworkZelle and  ;  and  ; ang . they reduce the n': 309,\n",
       " '. All models were implemented using Theano Theano Development Team . Word embeddings were initialized by the results of word2vec': 310,\n",
       " '. The regression network during test time uses the batch normalization parameters estimated on the training data, however, in th': 311,\n",
       " 'source language and nese as the target language. The labeled English data consists of balanced labels of 650k lp reviews from  .': 312,\n",
       " 'ddings are represented as real-valued vectors and capture syntactic and semantic similarity between words.For example, word2vec1': 313,\n",
       " 'epresentations are learned from word alignments, sentence alignments, or, more rarely, from aligned, comparable documents vy . .': 314,\n",
       " ' efficient online approximation of second-order methods , which can model correlations between input features. tch normalization': 315,\n",
       " 'lower levels was, to our knowledge,first introduced by  &   for natural languageprocessing tasks, and has since been extended by': 316,\n",
       " ', or recurrent spatial transformer networks .We recently proposed an attention-based model to transcribe full paragraphsof handw': 317,\n",
       " '.t Uw∈REu×|U|w denote the unit-level representation of w,where the jth column corresponds to the unit embedding of uj.The idea o': 318,\n",
       " ' We compare the performances of different architectures and report ROUGE scores in Tab. 1.Our baselines include the ABS model of': 319,\n",
       " ', Caffe , Torch , CNTK , and tConvNet .These platforms facilitate the definition of complex deep learning networks as compositio': 320,\n",
       " 'nes, as in the REINFORCE  algorithm, to mitigate the high variance problem, and carry out efficient neural variational inference': 321,\n",
       " '.We evaluate our grasp controller on the UR5 robot in three experimental scenarios:  objects in isolation on a tabletop,  object': 322,\n",
       " ' of mood states and stock market indices .Recently, Recurrent ral Network  with ng-Short Term Memory   or Gated Recurrent Units ': 323,\n",
       " 'and reduce the size of the vocabulary, typically the RNN model considers a certain number of frequent words e.g. 30,000 words in': 324,\n",
       " 'bottleneck by allowing referral back to previous output vectors.Recently, these mechanisms were applied to language modelling by': 325,\n",
       " 'ties over possible expansions and we backpropagate an error signal based on the ground truth programs. We use the Adam optimizer': 326,\n",
       " 'recently showed that it ispossible to use convolutions with various dilation factors to allow thereceptive field of a generative': 327,\n",
       " 'in sentence embeddings, keeping the models and learning procedure fixed. So we select models and a loss function from prior work': 328,\n",
       " 'or convolutional neural networks , which have recently achieved strong performance across many diverse text classification tasks': 329,\n",
       " ' Pl and Rl. This is equivalent to applying the following update to the original network:withThis is a special case of mma 5 from': 330,\n",
       " ', NEWSQA  and MS MARCO ,the answer to each question is in the form of a text span in the article. Articles of SQUAD, NEWSQA and ': 331,\n",
       " '.We use the publicly released model6 to re-generate label maps of FCN while the results of DeepLab are extracted from their publ': 332,\n",
       " 'n of highway connection isquite similar with that of GRU’s “update gate” z, which isessentially a variant of “leaky integration”': 333,\n",
       " 'that tree-structured models can outperform sequential counterparts. Comparison to the non-attentional baselines , the attention ': 334,\n",
       " 'ger body of text. The abstractive nature of the summary is likely to demand a higher level of comprehension of the original text': 335,\n",
       " 're d is the dimensionality of the representations. Given the paragraph p, we apply a bidirectional Gated Recurrent Unit  network': 336,\n",
       " '.neural:instruction use a sequence-to-sequence model to mapnavigational instructions to actions. Our model works on morewell-def': 337,\n",
       " 'along the input path.As a baseline, we use a fully-connected neural network withbatch normalization and ReLU activation function': 338,\n",
       " 'also demonstrate their powerful abilities on the sentence representation for paraphrase, sentiment analysis, and so on. Moreover': 339,\n",
       " 'ords in a command. We utilize them specifically as they have been shown to work well on natural language sequence modeling tasks': 340,\n",
       " 'tor to limit the discriminator capacity. nowozin2016f  further extended GAN training to any choice of f-divergence as objective.': 341,\n",
       " ',where sentences are decoded one word at a time.The most probable word is chosen and fed to the network at the next time step.Th': 342,\n",
       " 'to parallelizetraining of deep nets over a very large number of nodes. As we will see inSection 4, we hypothesize that as the si': 343,\n",
       " 'nguage model parameters .Our specialized neural architecture also addresses a recent problem found in neural conversation agents': 344,\n",
       " 'is also recently proposed to sequentially generates a pixel at a time, along the two spatial dimensions.In this paper, we combin': 345,\n",
       " 'orm and open-ended Image-QA dataset.They also propose a model which embeds question and image into a joint representation space.': 346,\n",
       " 'extract meaningful feature representations from both labeled and unlabeled data, one possible option is to train an auto-encoder': 347,\n",
       " ' by the best bidirectional LSTM models described by  . , which both make useof Polyglot word vector representations published by': 348,\n",
       " 'ns have beenproposed .wever, such approximations make the algorithms less invariant than theoriginal natural gradient algorithm.': 349,\n",
       " 'ntation of the passage and attempts to predict thefinal word. The input representation comes from adding pre-trainedCBOW vectors': 350,\n",
       " '. wever, when thebatch size is larger, the memory requirement for each layerincreases, thus effectively limiting the maximal siz': 351,\n",
       " ', if it uses both the content and the previous context to compute the next context. It is soft if it computes the expectation ov': 352,\n",
       " ', and learned fromscratch.As training data, we sample image patches from 49 scenes of the DTUdataset  Fig. 3.Positive samples  a': 353,\n",
       " 'se and sentence embedding space. All of the models are randomly initialized using standard techniques and trained using AdaDelta': 354,\n",
       " 'n  or dialogueagents . While traditional neural networks languagemodels have obtained state-of-the-art performance in thisdomain': 355,\n",
       " 'e observedsequence and across data examples.The need for highly structured output functions in an RNN has been previously noted.': 356,\n",
       " ') but also with σ∈{0,0.1,1}.In Figure 9 it is clearly visible that there is a significant advantage in using variable noise comp': 357,\n",
       " 'ations of such representation include the inability to encode similarity between words, as well as losing word order information': 358,\n",
       " 'y tape rather than a single memory cell.They achieved promising results on the standard Penn Treebank benchmark corpus .milarly,': 359,\n",
       " ', and so we use these 100-dimensional embeddings in all experiments. We concatenate a 5-dimensional word shape vector based on w': 360,\n",
       " 'tectures , as well as methods to increaseparallelism while decreasing the computational cost and memoryfootprint .In particular,': 361,\n",
       " '; . ; . ; . ; . ;': 362,\n",
       " '.ly recently, in the remote sensing field, the work proposed in  performs preliminary experiments with LSTM model on a  time ser': 363,\n",
       " '.For Wikipedia comments, we use a ‘narrow’ convolution layer, with kernels sliding  over  embeddings of word n-grams of sizes n=': 364,\n",
       " 'orld datasets to verify our method.We compare it with several related multi-task learning  methods, including DG-MTL  and GO-MTL': 365,\n",
       " 'traction tasks .We used two variants of skip-gram models:  the skip-gram model trained using the negative sampling techniqueSGNS': 366,\n",
       " '. This motivates us to propose a CNN encoder for learning generic sentence representations within the framework of encoder-decod': 367,\n",
       " 'll now briefly present them and explain how the dataset we are introducing in this article differs from them.The LAMBADA dataset': 368,\n",
       " ', RCS Zoghi2014WSDM:RCS , CCB Zoghi2015NIPS:CDB , SCB Zoghi2015NIPS:CDB , RMED1 miyama2015COLT:DB , and ECW-RMED miyama2016ICML:': 369,\n",
       " 'sample if the variational functionTω is larger than f′, and classifying it as asample from the generator otherwise.We found Adam': 370,\n",
       " 'd text to improve visual representations for image classification by coordinating CNN visual features with word2vec textual ones': 371,\n",
       " 'is complementary to our own, in that it sheds light on the complexity of functions that a DCN can compute. th approaches could b': 372,\n",
       " 'd in the literature vy . : Wordm353 Agirre . , MEN Bruni . , mx-999  . , the MSR analogy dataset  . , the Google analogy dataset': 373,\n",
       " ' have no real motivation to include additional learned normalization parameters, as considered in comparable NCE language models': 374,\n",
       " 'k, rowski .  then proposed attention-based models for speech recognition, which are claimed to be robust to long inputs. lvin  .': 375,\n",
       " 'The state vectors of the forward RNN and the backward RNN are respectively computed as:where gated recurrent unit  as defined in': 376,\n",
       " 'to not beparticularly data efficient, the attention model of  .  was found to be highly data efficient,as it has matched the per': 377,\n",
       " 'f finding physics representations and directly map visual observations to physical judgments  or passive  and action-conditioned': 378,\n",
       " 'realized via soft gating mechanisms.Though different extensions and variations to GRUs and LSTMs have been investigated recently': 379,\n",
       " '.wever, we did not see a meaningful improvement, so for simplicity,we stick to a multinomial prediction model.We train this mode': 380,\n",
       " 'n layer, after which ReLU is used as the nonlinearity (including xoutCNN, which makes out implementation slightly different from': 381,\n",
       " ', on-policy IL methods sample trajectories from the agent’s current distribution and update the model based on the data received': 382,\n",
       " ', and further tuned with the training of the model parameters. For pretraining, we have used an additional corpus  from English-': 383,\n",
       " ',or emotion detection ,the system’s focus matches up with human intuition.The gates modulating the network’s attention in these ': 384,\n",
       " 'perform the complex transformations.A disadvantage of deep LSTMs is that they can be difficult to train.directional LSTMs BLSTMs': 385,\n",
       " ',using either regularization or novel neural network architectures, though this work has not looked at transfer active learning ': 386,\n",
       " 'is to explore the impact of universal schema, testing on a dataset completely answerable on a KB is not ideal.WikiMovies dataset': 387,\n",
       " 'is applied with a threshold of 5.Parameter tuning is performed on both models using hyperopt8. For each model, configurations fo': 388,\n",
       " '. The second part is tointeract with source agent g2 so that the logits distilled from agent g2can be transferred by the deep al': 389,\n",
       " ', the algorithm ensuresfor all w∈K⊆Kt+1. Bythe curvature property in Assumption 2, we then have thatwhich completes the proof.∎W': 390,\n",
       " 'have been proposed to tackle the tasks of question answering and reading comprehension. wever, the performance of these neural m': 391,\n",
       " 'using publicly availablecode 2013d  with the provided parameters and necessary preprocessing.In the third column, results are sh': 392,\n",
       " 'fers slightly from the usual definition of blankout dropout,which alters the feature vector x by setting random coordinates to 0': 393,\n",
       " '. When dealing with language objects, most methods still focus on seeking vectorial representations in a common latent space, an': 394,\n",
       " 'on  and image captioning .In natural language processing they have been used for machine translation  and sentence summarization': 395,\n",
       " ', they exploit RNN and Convolutional ral Network  to build a question generation algorithm, but the generated question sometimes': 396,\n",
       " 'ever is that PRONG applies this update in thewhitened parameter space, thus preserving the natural gradient interpretation.K-FAC': 397,\n",
       " ',there is still no consensus on what constitute good options.In this paper, we show that a choice of options is equivalent to a ': 398,\n",
       " 'ing strategy has been shown to give good results faster without tweaking step size for GoogleNet implemented by Sergio  in Caffe': 399,\n",
       " 'for 50 epochs with stochastic gradient descent using restarts , the GELU nonlinearity, and standard mirroring and cropping data ': 400,\n",
       " 'l resources: belNet or x, and (English) VerbNet. Formalised as an instance of post-processing semantic specialisation approaches': 401,\n",
       " 'are also commonly used.The word embeddings and the composition function are jointly learned on a certain target task.nce composi': 402,\n",
       " '. Moreover, a significant advantage of DeepCoNN compared to most other approaches  which benefit from reviews is that it models ': 403,\n",
       " 'orating a target decoding algorithm into training. Wiseman and   and  .  proposed a learning algorithm tailored for beam search.': 404,\n",
       " ', our method is able to accommodate the attention layers seemlessly and easily. It also draws a clear distinction from those wor': 405,\n",
       " 'and enables classifiers trained on the source domain to be applicable to the target domain. Moment discrepancy regularizations c': 406,\n",
       " 'ection 3 v is only connected to s and not ~w. For the visual features v we used the 4096D 7th Layer output of BVLC reference Net': 407,\n",
       " 'hine translation,summarization. The level of granularity of this processing can range fromindividual characters to subword units': 408,\n",
       " 'f the architecture along with the training regime. The convolutional filters used in our work have the same hyper-parameters  as': 409,\n",
       " 'to simple grid search in the space of hyper-parameter values. wever, independently of the order of production of configurations,': 410,\n",
       " 'licable for this case, a policy gradient approach  can help to alleviate the high variance problem during stochastic estimation.': 411,\n",
       " '. The MAU on s dataset for this experiments resulted in 3.3% MAU which is better that the near Classifier in Table VIII. wever, ': 412,\n",
       " ' be found in. A variation of the ranking log loss allowing for adifferent margin for the negative and positive class is given in': 413,\n",
       " 'tivations and even low-precision gradients, including but not limited to naryNet , XNOR-Net , ternary weight network  , ReFa-Net': 414,\n",
       " ' recognition of individual behavior, pairwise interactions among individuals participating in differentroles in a joint activity': 415,\n",
       " 'de a linear embedding of the visual representation and L2 normalization of both input modalities, instead of batch normalization': 416,\n",
       " 'We call this model, illustrated in Figure 1, R-LDA-Conv.We evaluate our model from different aspects on the Ubuntu alogue Corpus': 417,\n",
       " 'obtained state-of-the-art semantic segmentation results using an architecture similar to  but enforcing structure using globally': 418,\n",
       " 'hey force a DNN trained by Metand, anonline API for deep learning, to misclassify inputs at a rate of 84.24%. In afollow-up work': 419,\n",
       " 'imple atomic labels given to entities that indicate what the entity is notable for, and so serve as a useful information source.': 420,\n",
       " 'solution, whose form is similar to the use of element-wise adaptive learning rate.The proposed method also reduces tonaryConnect': 421,\n",
       " 'chronize w.sed on this observation,   proposed toupdate several coordinates or blocks simultaneously and update the global w,and': 422,\n",
       " 'ses deghi and  -------171-Viske deghi . -------6500--DAQUAR linowski and  1,449--------12,468COCO QA  . 123,287--------117,684du': 423,\n",
       " ', the parse trees are linearized, with pre-terminals also normalized as “XX”. Figure 1 illustrates how we convert standard treeb': 424,\n",
       " 'entiment analysis with superior performance. wever, the model is not designed to capture the fine-grained sentence structure. In': 425,\n",
       " ' a 2×2 max pooling, and then a hidden layer of 500 and output layer of 10, fully connected. This is the structure used in Theano': 426,\n",
       " '.More specfically, we get rid of the softmax in equation 3 and optimize the following sentence-level objective:where S is a rand': 427,\n",
       " '. This task is closely related tothe larger-context language model we proposed in this paper in the sense thatits goal is to bui': 428,\n",
       " '. GANs also contain two networks – a generative network that is the same as in variational auto-encoders, and a classification n': 429,\n",
       " 'ent with retrofitted embeddings, in which embeddings are mapped in accordance with a lexical resource. Following the approach of': 430,\n",
       " 'bels as ground truth assignments.We compared our loss components Lsingle and Lmulti with the DeCov regularizer  and XCov penalty': 431,\n",
       " '. The abbreviation of the evaluated algorithms and the best parameter setting after several experiments are listed below:We use ': 432,\n",
       " '.This algorithm allows more exploration of local optima thanwnPour and alleviates the need for frequent communicationbetween wor': 433,\n",
       " ', Thompson sampling , entropy search , and dynamic combinations of the above functions ; see newciteshahriari2016taking for an e': 434,\n",
       " 'As baselines, we consider two models which have shown remarkableaccuracy on the VQA task, given their simplicity: BOW andiBOWIMG': 435,\n",
       " ' explored in the past.For example, quiz-style datasets  have multiple-choice questions with answer options. Cloze-style datesets': 436,\n",
       " 'ing over the entire tweet. A filter w∈Rh×d operates on the tweet to give a feature map c∈Rn−l+1. We apply a max-pooling function': 437,\n",
       " '. In the baseline model, this is enabled by using bidirectional layers, which are impossible to deploy in a streaming fashion, b': 438,\n",
       " 'with default learning rate 1.0 and sentences were grouped into batches of size 64.Performance on the development set was measure': 439,\n",
       " 'el obtained a lower performance using grayscale images, so we decided to use colors in all experiments.We use the ViZom platform': 440,\n",
       " 'ethods are shown in Table 3.We compare our model with several state-of-the-art methods: SDT-RNN  . 2014, DeViSE  . 2013, DeepFE ': 441,\n",
       " '.Formally, let xi∈Rk be the k-dimensional word vector corresponding to the i-th word in the sentence. t n be the length  of the ': 442,\n",
       " '.As shown in Table 1, the returns data is generally resistant to overfitting. The network generally performs better when made de': 443,\n",
       " ' manifest invariant factors underlying different populations and are transferable from the original tasks to similar novel tasks': 444,\n",
       " 'and isknown to be able to transliterate rare, proper nouns up to a certainextent nrich . . Inall the cases, we use up to top-30k': 445,\n",
       " ', convolutional neural networks  are used to generate word embeddings, and achieved the state of the art results on English Penn': 446,\n",
       " 'ge models, we found that usually only the previous five output representations are utilized.This is in line with observations by': 447,\n",
       " '; in a Pointer Network,the only way to generate output is to copy a symbol from the input.Examples, next_to, const“what is the h': 448,\n",
       " 'ected Improvement EI Močkus 1974 as acquisition function as this combination has been shown to outperform comparable approaches': 449,\n",
       " ', vectorxi is simply the word embedding ei of the word at position i of the input sentence. For parsing conversational speech, w': 450,\n",
       " 'ressive results on several sequence learning tasks including handwritting recognition , speech recognition , machine translation': 451,\n",
       " 'with default coefficients and a batch size of 32. We apply a dropout with probability 0.5 to the vertical connections of LSTM  .': 452,\n",
       " 'are given in Table 2.mples from the MNIST model are given in Appendix Figure App.1.Our training algorithm provides an asymptotic': 453,\n",
       " 'mans to more accurately predict the “knowledge”, i.e., the actual output of an AI agent.mans adapting to technology. A few works': 454,\n",
       " 'pen-domain QA;  . shortcite:2014 applied recursive neural networks to the factoid QA over paragraphs.The work closest to ours is': 455,\n",
       " 'and Universe8, crosoft’s lmo project  . .The final goal of operating in these environments is in maximizing the final score.Even': 456,\n",
       " 'etc. Our performance on VOC-2012 is similar to Mostajabi   despite the fact we use information from only 6 layers while they use': 457,\n",
       " 'ncerning optimization algorithms, parallel synchronisations on clusters w/o GPUs, and stochastic binarization/ternarization, etc': 458,\n",
       " 'have facilitated the development of powerful neural models for reading comprehension.These models fall into one of two categorie': 459,\n",
       " '.In our future work, we plan to investigate applicability of neural networks architectures extended with memory  on this task.It': 460,\n",
       " ', with an initial learning rate of 0.001. We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on t': 461,\n",
       " '. These innovations have both scaledyesian analysis and removed the analytic burdens that havetraditionally taxed its practice.G': 462,\n",
       " 'tion is given in . Some model-freemethods produce many promising results in recent years, like deep Q network ,double Q learning': 463,\n",
       " 'and .’s data-set focuses on biographies and associates Wikipediainfobox with the first sentence of the corresponding article inW': 464,\n",
       " '.For different pretrained embeddings, Stanford’s GloVe 100 dimensional embeddings achieve best results on both tasks, about 0.1%': 465,\n",
       " 'vailable training data size. We observe that wecompare very well against Deep Speech 1 & 2 which were trained with muchmore data': 466,\n",
       " 'will be reviewedlater.  sentiment classification ,the region size chosen by model selection for our models was 5, larger than 3 ': 467,\n",
       " '.We develop a compositional vector space model for knowledge base completion using recurrent neural networks.In our challenging ': 468,\n",
       " 'with decay parameter ∈ and regularizer in ,RPROP  with initial stepsizes η0,RMSprop  with minimal learning rates η0, maximal lea': 469,\n",
       " 'manually generated rules.More specifically, inspired by the recent success in neural machine translation  . ;  . , summarization': 470,\n",
       " 'ing such units, divide the input space in a number of regions that is exponentially proportional to the number of network layers': 471,\n",
       " 'on the same set of tasks. We see that our Twitter models are state-of-the-art on a majority of tasks.Also, on a majority of task': 472,\n",
       " '.wever the linguistic structure used in applications haspredominantly been shallow, restricted to bilexical dependencies or tree': 473,\n",
       " 'em and solved through transfer learning socher2013zero .The general idea of predicting parameters has been explored before by  .': 474,\n",
       " ', we obtain the character-level embedding of each word using Convolutional ral Networks . Characters are embedded into vectors, ': 475,\n",
       " ';  PV-Cnt: combine PVwith  and ;  CNN-Cnt: combine CNN with  and.Apart from the baselines considered bynewciteyang2015wikiqa, we': 476,\n",
       " 'r relatively small models fitted to this large training corpus. We trained our model for only one epoch using the Adam optimizer': 477,\n",
       " 'uses  LSTMs to model the states ofthe buffer, the stack, and the action sequence of a transitionsystem. ss15  is another transit': 478,\n",
       " '. wever, the computational model of GPUs is organized for high parallelism by reusing large temporary buffers efficiently. This ': 479,\n",
       " 'bability, is a sigmoid see Eqn. 1. The model inputs are the concatenation of xt,ut:t+H and ot. We trained the network using ADAM': 480,\n",
       " '. The network has two convolutional branches, one to process A and the other to process B, with shared weights. These branches c': 481,\n",
       " 'erefore, it is difficult for the final hidden state of RNN to capture the early local information when the sentence is too long.': 482,\n",
       " 're allowed to view k coordinates instead of 1,corresponding to say the semi-bandit feedback model, the side-observation model of': 483,\n",
       " ' has been succesfully applied to several scenarios, such as, image captioning , texture classification , and machine translation': 484,\n",
       " '.The black-box threat model explored in this paper represents a more serious attack on deep learning. . applied a genetic algori': 485,\n",
       " 'etwork architectures are convolutional neural networks  for vision related tasks and LSTM-type network for language related task': 486,\n",
       " 'tation to the 3,000 top answers.To incorporate spatial information, we use soft attention on our MCB pooling method. Explored by': 487,\n",
       " 'his translates to a OpolyD/T rate whenever the expected loss function is “locally strongly convex” at the optimum.More recently,': 488,\n",
       " 'learning tasks sutskever2014sequence . The incorporation of long short-term memory  hochreiter1997long  or gated recurrent unit ': 489,\n",
       " ' expect there is room to improve this approach, by using more involved representations, e.g., linear ing kui . , Memory Networks': 490,\n",
       " 'ts of universal sentence embeddings usuallytrained by unsupervised learning . Thisincludes SkipThought vectors , ParagraphVector': 491,\n",
       " '. By contrast, Hybrid Code Networks are a hybrid of hand-coded rules and learned models.ults are shown in Table 1. nce Task5 is ': 492,\n",
       " ', user profiles , and diffusion patterns of the posts . Embedding social graphs into a classification model also helps distingui': 493,\n",
       " 'DRL works  ., 2016;  ., 2016b; endran ., 2017; ando ., 2017 attempt to solve the transfer learning problem. Progressive networks': 494,\n",
       " 'coherent responses. the other hand, many attempts have also been made to improve the architecture of encoder-decoder models.  ,.': 495,\n",
       " 'n the sequenceof preceding tokens.By sampling from this conditional distribution, one cangenerate reasonably realistic sequences': 496,\n",
       " '. Our approach uses Trust Region Policy Optimization.A discrete-time finite-horizon discounted rkov decision process  is represe': 497,\n",
       " '.Recently, purkar .  released the Stanford estion Answering dataset , which is orders of magnitude larger than all previous hand': 498,\n",
       " '.An exhaustive comparison of different composition functions has indeed revealed that an additive model performs well on pre-tra': 499,\n",
       " 'e released as open source upon publication.For any model using character embedding CNNs, we closely follow the architecture from': 500,\n",
       " 'more penalty to misclassification items and obtain smaller margin and vice versa.Target tasks are implemented using Scikit-learn': 501,\n",
       " 'r instance proposed to augment each source word with its corresponding part-of-speech tag, lemmatized form and dependency label.': 502,\n",
       " ', but none of them meaningfully improve upon SGD with momentum and gradient clipping in our preliminary experiments.Early Stoppi': 503,\n",
       " 'trained Memory Networks with RNN components end-to-end with soft memory access, and applied them to additional language tasks. T': 504,\n",
       " 've been successfully applied in sequence-to-sequence mapping tasks. They have made significant progresses in machine translation': 505,\n",
       " 'ame-by-frame modelling .Our approach, in contrast, operates directly on a fixed-dimensional representation of speech segments. .': 506,\n",
       " 'for learning the network parameters. For DDPG, the learning rates for actor and critic are 10−4 and 10−3, respectively. We allow': 507,\n",
       " '. Other approaches to combining evolution and learning have involved parameter copying, whereas there is no such copying in the ': 508,\n",
       " 'combines the functionalities of the CNN and RNN by introducing a new multimodal layer, after the embedding and recurrent layers ': 509,\n",
       " 'erify that the effects of Type Swaps are not limited to our specific model by observing the impact of augmented data on the DCN+': 510,\n",
       " 'so result in deep trees which in turn lead to thevanishing gradient problem when training. To cope with the vanishing gradients,': 511,\n",
       " '. Other methods also attempted to solve a similar task but were either generating object proposals Noh . ;  .  or required multi': 512,\n",
       " 'and supported by  . . At the same time such a tremendousreduction without significant loss of accuracy suggests that SELL is a p': 513,\n",
       " 'utomated analysis and labeling.We obtained word embeddings1 of rkish words as vectors of length 100 using the skipgram algorithm': 514,\n",
       " ' model in mini-batches using Adamma and   withthe learning rate of 10−4and maximal batch size64.We clip gradients by global norm': 515,\n",
       " 'explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model.  .  p': 516,\n",
       " 'nd one negative t2 paraphrase. We set the margin θ of the loss function to 0.2. ss minimization is done using the Adam optimizer': 517,\n",
       " 'These steps are important for effectively training the models, and the latter two have been used in previous work by wman .  and': 518,\n",
       " 'he same size as subword vectors ).CNN: In character-based models we choose the same values for hyperparameters as in the work of': 519,\n",
       " 'cently suggested in the context of ConvNets, which has been shown to outperform the traditional “dense classification” paradigm ': 520,\n",
       " 'and mirror performance of more conventional methods such as LDA Griffiths & Steyvers . rthermore, our approach can deal with deg': 521,\n",
       " ' accurate.We conduct topic learning and response generation in two separate steps rather than let them deeply coupled like VHRED': 522,\n",
       " 'if the erroneous labels reflect some underlying similarity between the examples in question.All models are trained with AdaDelta': 523,\n",
       " 'on exploring the limits of NNLM, only some practical issues, like computational complexity, corpus, vocabulary size, and etc., w': 524,\n",
       " ', we use multiple long short-term memory  layers to approximate the Q-function.LSTM can achieve good performance for complex tas': 525,\n",
       " '.Formally, an attention mechanism with a fixed memory Mt∈Rk×K of K vectors mi∈Rk for i∈, produces an attention distribution αt∈R': 526,\n",
       " ', and improving specificity .Concurrent with this work, citeauthorluan-16  improve topic consistency by feeding into the model t': 527,\n",
       " 'first pass all spatial image features through a bi-directional GRU that captures spatial information from the neighboring image ': 528,\n",
       " ', which matches the current decoder state st with each encoder hidden state hi to get an importance score. The importance scores': 529,\n",
       " 'back.Our algorithm combines the advantage actor-critic algorithm  . with the attention-based neural encoder-decoder architecture': 530,\n",
       " 'explored in more complex cases including StarCraft combat games . Recent work in foerster2017stabilising  applies the DIAL model': 531,\n",
       " 'operations make use of the internal structure of data and provide a mechanism for efficient use of words’ order in text modeling': 532,\n",
       " '.The main criticism of our agents is that they play with unrealistic reaction speed: 2 frames , compared to over 200ms for human': 533,\n",
       " 'er the mx-999 or Wordm-353-R intrinsic benchmark  to tune the CCA hyperparameters10with the Spearmint yesian optimization tool11': 534,\n",
       " '.Despite promising results, there are still many challenges with this approach. In particular, these models produce short, gener': 535,\n",
       " 'ly, there has been the flourish development of variance reduction techniques for SGD.Examples includestochastic average gradient': 536,\n",
       " '. The networks are trained on NVIDIA Tesla K40t and K80 GPUs. In this section, we present results for all the datasets and exper': 537,\n",
       " ' and embedding sizeto 100.We use a simple binary distributional model: Entry 1≤i≤n in the distributional vector of w is set to 1': 538,\n",
       " '. In Section 3, we provide a very brief overview of the model, and then follow up with our new bilingual model which is based on': 539,\n",
       " 'plied in . All of these techniques can also been used for SRU. Moreover, we believe different highway variants such as grid LSTM': 540,\n",
       " 'ce-to-Sequence  models have demonstrated excellent performance in several tasks including machine translation  . , summarization': 541,\n",
       " '.Our proposed feature extractors are based on a bidirectional recurrent neural network, an extension of RNNs that take into acco': 542,\n",
       " 'ix Σprior a different matrix multiplication is applied to the net’s output followed by softplus function, to ensure positiveness': 543,\n",
       " 'and belNet .The approach often uses unsupervised embeddings for initialization and attains state-of-the-art on standard NLP task': 544,\n",
       " 'sparency, we first trained an SVM with a simple bag-of-words model and default parameters as per the Scikit-learn implementation': 545,\n",
       " 'at “it is the entire space of activations, rather than the individual units, that contains the bulk of the semantic information”': 546,\n",
       " 'and Blocks .Table 1 shows the results of our hybrid tracker and other top performing trackers known from the literature. In the ': 547,\n",
       " 'pper, when considering maximization to construct acquisitionfunctions that minimize regret over the course of their optimization': 548,\n",
       " 'to perform feedforward and then backpropagation pass for some number of times, via the distributed wait-free backpropagation  al': 549,\n",
       " ', are becoming popular choices in designingdeep models, and most current state-of-the-art results involve using one ofsuch activ': 550,\n",
       " '.Thompson sampling uses a posterior distribution over the cell containing theta, and this posterior is computed by beginning wit': 551,\n",
       " '.We believe it is also important to investigate the theoretical aspects of these approaches,which remain largely unclear.As for ': 552,\n",
       " 'ates given new information. Recently, RNNs and LSTMs have been successfully applied in large-scale vision , speech  and language': 553,\n",
       " 'ese things. Exceptions to that rule are patterns that play supporting roles.As a general rule—the DONSVIC principle described in': 554,\n",
       " ':r∈Rnl is then used for the final classification, where n is the number of windows in CNN.In this section, we evaluate our model': 555,\n",
       " 'el content selectionand surface realization as local decision problems via log-linearmodels and employ templates for generation.': 556,\n",
       " '.In the context of visual dialog, visdial  uses attention to identify utterances inthe dialog history that may be useful for ans': 557,\n",
       " '; Vendrov . ;  . are all these kinds of work. There are 3 main works in the space, which claimsstate of the art results. 1. To a': 558,\n",
       " '. To enforce spatial consistency in the labeling characteristics of patches, we constrain the first MEX layer’s offsets to be un': 559,\n",
       " 'coherence score can still be high but meaningless.To alleviate this, we also use the similarity count mCount that was adopted in': 560,\n",
       " 'released the TORONTO–QA dataset, which contains a large number of images  and questions , but the questions are automatically ge': 561,\n",
       " '. The MT system is trained for 20 epochs, and the model with the best dev loss is used for extracting features for the classifie': 562,\n",
       " 'those samples, here the samplesare i.i.d., avoiding the very serious problem of mixing between modes thatcan plague MCMC methods': 563,\n",
       " 'which contains movie-related questions from the OMDb and Moviens databases and where the questions can be answered using Wikiped': 564,\n",
       " 'ts of first learning a latent space to represent  sentences using an encoder-decoder  framework based on Recurrent ral Networks ': 565,\n",
       " 'atenation of the two:Answer Pointer LayerThe top layer, the Answer Pointer  layer, is motivated by the Pointer Net introduced by': 566,\n",
       " ', we propose a loosely coupling model for two interdependent LSTMs.More concretely, we refer to hi,j as the encoding of subseque': 567,\n",
       " 'and NAF .We present the asynchronous version of DDPG in Algorithm 1. Compared with the original DDPG, we separate the sample col': 568,\n",
       " 'ntly, which can be grouped into different categories according to their motivations: dealing with rare words or large vocabulary': 569,\n",
       " 'd network starts at0.001 resp. 0.,and decays by a factor of 0.1 at epochs 15 and 25.nce binarization is a form of regularization': 570,\n",
       " 'ility of wi and the smoothed marginal probability of wj, respectively. We use α=0.75 as it is found to give better results vy . ': 571,\n",
       " 'NNs with low-precision weights, low-precision activations and even low-precision gradients, including but not limited to naryNet': 572,\n",
       " 'DNN for alignment. DNNs have beensuccessfully developed to detect alignments, e.g., inmachine translation and textreconstruction': 573,\n",
       " 'coversand a memory vector. F could be any LSTM that can combine two suchconcatenation vectors, such as Structure-LSTM ,Tree-LSTM': 574,\n",
       " 'um , Nesterov accelerated gradient , Adagrad chi . , 2011 and its extension Adadelta .Most recently, Adaptive Moment Estimation ': 575,\n",
       " '. In our work, we use pre-training but also use more powerful methods including graph-based semi-supervision bramanya and  ; u a': 576,\n",
       " 'and recent optimization algorithms such as Adam  further improves the learning speed.An AutoEncoder  is a type of Feed-Forward N': 577,\n",
       " 'in various natural language processing  tasks, including but not limited to chine slation , Syntactic Parsing , Text mmarization': 578,\n",
       " 'that aim to extract meaningful representations, but often come with an intractable inference step that can hinder their performa': 579,\n",
       " 'e of data than training against the binary label. In future work, we plan to compare against sentence classification using LSTMs': 580,\n",
       " 'decomposes the weight matrix into two low-rank matrices. e of these component matrices is fixed while the other is learned.Eleme': 581,\n",
       " ' such as depth/normal recovery , and high-level tasks such as keypoint prediction , object detection , and semantic segmentation': 582,\n",
       " ', and speechrecognizers.Parts of end-to-end systems, such as image featuresextracted by convolutional networks, often successful': 583,\n",
       " '93.681.548.087.293.4-CNN-multichannelWe evaluate our model on various benchmarks. Stanford timent Treebank  is a popular sentime': 584,\n",
       " 'apply GANs with the policy gradient method to dialogue generation. They train a Seq2Seq model as the generator, and build the di': 585,\n",
       " 'and question answering rmann . .They consist of two recurrent neural networks : an encoder that maps an input sequence of words ': 586,\n",
       " '.t us now describe the model in more detail. Figure 1 may help you in understanding the following paragraphs.The words from the ': 587,\n",
       " ', and we set the window size to 5, used a negative sampling rate to 10, and ran 5 epochs through unannotated corpora described i': 588,\n",
       " 'introduce byte pair encoding  for word segmentation to build translation models on sub-word units. Rare words are decomposed int': 589,\n",
       " 'ast layer as a feature map, or Image Embeddings input for simpler SVM classifiers.Another popular work was done a bit earlier in': 590,\n",
       " 'to strokes corresponding to crude sketchesha2017neural  to natural language in automated responses to userquestions vinyals2015s': 591,\n",
       " 'that have proved to fit well real data sets in massive networks , and at the theoretical level due to new phase transition pheno': 592,\n",
       " 'hat model the preference function, i.e. are one-sided, whereas BOPPER uses a GP to model the entire preference matrix. Following': 593,\n",
       " 'y, for bigrams  RCNN computes ht=f as followsRCNN has been shown to work remarkably in classification and retrieval applications': 594,\n",
       " '.This latter group of methods reflects the classic intuition of connectingbottom-up and top-down signals. Our work differs from ': 595,\n",
       " '. th the encoder and decoder LSTMshave two layers with 256 hidden neurons in each layer. We regularizedall LSTMs with a dropout ': 596,\n",
       " 'n. It might also be more biologically-plausible, since the gradient computations are much simpler.The original tch Normalization': 597,\n",
       " 'that if some of these assumptions are dropped one basically recoversthe result of the linear case, but the model is still unreal': 598,\n",
       " '.We compare our results only with work that is most closely related to ours.Techniques including the generation of test cases  o': 599,\n",
       " 'ictated by a small number of latent factors . In addition, since P is generated by Y via a one-sided sampling process, we follow': 600,\n",
       " 'efficient than its attention-free counterpart: it can achieve comparable results with far less parameters and training instances': 601,\n",
       " ', although these models tend to underestimate the true uncertainties, they can provide uncertainty estimations during model infe': 602,\n",
       " 'as the optimizer.For improved efficiency we use 8 workerswhich average the parameters after every update.See Appendix A for more': 603,\n",
       " ';and  empirically: the two top-performers of ImageNet use deep convolutionalnetworks with 19 and 22 layers, respectively  and .N': 604,\n",
       " 'l weights of a new network which we then trained on the second task.All neural networks were implemented in ras llet  and Theano': 605,\n",
       " 'In this work, we used majority voting to combine different feature spaces.All experiments were conducted using the Scikit-learn6': 606,\n",
       " ',dependency parsing ,named entity recognition ,and sentiment analysis .We evaluate phone vectors learned by Polyglot LMs intwo d': 607,\n",
       " '.To regularize, variational dropout  was used. The first and second model used dropout probabilities of 0.1 at input embedding, ': 608,\n",
       " ', sequence tagging , and translation .th w×h and Δw×hD are Cartesian products of probability simplices. To optimize over them, w': 609,\n",
       " 'ined in the paper could be compared to a global skip-gram model as opposed to a fixed window-size skip-gram model as proposed by': 610,\n",
       " 'or 1000r iterations, where r is the number of functions in the decomposition. Our choice is based on the empirical results of  .': 611,\n",
       " ', who were also testing their proposed algorithm for fully connected deep nets.ring each iteration, for each real image one synt': 612,\n",
       " '.To prove the first fact, note that the exploration distribution in Qμ is exactly Ux, sobecause ∑a∈AcalU=L since U is a distribu': 613,\n",
       " 'to optimize our models withhyper-parameters recommended by the authors . To alleviate the gradient exploding problem, werescaled': 614,\n",
       " 'nd stochasticity of natural dialogue.For example both goal-oriented dialogue systems  and sequence-to-sequence learning chatbots': 615,\n",
       " 'hing/exploding gradient and hard optimization when increasing the model’s parameters i.e. adding more layers.Zagoruyko & modakis': 616,\n",
       " 'ilarity. We experimented with two types of word representations as inputs to the network: the standard skip-gram word embeddings': 617,\n",
       " ' which extracts the diagonal elements of the argument matrix. The additional parameters used in Arch2 are updated as proposed in': 618,\n",
       " ', but have not been considered for modeling domain similarity. In line with previous work, we use a weighted sum of pre-trained ': 619,\n",
       " 'for implementation. The lengths of words, paragraphs, and documents are fixed at 24, 128, and 16 with necessary padding or trunc': 620,\n",
       " 'nd is trained in an unsupervised fashion.Using word sequences allows the model to improve over the earlier work of paragraph2vec': 621,\n",
       " 've achieved excellent results in semantic parsing , search query retrieval , sentence modeling , and other traditional NLP tasks': 622,\n",
       " '16. The model casts dialogue as a source to target sequence transduction problem modelled by a sequence-to-sequence architecture': 623,\n",
       " 'dy in the next final subsection.In this section, we show that increasing K increases the classification accuracy of the network.': 624,\n",
       " 'word representation learning methods have been used for various related tasks in NLP such assimilarity measurement , POS tagging': 625,\n",
       " 'has recently shown success. In cooperative settings, Tampuu .  have adapted deep Q-networks  .  to allow two agents to tackle a ': 626,\n",
       " 'which trims the vocabulary ofdifferent documents to the same length. At each decoding step themodel is trained to differentiate ': 627,\n",
       " 'cument i. The above objective is differentiable and can be minimized with stochastic gradient descent   or variants such as Adam': 628,\n",
       " ' frequency counts, we use the British National Corpus  which contains 100 million words of English from various sources.Word2Vec': 629,\n",
       " '. In , these CRNN’s were extended to accommodate multiple feature classes and the feature maps from CNNs were processed using a ': 630,\n",
       " 'N is straightforward, as it is simply a form of a CNN. The networks in our experiments all required only several lines of Theano': 631,\n",
       " 'or detect objects ba2014_multiple . Our model can be understood to generate a hard temporal attention mask on the fly given the ': 632,\n",
       " ', and others have argued, it is important for datasets to be sufficiently challenging to teach models the abilities we wish them': 633,\n",
       " 'eaders. Aggregation readers appeared firstin the literature and include Memory Networks , the Attentive er , and the Stanford er': 634,\n",
       " 'augments dynamic memory network with a new input fusion module and retrieves an answer from an attention based GRU. In concurren': 635,\n",
       " 'as it was found in  that they outperformed the ng-term Short Memory   units. wever, we do not utilize the stateful RNN training ': 636,\n",
       " 'and by expressing the objective function in terms of ximum tual Information to enhance the diversity of the generated responses ': 637,\n",
       " 'propose a system solely dedicated to learning these representations. These “embeddings” are dense vectors that not only uniquely': 638,\n",
       " 'and ishi . , each of which has released a set of large-scale closed-world question-answer pairs focused on a specific aspect of ': 639,\n",
       " '. Given an observed variable x, VAE introduces a continuous latent variable z, with the assumption that x is generated from z. V': 640,\n",
       " 'ed in generating high quality images. As approaches focusing on different network architectures, a recurrent network based model': 641,\n",
       " 'are to language modelling.In recent years, various models for learning cross-lingual representations have been proposed. In the ': 642,\n",
       " ' tokens, with the exception of a few special tokens.rther details are given in appendix B. 6The models are implemented in Theano': 643,\n",
       " ' machine translation , and parsing . These methods are based on 1 a fixed and carefully chosen set of basic units, such as words': 644,\n",
       " 'tination . Another interesting directionfor a future work would be to involve the integration of a model ofinstruction followers': 645,\n",
       " '. The Adam algorithm  with learning rate 2×10−4 is utilized for optimization. For both the LSTM and CNN models, we use mini-batc': 646,\n",
       " '. All training runs are early-stopped based on BLEUon the development set. As we observed in preliminary experiments better scor': 647,\n",
       " 'r relies on samples from the proposal distribution qΦ.To reduce the variance during inference, we follow the REINFORCE algorithm': 648,\n",
       " 'realistic outputs,against a discriminative model,whose goal is to distinguish the generator’s outputs from real data.wman .  and': 649,\n",
       " ' samples for each word. Formally, the negative sampling estimatesp following Equation 6:where σ=1/). The experimental results in': 650,\n",
       " 'pted variants.We used a similar pre/post-processing pipeline for ral MT asour phrase-based systems , and additionally appliedBPE': 651,\n",
       " '. The GRU was demonstrated to be better than LSTM in some tasks , and is formulated as follows :where ht, rt and zt are hidden a': 652,\n",
       " 'otential applications may be developed on relatively short timescales.ke most scientific theories, the SP system is not complete': 653,\n",
       " 'tiate it by other popular algorithms, including gradient descent , stochastic variance reduction  , and its accelerated version ': 654,\n",
       " '.We investigate the use of RL to fine-tune our question generation model.Specifically, we perform policy gradient optimization f': 655,\n",
       " 'which discovers the hierarchy using online learning algorithms, the construction of the tree being made during learning. Other f': 656,\n",
       " 'al  Cireşan .  reduces the total number of model parameters by randomly removing weights prior to training.-Rank Decomposition ': 657,\n",
       " 'enter digits in natural scene images,which is significantly harder than classification of hand-written digits.We follow the work': 658,\n",
       " 'ect category/word look-up tables and MLP parameters by minimizing the negative log-likelihood of the correct answer. We use ADAM': 659,\n",
       " 'to split the dataset into 598,388 training data, 6000 validating data and26, 032 testing data and preprocess the data by cal Con': 660,\n",
       " 'chitecture has been successfully applied tomulti-task learning in NLP such as part-of-speech tagging andnamed-entity recognition': 661,\n",
       " 'have also been used earlier for the problem of rare words in the context of machine translation , but the novel addition of swit': 662,\n",
       " 'feature vectors, we augment our bag-of-words representation with vectors built by n-gram filters max-pooled over the entire text': 663,\n",
       " 'with ρ=0.95, ε=1e−6 and gradient clipping at a norm of 10.The framework described in this paper is general, and we are intereste': 664,\n",
       " ' functions; this allows us to interpret the output as a probability.To train a deep neural network, the internal covariate shift': 665,\n",
       " '. Therefore, ∀  i∈1..NQ,j∈1..NS:where W5∈R1×h and b5∈Rare learned weights and biases, and σ the logistic function that introduce': 666,\n",
       " 'deeplearning framework to our task i.e. differentiating horizontal fromvertical shapes.For all experiments we started with an in': 667,\n",
       " 'introduced PixelCNN, a fully convolutional neural networkcomposed of residual blocks with multiplicative gating units,which mode': 668,\n",
       " '.This method works especially well on fully-connected layers, yielding ∼3x model-size compression however without notable speed ': 669,\n",
       " 't tothe quantities before the discretization  would be zero.Note that this remains true even if stochastic quantization is used.': 670,\n",
       " '. Ioannou .  . .Sparsity regularization and connection pruning approaches, however, often produce non-structured random connecti': 671,\n",
       " 'space. Ls= and LT= are the label vectors for auxiliary and test dataset to be predicted respectively.The possible textual labels': 672,\n",
       " '6×16 blocks of pixels drawn from the dak set1. We used the stochastic optimization algorithm Adam to facilitate the optimization': 673,\n",
       " 'with two criteria: gini purity and information gain and set min_samples_leaf=10 .5In addition, we performed Laplacian smoothing ': 674,\n",
       " 'of the data set , which contains400 64x64 grayscale images.ICNNs for face completion should be invariant to translationsand othe': 675,\n",
       " 'and ng .  proposed touse a character sequence as an alternative to the word-level one-hot vector. Asimilar idea was applied to d': 676,\n",
       " 'significantly outperformed the original Recurrent Attention Model proposed by    while leaving the rest of the systems identical': 677,\n",
       " '.To overcome these shortcomings, we propose a novel hybrid architecture for NMTthat translates mostly at the word level and cons': 678,\n",
       " ' models for structured prediction, unsupervised learning of generative models , and most recently, attention and memory networks': 679,\n",
       " ', which also computes policy gradient via back-propagation of value gradients, but optimizes stochastic policies.e appealing pro': 680,\n",
       " 'nctions, where recent techniques  have demonstrated competitive performance with batch techniques. In particular, recent methods': 681,\n",
       " ' we adopt negative sampling  for optimization. Negative sampling represents a simplified version of noise contrastive estimation': 682,\n",
       " 'proposed a hierarchical co-attention model for visual question answering, which achieved state of the art result on the COCO-VQA': 683,\n",
       " 'have become a popular method for measuring semantic relatedness in the biomedical domain. This is a neural network based approac': 684,\n",
       " 'ctors across domains; and in , multitask sequence-to-sequence learning is proposed for text translation.Also, architectures like': 685,\n",
       " 'he next utterance directly from the history of the dialog, using a recurrent neural network trained on a large corpus of dialogs': 686,\n",
       " '.The three methods presented in Section 2.2 achieve similar, state-of-the-art performance on analogy query tasks. In our experim': 687,\n",
       " 'an influence the performance of neural models. In our models, we use random initialized word embeddings as well as na embeddings': 688,\n",
       " 'gradients, and to make the TCL more robust to changes in the initialization of the factors, we added a batch normalization layer': 689,\n",
       " 'and ras llet , and all models are trained on Tesla K40 GPU.Our experiments are carried out on public datasets: CNN news datasets': 690,\n",
       " 'y have a heavy memory footprint. Moreover, in many deep neural network models parameters show a significant amount of redundancy': 691,\n",
       " 'versary. In summary, they emphasize that not all data is the same. Contrary to the massive 800,000 grasping dataset presented by': 692,\n",
       " ', Adadelta Zeiler , and so on.We used two corpora: the TED Talk corpus  and the nof the Day corpus6 . Note that we normalized wo': 693,\n",
       " 'es the hidden states, e.g. choosing the last state hTS. In practice it is found that gated RNN alternatives such as LSTM  or GRU': 694,\n",
       " 'to optimize the model with learning rate 0.001; thefirst momentum coefficient was set to 0.9 and the second momentumcoefficient ': 695,\n",
       " '.In this work, we go a step further and investigate how well recurrent neural networks can produce true hypotheses given a sourc': 696,\n",
       " 'the WLMs, it also needs a very large number  of parameters and cannot handle out-of-vocabulary  words.The e llion Word Benchmark': 697,\n",
       " '. Our architecture is related to deeply supervised networks  in that it incorporates classifiers at multiple layers throughout t': 698,\n",
       " 'e of interest recently inrepurposing sequence transduction neural network architecturesfor NLP tasks such as machine translation': 699,\n",
       " 'in that it lacks separate new-nonterminal tokens for different phrase types, and thus does not include the phrase type as an inp': 700,\n",
       " '. sed on matrix factorization,  . proposed a spectral word embedding method to measure the correlation between word information ': 701,\n",
       " 'we used Bleu for parameter tuning and evaluation.Bleu has been shown to correlate well with human judgment on the response gener': 702,\n",
       " 'to predict the next word with a softmax layer over the decoder vocabulary.where Wa, Ua, Wr, Ur, Vr and Wo are weight matrices.ou': 703,\n",
       " 'as well as sentence matching . A typical neural architecture to model sentence pairs is the “amese” structure , which involves a': 704,\n",
       " 'for experimental analysis. The CamVid dataset itself is split into 367training, 101 validation and 233 test images, and in order': 705,\n",
       " '. We collect the empirical statistics and estimate, at the beginning of a line search from xk,This amounts to the cautious assum': 706,\n",
       " 'showed that under mild conditions the denoising models learn the transition operator of a rkov chain whose stationary distributi': 707,\n",
       " 'l structure employed in these models, how the models were trained and  an empirical evaluation comparing thesemodels to those in': 708,\n",
       " 'dilated convolutionshave shown success in a number of tasks, includingimage segmentation  , speech synthesis and ASR .van den  .': 709,\n",
       " 'd 1,821 sentences, respectively.We use the average of the word vectors of a given sentence as a feature vector forclassification': 710,\n",
       " 'raining data comprises about 153,000 rman-Englishsentence pairs. In addition we considered a larger WMT14 English-French dataset': 711,\n",
       " 'her with the capacity to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies': 712,\n",
       " 'ty concept of convolutional neural networks  from Euclidean domain to graph case, using hierarchical clustering, graph Laplacian': 713,\n",
       " 'for tasks that involve understanding sentence semantics.These evaluations are performed by training a classifier on top of the e': 714,\n",
       " 'software library.The CPU implementation employed an Intel Core i7-5930K processor.re, the Intel Integrated Performance Primitive': 715,\n",
       " 'ide a model with more information about how to distribute probability mass among sequences that do not occur in the training set': 716,\n",
       " 'ing framework,DataGrad , was proposed to generalize adversarial training of deep architectures and help explain prior approaches': 717,\n",
       " 'we apply an IP layer to the input pose vector  and then tile the resulting output over the spatial dimensions to match the dimen': 718,\n",
       " '. DA obtained state-of-the-art performance on the NLI task which is why we chose it as our main approach for discourse connectiv': 719,\n",
       " 'g the properties.16In addition to the knowledge-based methods described above, we also adapt a neural sequence-to-sequence model': 720,\n",
       " 'ctor with fixed length. We have different modeling choices for this purpose, e.g., CNN  and RNN , while in this paper we use GRU': 721,\n",
       " 'of multiple task-specific expert networks into a single student network. The Policy stillation framework  and Actor-mic Networks': 722,\n",
       " 'orthermostats .  .  give acomplete classification of possible stochastic gradient-based MCMCschemes.Above, we analyzed propertie': 723,\n",
       " 'ctation with respect to the posterior distribution of ψ is approximated with Monte Carlo samples. A similar approach is taken in': 724,\n",
       " ' prediction may not result in a feasible solution , such that post-processing or heuristics such as beam search must be employed': 725,\n",
       " 'tor, which effectively avoids saturation.Another method is to train all three components together with a gradient reversal layer': 726,\n",
       " ', generating a k-best list g .  or mode approximation  and  . Then, standard stochastic gradient descent algorithms can be used ': 727,\n",
       " 'ay several 2D Atarigames  without the use of hand-crafted features. It hasalso been applied to simple tasks in thenecraft  andom': 728,\n",
       " ', without any knowledge on the syntactic or semantic structures of a language. wever, all of these architectures were only appli': 729,\n",
       " ', the conditional probability can be written as:withp=g,where I is the length of the target sentence and J is the length of sour': 730,\n",
       " '  we initialize all parameters uniformly in range . We use mini-batchstochastic gradient descent with batch size 100 and RMSProp': 731,\n",
       " 'the input window is related to its context or not.More recently, the need of full neural architectures has been questioned  &  ;': 732,\n",
       " 'he corresponding object names in language-baseddistributional semantic space. Given such paired training data,various algorithms': 733,\n",
       " ', there is a very recent model on sentence representation with dynamic convolutional neural network . This work relies heavily o': 734,\n",
       " 'ain the pooling weights. Later, we train the classifier on top of the concatenation of the trained, partial models.As opposed to': 735,\n",
       " 'ur model onthe validation set and chose the best performing model for the testset. We trained our models with the optimizer Adam': 736,\n",
       " ', and trains a logistic regression to distinguish between data samples of vn from “noise” distribution. Our objective is to maxi': 737,\n",
       " 'ading comprehension .Our work belongs to text based QA where the answer is a sentence.In recent years, neural network approaches': 738,\n",
       " '.Most methods for solving this task require training a statistical modelon a dataset of  pairs.The model is usually trained to m': 739,\n",
       " '. From the various classification algorithms we tested, Random Forests performed best with our selection of features .For the re': 740,\n",
       " ', the authors introduced several techniques to improve the performance. Negative sampling is introduced to speed up learning, an': 741,\n",
       " ' we used 100 feature maps for the baseline CNN, and 20 for all the other CNN variants. For parameter estimation we used ADADELTA': 742,\n",
       " 'surebroader passage understanding.Of course, text understanding can be tested through other tasks,including entailment detection': 743,\n",
       " 'n applications such as machine translationnrich . ;  . , conversational response generation and  ;  . ,abstractive summarization': 744,\n",
       " '. We attribute this advantage to our end-to-end feature embedding learning and its large margin nature, which directly translate': 745,\n",
       " '. Some new attention mechanisms  recently proposed are shown to be helpful on reading comprehension dataset where the answer to ': 746,\n",
       " 'NLP,attention-based CNNs have been used incomputer vision forvisual question answering , imageclassification , captiongeneration': 747,\n",
       " 'tation. There are theoretically motivatedguidelines for setting and scheduling the hyper-parameter √βto achieve sublinear regret': 748,\n",
       " 'model that accounts for this uncertainty.Specifically, our contributions are as follows:We employ the popular child-sum TreeLSTM': 749,\n",
       " 'ivation for using convolutions as our feature extraction component flows from similar approaches in natural language processing ': 750,\n",
       " 'are a form of distributional representations for the words in a vocabulary, where each word is expressed as a vector in a low di': 751,\n",
       " 'show that character-based representations of words help improve POS tagging and dependency parsing performances. So, we also use': 752,\n",
       " 'rent from maxout ). Our model design is similar to VGGNet vggnet  where 3×3 filter sizes are used, as well as Network in Network': 753,\n",
       " '.The text feature of each utterance was formed by concatenating the word embeddings for all the words in the sentence and paddin': 754,\n",
       " 'to extract the answer span from the context. newcite:2016uq propose the Dynamic Coattention Network, which uses co-dependent rep': 755,\n",
       " 'xtracted nouns using TreeTagger  from the list of translated phrases and discovered 3,099 total nouns.We then extracted word2vec': 756,\n",
       " '. While we do not study these models in the present work, the CBT would be ideally suited for testing this class of model on sem': 757,\n",
       " ' to an MLP to compute the matching score.We implemented all baselines and KEHNN by an open-source deep learning framework Theano': 758,\n",
       " ' to derive character-based representations.Optimizer. Besides Stochastic Gradient Descent , we evaluate Adagrad chi . , Adadelta': 759,\n",
       " ', and inherits ideas from the extensive literature on semantic parsing  . ;  . ;  . ; irk . ; ang . ;  .  and program generation': 760,\n",
       " '. The Wikipedia model got trained on 1 billion words resulting in a vocabulary of size of≈120,000 words and word vectors of 250 ': 761,\n",
       " 'ent estimators based on REINFORCE trick  suffer from high variance. Although some variance reduction remedies have been proposed': 762,\n",
       " 'that takes a document as input and generates aquestion  conditioned on an answer  as output. To address the mixed extractive/abs': 763,\n",
       " 'ncies per dataset?As feature side-information we use the word2vec representation of the words which have a dimensionality of 300': 764,\n",
       " 'rrection is computed on these momentsThe vector ^vt represents an approximation of the diagonal of the Fisher information matrix': 765,\n",
       " '.The key contribution of this work is that we propose Attentive ing ,a two-way attention mechanism, that significantly improves ': 766,\n",
       " ', which automatically computes the learning rate in each epoch, was used for training with a mini-batch size of 20 utterances. E': 767,\n",
       " 'allel corpus between all the languages involved, which isdifficult to obtain in practice. Most closelyrelated to our approach is': 768,\n",
       " '.With those pre-trained embeddings, we transform S and T into sentence matrixesS= and T=,where si and tj are d-dimension vectors': 769,\n",
       " 'at EMNLP, and,value iteration networks  at NIPS.lly and   was the recipient of Test of Time Award at ICML 2017.In 2017, the foll': 770,\n",
       " 'and biomedical domain .LSTM - We also compare against RNN  classifier which is similar to the models used in  .These methods for': 771,\n",
       " 'ormance on a wide range of complex data problemsincluding speech recognition , image recognition  and naturallanguage processing': 772,\n",
       " 'in, the GRUscan be trained in the same way.Convolutional neural networks  have also been successfully applied tovarious NLPtasks': 773,\n",
       " ', part-of-speech tagging , and text classification . The main advantage of the character-based approach is its language-independ': 774,\n",
       " '. We use sentences of length up to 50 subword symbols for MLE training and 200 symbols for trainable decoding. For validation an': 775,\n",
       " '.We used 40 mel filter bank energies as features along with theirfirst and second order derivatives. Decoding and evaluation was': 776,\n",
       " 'andvisual QA .Most previous approaches to visual question answering either apply a recurrentmodel to deep representations of bot': 777,\n",
       " 'd achieves optimalconvergence rate with only one pass over the data and a constant learning ratewith a logistic regression model': 778,\n",
       " '.This raises a central open question: are such functions merely rare curiosities, or is any function computed by a generic deep ': 779,\n",
       " '. This approach is informative because the neurons in a layer interact with each other to pass information to higher layers, and': 780,\n",
       " 'et knowledge from a resource-rich to a resource-lean language?To investigate Q1, we induce standard distributional vector spaces': 781,\n",
       " ', which means that for 4 consecutive steps the agent will use the same action picked at the beginning of the series. For this re': 782,\n",
       " 'earning multilingual sentence representations.There are several work on learning multilingual representations at document levels': 783,\n",
       " 'is the state-of-the-art model on several datasets. It employs a gating mechanism to represent document which is query-specific i': 784,\n",
       " 'f the memory,  updates its own hidden state vht, and  outputsa value vyt  In this paper, we use both a gated recurrent unit GRU,': 785,\n",
       " ':ICCV15MTRNN ; cite:ICLR17MTLTF , where significant performance gains have been witnessed. Most multi-task deep learning methods': 786,\n",
       " 'ut do not.We trained both the analytical and the stochastic models, as well as all baselines against which we compare,using Adam': 787,\n",
       " 'learning rule with learningrate 10−3.As IMDB is a fairly simple dataset, we observe little performancedegradation even when quan': 788,\n",
       " 'compares the document encodings with the question encodings. This network processes the document sequentially and at each token ': 789,\n",
       " 'and are capable of producing rich sentence representations .1ving produced a representation r for each mention m, a slot-specifi': 790,\n",
       " 'generally employ a gated unit  as the activation function in the decoder.e might suspect that the context gate proposed in this ': 791,\n",
       " 'ns.The FIGMN algorithm was compared to other 3 algorithms with high scores on OpenAI Gym: rsa, Trust Region Policy Optimization ': 792,\n",
       " ', in which we try to predict the following sequence rather thanrecite the current sequence.Another approach we would like to inv': 793,\n",
       " 'age model with global context.  .  propose to use parallel data for WSI and learning word sense embeddings.  .  extend Skip-gram': 794,\n",
       " 'x operation system on Intel Edison computing platform . Far fairness, all compressed deep learning models are run through Theano': 795,\n",
       " 'gnition, and sentence generation from images .Also relevant is the sequence-to-sequence model used for machine translation by  .': 796,\n",
       " 'where the variance is assumed to be bounded, that is k≤1 for all x∈X.At iteration T+1, given the previously observed noisy value': 797,\n",
       " 'd thus its weights, are huge as we will see now.The learning rate for the network’s parameter was governed by the Adam optimizer': 798,\n",
       " '. In this paper, we focus on existingresearch interested in decreasing the training time, as these approaches arecloser to the p': 799,\n",
       " 'ng pretrained or end-to-end learned word representations. We offer comparisons with both using the pretrained word2vec embedding': 800,\n",
       " 'for a complete description of the A3C algorithm used. P genotypes pathways are initialized randomly, each genotype is at most a ': 801,\n",
       " 'collision is much less severe for sparse feature vectors and can be counteracted through multiple hashing  or larger hash tables': 802,\n",
       " '. The questions and answers are human-generated.The answer to each question is determined by two pointers in the passage, one po': 803,\n",
       " ', which we found to be beneficial when using MCB for the grounding task.We evaluate the benefit of MCB with a diverse set of abl': 804,\n",
       " '. The version with beam search optimization showssignificant improvements on all three tasks, and particularimprovements on task': 805,\n",
       " ' 17.41% on CIFAR-100 using large DenseNets, far outperforming the record of 19.25% under the same training cost and architecture': 806,\n",
       " '.For each minibatch, we normalize the mean and variance of the textual facts and then scale and shift to match the mean and vari': 807,\n",
       " 'rove NMT performance, showing that using SMT knowledge helps NMT. wever, the improvement seems less significant than reported in': 808,\n",
       " 'for f, and for q an attention mechanism that defines the context vector as a weighted sum over encoder hidden states  . ; ong . ': 809,\n",
       " '; see also    for an introduction to cnns, and  and   and  .For Wikipedia comments, we use a ‘narrow’ convolution layer, with ke': 810,\n",
       " 'composed of five convolutional layers and three fully connected layers. This model performed 19.6% top-5 error when a single cen': 811,\n",
       " 'inducesvector-based word representations by trying to predict a target wordfrom the words surrounding it within a neural network': 812,\n",
       " 'and the lack of true reference responses from persona models,we evaluated the quality of our generated text with a set of judges': 813,\n",
       " 'proposed a word-level shallow neural network with one convolutional layer using multiple widths and filters followed by a max-po': 814,\n",
       " '. This results in a waste of both computation and memory. There have been various proposals to remove the redundancy:  .  explor': 815,\n",
       " 'an traditional networks and they do not scale easily to a large memory. End-to-End Memory Networks  and y-Value Memory Networks ': 816,\n",
       " 'g the pixel-wise loss encourages the model togenerate the average of plausible solutions, thus leading to poorperceptual quality': 817,\n",
       " 'section we test our proposed method in a cross-modal setting,mapping images to word labels and vice-versa.We use the data set of': 818,\n",
       " 'and restaurants Personalizing dialogue systems requires sufficient information from each user and a sufficient user population t': 819,\n",
       " 'ving ubiquitous success in wide applications, ranging from computer vision , to speech recognition , natural language processing': 820,\n",
       " 's of sentences and other short pieces of text,including, for example,recursive neural networks  andconvolutional neural networks': 821,\n",
       " ', using an unsupervised learning algorithm. nce the performance of single-layer learning has a big effect on the final represent': 822,\n",
       " 'tion layer and a non-linear hidden layer to jointly learn the word vector representations and a statistical language model.  and': 823,\n",
       " ', Pearson’s r are showed. We then compare our method against the performance of supervised models in  As we can see from the tab': 824,\n",
       " 'yer is added on top of W with a negative loglikelihood objective for training.Our model uses the idea of attention input-feeding': 825,\n",
       " 'former computes the weights according to question information.The major difference between this approach and the one in  is that': 826,\n",
       " 'both in the number of evaluations it takes to converge and the value reached. As an additional bonus, the method finds good solu': 827,\n",
       " ', and thedifferences in terms of the feasible sets are shown in Figure 3.Denote G=12∥y||22−12∥λ2θ−y||22.The SAFE approach makes ': 828,\n",
       " 'with GAE . After training our policy, we evaluate our policy’s performance on these goals; this performance is used to determine': 829,\n",
       " 'introduced several heuristics to encourage exploration, including:  raising the temperature of the proposal distribution,  regul': 830,\n",
       " 'and speech recognition .Recently, g .  proposed an architecture that modifies the standard LSTM by replacing the memory cell wit': 831,\n",
       " ', network embedding  and user representations .To the end, we propose a novel ICE model that learns the joint representations of': 832,\n",
       " 'have sparked a series of latent variable models applied to NLP  .For models with continuous latent variables, the reparameterisa': 833,\n",
       " ', where it is shown that quite generally, if the number of neurons in the penultimate layer is larger than the data size, then g': 834,\n",
       " 'ation classification can also be divided intohandcrafted feature based methods Rink ; Kambhatla  andneural network based methods': 835,\n",
       " ' showed that wide residual networks (WRNs) are superior over the commonly used narrow and very deep counterparts (original Nets)': 836,\n",
       " ' with a single rollout, so ateach step we update θl in the direction⋅logpfor a single z∼p. θe and θl are optimized usingadadelta': 837,\n",
       " ', relation classification  and,similar to this setting,event detection .We use  ’s open-source CNN implementation,12where a logi': 838,\n",
       " 'that provide richer region-to-phrase correspondence annotations. In addition, the fusion of object counts and spatial informatio': 839,\n",
       " 'with learning rate 10−3 for 50 epochs to train it. We set J=10−3 and τ0=500 along with an l2 regularization of 10−5. As Fig. 2 s': 840,\n",
       " 'hs in the past few years.Deep convolutional networks have been instrumental in speechrecognition  and natural languageprocessing': 841,\n",
       " 'grammar of questions and answers, while allow the sharing of the word-embeddings.For the dataset,  adopt the dataset proposed in': 842,\n",
       " 'amic Ensemble of DenseNets*. ImageNet, we compare MSDNet with five  Nets models described in Section 5.1, the 121-layer DenseNet': 843,\n",
       " 'ions of entities and documents to address NED.Blanco .  proposed a method to map entities into the word embedding i.e., Word2vec': 844,\n",
       " ', though we will focus on n-step returns here.sed on the definition of gγ above, the natural choice of variance-reduced estimato': 845,\n",
       " '. The batch size is set to 50 examples. The network is trained for 25 epochs with early stopping, i.e., we stop the training if ': 846,\n",
       " 'study the recovery of denoisingauto-encoders. They assume that the top-layer values of the networkare randomly generated and all': 847,\n",
       " '/alignment was qualitatively visualized in  and quantitatively evaluated in  using the alignment error rate.In image captioning,': 848,\n",
       " 'ntences. Other evaluation methods require word ontologies linowski and  2014, multiple choices  . 2015;  . 2015, or human judges': 849,\n",
       " 'for machine translation. This baseline decodersearches for the most likely output sequence using a simple left-to-right beam sea': 850,\n",
       " ', image captioning  and speech recognition .Recently, g .  proposed an architecture that modifies the standard LSTM by replacing': 851,\n",
       " '. In case there is no ground-truth information regarding similar/dissimilar pairs , random sampling is typically used to generat': 852,\n",
       " '. The λ for max pooling-based and label assignment-based MIL are 1×10−5. The λ and μ for sparse MIL are 5×10−6 and 1×10−5 respec': 853,\n",
       " 'form the convolution. It also provides two additional implementations for the convolution operation, an FFT-based implementation': 854,\n",
       " '.2 For each dataset, we use approximately 1.2M tokens to train, and approximately 150K tokens each for development and testing. ': 855,\n",
       " '. By comparing line 6 and line 7, we see that the query attention mechanism allows improvements up to 2.3 points in validation a': 856,\n",
       " 'and an output network. The input network takes the incoming information images , message  and the agent’s previous action , embe': 857,\n",
       " 'onding weights in the input-to-state convolutions after each update. milar masks have also been used in variational autoencoders': 858,\n",
       " ', retrieved by their IR model from the ClueWeb09 data source, to extract the answer.The other four datasets we consider are: SAD': 859,\n",
       " '.r perplexity indicates a better model.In our steganographic LSTM, we cannot use this metric as is:since we enforce p=0 for wi∉': 860,\n",
       " '.nctionally, if feedforward nets and highway nets are functionalapproximators for an input vector, CLN can be thought as anappro': 861,\n",
       " '.The features used in our text classification system are shown below:n-grams: presence or absence of contiguous sequences of 1, ': 862,\n",
       " 'as well as our own re-implementation, which appears to be the new state-of-the-art on this dataset.The gap between our greedy mo': 863,\n",
       " 'with tuned hyperparameters. We found the suggested learning rate of 0.001, to be too high, using 0. instead. Additionally, we fo': 864,\n",
       " 'or are unbiased but higher variance . As Prop is unbiased and has relatively low variance due to its use of backpropagation for ': 865,\n",
       " ', use a purely feed-forwardhierarchy that maps the input image to a set of features which arepresented to a simple classifier. A': 866,\n",
       " 'is an excellentframeworkto buildon. retto takes a trained model as input, and automatically brews a condensednetwork version. In': 867,\n",
       " ', vectornets struggle to learn when the depth goes beyond 20, as evidencedin Fig. 2. The erratic learningcurves of the vector ne': 868,\n",
       " ' yeed .  are similar to roni and nci’s, but their DSMs were built by using the roles assigned by the SENNA semantic role labeler': 869,\n",
       " 'ate on the technical support response generation task for the Ubuntu operating system.We use the well-known Ubuntu alogue Corpus': 870,\n",
       " '. The algorithm uses  importance sampling  to approximate the probability distribution of words over a large target vocabulary w': 871,\n",
       " ' proper inversions, just not particularly interesting ones.e approach to solve this is to employ adversarial learning algorithms': 872,\n",
       " '. As we will show later, our proposed deep conflation model achieves high prediction accuracy in the conflation task for busines': 873,\n",
       " ', and the Attention-over-Attention er . In thissection we define explicit reference readers more specifically by equation  below': 874,\n",
       " ', and were trained on the concatenationof ukWaC  and a 2013 dump of the English Wikipedia.The spaces, which we refer to as s2, s': 875,\n",
       " ', we use a second hash function b to remove bias induced by hashing. This is a signing function, i.e., it maps  tuples to {+1,−1': 876,\n",
       " 's because of the vast quantity of possible decisions. Thus, there has been a growing interest in applying encoder-decoder models': 877,\n",
       " 'were shown to be very successful in end-to-end question answering  . Some new attention mechanisms  recently proposed are shown ': 878,\n",
       " 'asks with real users.newciteRE train an end-to-end RL dialogue model using human users.alogue quality is traditionally evaluated': 879,\n",
       " 'and optimize over the whole batch of training words. We use a robi7 solver for the ILP.Morphological family clustering is the ta': 880,\n",
       " 'y we use temporal convlution neural network for event span and attribute classification is similar with the approach proposed by': 881,\n",
       " 'where it is combined with an attention mechanism in a way similar to SliceNet.Depthwise separable convolutions were first studie': 882,\n",
       " 'N in Figure 3 by a GRU while keeping other parts unchanged.Variant-II:w tomodel attentionat the granularity of wordswas shown in': 883,\n",
       " '. And we modify the architecture to make it suitable to the text classification tasks. Table 1 shows the configuration of each l': 884,\n",
       " 'proposed negative sampling to speed up skip-gram training. This approximates Eq.  using sigmoid functions and k randomly-sampled': 885,\n",
       " 'iques, it has been reported that the performance of NMT can surpass the performance of traditional SMT as measured by BLEU score': 886,\n",
       " '.Unlabeled data contains a lot of abstract syntax and semantic information that can be very useful to NLP tasks.In order to take': 887,\n",
       " ' are randomly initialized from .We then ran 5 additional epochs until the perplexity on the development set stabilized.Following': 888,\n",
       " 'e solvent accessibility. Defined as the amino acid having more than 0.15 Å ofaccessible surface area.We use yesian Optimization': 889,\n",
       " ' normalization on the output scores fv and ft along the batch-wise direction. It can be viewed as a mixture of tch Normalization': 890,\n",
       " ' embedding of the true output and the predicted embedding. This approach is inspired by the distributed representations of words': 891,\n",
       " 'equences.We address the task of polyphonic music prediction, using the datasetsPiano-midi.de, Nottingham and seData described in': 892,\n",
       " '.  .  successfully decompose all the layers by using cker decomposition. wever, cker decomposition does not seem to compress as ': 893,\n",
       " 'with a learning rate of 0.001 .5 The batch size is set to 80. Between layers we apply dropout with a probability of 0.2, and in ': 894,\n",
       " ', joint , RC-Net  and CBOFP . For the first four, we got their public online available implements by the authors. We used the im': 895,\n",
       " ', thispaper for the first time introduces latent variables without requiringback-propagation for training.Figure 3 shows generat': 896,\n",
       " 'cture of conversations through hierarchical networks  . . ral conversation models can also be learned using adversarial learning': 897,\n",
       " '. wever, these works do not necessarily discuss how these temporal hierarchies of options, skills or macro-actions play with int': 898,\n",
       " '.For language modeling evaluation, we train our model on the training set from the CoNLL 2012 dataset with coreference annotatio': 899,\n",
       " 'on of Net , whose layers are linked through “skip connections”.Nowadays, nearly all state of the art convolutional networks e.g.': 900,\n",
       " 'ely used in the sequence to sequence learning  framework whose applications ranges from machine translation , text summarization': 901,\n",
       " ' a teacher network before obtaining the student TNN.For the teacher network, we use a modified version of narized NN’s algorithm': 902,\n",
       " 'sue, we propose a very efficient optimization algorithm that solves problem  based on exact cyclic coordinate descent . Although': 903,\n",
       " ' as language generation – egimagecaptioning dai2017towards ; ttyRHFS17 , dialog generationli2017adversarial , or text generation': 904,\n",
       " 'tion such as binary attributes or text. It is usually framed as a modality transfer problem and solved through transfer learning': 905,\n",
       " 'els which use measures computed outside the vector spacee.g. symmetric measures LIN , asymmetric measuresPrec , balAPinc , invCL': 906,\n",
       " ' over the 20 random samples; and we get an accuracy of 55.8% that is above moststate of art results, including GFK  , SA  , MMDT': 907,\n",
       " ' regret bounds for PSRL and even argues for the potential benefits of sampling-based methods over existing optimistic approaches': 908,\n",
       " 'for our implementation.Training Parameters Model 1 and Model2 were trained by back propagation with stochastic gradient descent.': 909,\n",
       " ',linguistic coverage model .None of them were able to improve the systems’ performance, so we donot include them in our submissi': 910,\n",
       " 'chain of novel NN-based LMs continued with more complex and advanced models such asConvolutional ral Networks   and autoencoders': 911,\n",
       " 'performance of these mimic-ing shallow networks are still not quite as good as the deep networks or ensembles they are mimic-ing': 912,\n",
       " '.games – at the end, there are many games that don’t have tutorials nor walkthroughs. We downloaded a big collection of games, d': 913,\n",
       " 'uses generative convolutional neural networks  to directly hallucinate RGB pixel values of synthesized video frames. While these': 914,\n",
       " 'he probability of an image.Thus, we calculate a lower bound of the true log-likelihood using the methods described in prior work': 915,\n",
       " '.For instance,  .  propose a “retrofitting” technique consisting in a leveraging of lexicon-derived relational information, name': 916,\n",
       " '.Unless otherwise noted, significant testing is performed on one-word-one-vector embedding  versus multi-sense embedding using E': 917,\n",
       " 'draws quite a lot of attention from the community due to its simplicity and effectiveness. An interesting result given by word2v': 918,\n",
       " ' the distributionfunction which generates by the i-th row of W. The optimization functioncan get F by using word embedding model': 919,\n",
       " 'ssifier output increases from K to K+1. The configuration of our generator network G is inspired by the architecture proposed in': 920,\n",
       " 'ata from a different behavior than the one being followed, which can provide many benefits: efficient parallel exploration as in': 921,\n",
       " ' atsignificant scales in the context of multilabel classification multilabel learning, with missing labels, is also addressed in': 922,\n",
       " '.Typically, NMT adopts the encoder-decoder architecture which consists of two recurrent neural networks. The encoder network mod': 923,\n",
       " ', the space itself is not semantically well organized – the particular dimensions of the latent vector do not correspond to sema': 924,\n",
       " ' increase of model size.The training of large-scale models with huge amounts of data are often carried on distributed systems  .': 925,\n",
       " ', GMemNN uses an explicit memory module as well as an adaptive gating mechanism to learn to attend to relevant memories. The QRN': 926,\n",
       " 'tor was only fed into the RNN once at the first time step in , while it was used at each time step of the RNN in .Most recently,': 927,\n",
       " 't progress in a diverse set of machine learning applications, ranging from computer vision  to natural language processing tasks': 928,\n",
       " 'onstrate our framework by applying it to assess thesecurity of SVMs.We discuss our recently devised evasion attacks against SVMs': 929,\n",
       " 'or cross-lingual Vulić .  representations. These methods show good performance on comparison tasks. They additionally demonstra': 930,\n",
       " 'e 32classes prescribed in the original data, and are ignored during evaluation. Weused the same subset of 11 class categories as': 931,\n",
       " 'to investigate variable-size filters in a convolution layer. Compared to , MVCNN has rich feature maps as input and as output of': 932,\n",
       " ', demonstrated domain transfer using features learning in JoCo simulation with a Kinova arm to the physical system—the reaching ': 933,\n",
       " 'it would be better to fine-tune  the parameters.INIT is also related to unsupervised pretraining such as word embedding learning': 934,\n",
       " 'itting and further improves the results . From rows 12 and 13 we can see that the performances of LSTM and GRU cells are similar': 935,\n",
       " 'amend this model to a conditional NADE layer:where we use W:,<i to refer the sub-matrix consisting the first i columns of W.Vari': 936,\n",
       " '1,1,1,100,150} in this study but it will be wp={1,1,1,1,1,1,1} for unweighted nonlinear least-squares estimator.Using the Caffe ': 937,\n",
       " '.In Section 4.1, we describe two standard knowledge base completion datasets.These consist of single-edge queries, so we call th': 938,\n",
       " ' learners on two problems from this sub-field: a classification domain called dmine  and a regression domain called ndon schools': 939,\n",
       " 'are inspired by similar strategies in computer vision , and have also been recently adopted in NLP for recurrent neural networks': 940,\n",
       " ', where text in the input has been lowercased and stemmed using NLTK , and matching entities appearing in the same input-output ': 941,\n",
       " 'compressing all the external sentences into a fixed-length vector, is typically too small to accurately remember facts from the ': 942,\n",
       " 'for dialog systems.biseq2seq. Another component in our approach, adapted from , which is essentially a seq2seq model extended wi': 943,\n",
       " 'ior distribution is to approximate it with a recognition  network, anda variational lower bound is then optimized ma & Welling ;': 944,\n",
       " 'proposes a method to choose good learning rate for SGD, which relies on the square norm of the expectation of the gradient, and ': 945,\n",
       " 'by reducing internal covariateshift. Another solution is the residual learning framework proposedin  . , which employs the resid': 946,\n",
       " '.ssively parallel graphics processing units  became available in the last few years. nce then, Convolutional NNs  trained bygrad': 947,\n",
       " 'ell. This also serves as an alternative explanation for the gains that other related workhave found with the “word-dropout” idea': 948,\n",
       " 'espectively, following the notation in Eq.1.For node-node links, we specify SP as Enn. Inspired by the idea of negative sampling': 949,\n",
       " '.As a test case, we train a NN with a single hidden layer of30 units over the MNIST data set. We adopt the experimental setup of': 950,\n",
       " 'dary similar to .Comparisons. DPN is compared with the state-of-the-art segmentation methods, including FCN , Zoom-out , DeepLab': 951,\n",
       " 'only used L2 regularization. newcitewieting-16-full also regularized the word embeddings back to their initial values. re we use': 952,\n",
       " 'entification etc involve modelling a pair of sentences so that they perform well on a particular task or multitude of such tasks': 953,\n",
       " '. wever, finetuning of upper levels might be necessary if the input scale changes dramatically or the original model was not des': 954,\n",
       " ' for extracting position and velocity, as well as work for extracting latent object properties  ., 2015, 2016. Therefore this pa': 955,\n",
       " ', 2014; we chose values of p smaller than or close to the minimal values for which the algorithm of SDP relaxation. We use a sim': 956,\n",
       " 'ing cost.Because of the simplicity, our method is also easy to extend with other defense strategies such as adversarial training': 957,\n",
       " '. Word embedding is directed to LSTM controller within NTM, and the output of NTM is generated from softmax layer where each bit': 958,\n",
       " 'and all convolutional filters are of size 3×3.To compress the network we replace each convolutional layer excluding the first on': 959,\n",
       " 'izing the individual filters in the network. In this work, we apply the visualization techniques proposed by Zeiler and   and  .': 960,\n",
       " '. Intuitively, we expect similar verbs to have similar distributions of nearby nouns and adverbs, which can greatly help us in z': 961,\n",
       " 'with a learning rate of .01 as well as standard random initialization. We train each pair of agents for 2000 rounds and look at ': 962,\n",
       " 'showed that most latent factor models can be modified to learn from paths rather than individual triples which improves performa': 963,\n",
       " 'and GloVe. wever, there are manyother resources that are undoubtedly useful in NLP,including lexical resources like WordNet and ': 964,\n",
       " '.Recent work has begun using more flexible and reliable evaluation metrics;automatic prediction of human ratings  .  is one such': 965,\n",
       " '.Vθ,πθ are parametrized as the same neural network as A2C, where convolutional layers and the first fully connected layer are sh': 966,\n",
       " ' Imagenet Russakovsky . . Second, the features from the low to mid-levels have been shown to transfer well to a variety of tasks': 967,\n",
       " 'tion coefficient and M is the index of the null API.The training process of the proposed model is summarized in Algorithm 1.Adam': 968,\n",
       " ' deep architectures. ch an approach has been applied in a variety of contexts for training very deep networks in computer vision': 969,\n",
       " 'g attention to various distributed representations in continuous vector spaces.In the computer science literature, Skip-gram/CW ': 970,\n",
       " '. wever, unlike the standardalignment techniques, our model learns to align based not only on thehigh-level input abstraction, b': 971,\n",
       " ', it is much less convenient than using automatic metrics. rthermore, we believe that current text generations are sufficiently ': 972,\n",
       " ' the stochastic gradients, while in prediction we use 20 samples for predicting document perplexity.The model is trained by Adam': 973,\n",
       " 'ng models, that comes with an intuitive programming model , and that is scalable. Popular deep learning systems, including Caffe': 974,\n",
       " ' quality of language models rtin and rafsky ,and it is defined as the average per-word log-probability on the valid data set:exp': 975,\n",
       " 'on an order of magnitude more data and neural network parameters. In particular, we want to see what other “grandmother neurons”': 976,\n",
       " ' language processing applications such as named entityrecognition, chunking, paraphrasing, or sentimentclassification .Recently,': 977,\n",
       " 've duringtraining, due to the need to sample multiple interaction sequenceswith the environment.  the other hand, the DRAW model': 978,\n",
       " ';and  the sequential program, a discrete sequence of steps, each selected from a discrete set of simple, approximately-discrete ': 979,\n",
       " 'to compute similarity between each story for a given sequence. Skip-thoughts provide a tence2Vec embedding which models the sema': 980,\n",
       " 'and  introduced end-to-end neural speaker verification systems, combining all three steps.In this paper, we extend the end-to-en': 981,\n",
       " '. .  train multiple value functions and makes use of bootstrapping and Thompson sampling for exploration. ny approaches measure ': 982,\n",
       " 'he sense with the largest Q-value .ε-Greedy: selects a random sense with ε probability, and adopts the greedy strategy otherwise': 983,\n",
       " 'optimized separate regularization parameters for each layer in a neural network, and found that it improved performance.We can t': 984,\n",
       " 'ies, items, and weapons. A level editor can be used tocreate custom tasks. We use the Deathtch task defined in the Gymcollection': 985,\n",
       " '.t G= be a directed graph with vertex setV={1,...,K}, and arc set D. t N−i be the in-neighborhoodof node i, i.e., the set of nod': 986,\n",
       " 'codethe entire input into the network’s hidden state vector and then, in a secondstep, decode the entire output from this vector': 987,\n",
       " ' a short clip with a single sentence.  first proposed to describe videos using an LSTM, relying on precomputed CRF scores from .': 988,\n",
       " 'searches for the best suited algorithm selection approach  for a given algorithm selection scenario usingalgorithm configuration': 989,\n",
       " 'bedding similarities and human judgments . In addition to the skip-gram and  and  models, we also compare against the CBOW model': 990,\n",
       " 'ity over standard attention-based NMT.1The past several years have witnessed the rapid progress of end-to-end ral chine slation ': 991,\n",
       " 'se for the paraphrase detection is the crosoft Paraphrase Detection Corpus  . . We follow the same feature engineering idea from': 992,\n",
       " 'with the network architecture and hyper-parameters as in : the encoder and decoder networks consist of 1000 gated recurrent unit': 993,\n",
       " 'with default parameters, which we found to converge more quickly and effectively than SGD. We used a mini-batch size of 1000.The': 994,\n",
       " ', learn an inference model to map images to a latent space and a decoder to map from latent space back again to image space. Unf': 995,\n",
       " ', and are a default tool for the understanding of many physical and chemical systems.Despite these successes and ongoing advance': 996,\n",
       " 'tely isolated from each other, which has been implicitly usedas a fundamental assumption by most prior transfer learning studies': 997,\n",
       " ', instead of an MLP.Gated recurrent units  are a neural architecture that parameterizes the recurrence equation in theRNN with g': 998,\n",
       " ', focus on the planning problem associated withnavigation under full state observation , designingstrategies for faster learning': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_citated_text  = df['#1 String'].values.tolist()\n",
    "right_citated_text = df['#2 String'].values.tolist()\n",
    "total_citated_text = list(set(left_citated_text + right_citated_text))\n",
    "citated_voca = {}\n",
    "left_citated_id = []\n",
    "right_citated_id = []\n",
    "for i, v in enumerate(total_citated_text):\n",
    "    citated_voca[v] = i\n",
    "citated_voca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l, r in zip(df['#1 String'], df['#2 String']):\n",
    "        left_citated_id.append(citated_voca[l])\n",
    "        right_citated_id.append(citated_voca[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['#1 ID'] = left_citated_id\n",
    "df['#2 ID'] = right_citated_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_citated_text</th>\n",
       "      <th>right_citated_text</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>target_year</th>\n",
       "      <th>target_author</th>\n",
       "      <th>source_author</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1409.3215v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>ilya sutskever;oriol vinyals;quoc v le</td>\n",
       "      <td>le, recurrent neural networks  have made swift...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>24142</td>\n",
       "      <td>12541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1412.7449v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;lukasz kaiser;terry koo;slav pet...</td>\n",
       "      <td>networks  have made swift inroads intomany str...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>23803</td>\n",
       "      <td>4098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reproducibility. All code, data, and experimen...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1506.03134v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;meire fortunato;navdeep jaitly</td>\n",
       "      <td>n-based copying can be seen as acombination of...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>11893</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>st like CWS and POS tagging, automatic prosody...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>1511.00360v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>chuang ding;lei xie;jie yan;weini zhang;yang liu</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>sing neural networks from raw text in a fully ...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>23709</td>\n",
       "      <td>15528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We begin by considering a document as the set ...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>s their usefulness for real-world tasks.As a f...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>287</td>\n",
       "      <td>5463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16662</th>\n",
       "      <td>With human annotation of data, significant int...</td>\n",
       "      <td>proposed a yesian EM framework for continuous-...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>1512.02393v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>tingting zhu;nic dunkley;joachim behar;david a...</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "      <td>ion of each expert annotator and the underlyin...</td>\n",
       "      <td>proposed a yesian EM framework for continuous-...</td>\n",
       "      <td>5633</td>\n",
       "      <td>8530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16663</th>\n",
       "      <td>An effective probabilistic approach to aggrega...</td>\n",
       "      <td>. as is defined as the inverse of accuracy: It...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>1512.02393v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>tingting zhu;nic dunkley;joachim behar;david a...</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "      <td>reduce annotator inter- and intra-variability....</td>\n",
       "      <td>. as is defined as the inverse of accuracy: It...</td>\n",
       "      <td>2682</td>\n",
       "      <td>14024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16665</th>\n",
       "      <td>This approach works reasonably well, but does ...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "      <td>1610.03946v1</td>\n",
       "      <td>1301.3781v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>jessica ficler;yoav goldberg</td>\n",
       "      <td>tomas mikolov;kai chen;greg corrado;jeffrey dean</td>\n",
       "      <td>ell in the CKY chart.3In both approaches,the P...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "      <td>20866</td>\n",
       "      <td>6656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16667</th>\n",
       "      <td>Fig. 10 shows histograms of the execution time...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1603.02199v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>sergey levine;peter pastor;alex krizhevsky;dei...</td>\n",
       "      <td>l concept is shown in Fig. 1.Recent studies ha...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>19226</td>\n",
       "      <td>15790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>As we can see from Eq. , the target of the lea...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1312.5602v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>volodymyr mnih;koray kavukcuoglu;david silver;...</td>\n",
       "      <td>work decision.Algorithm 2 shows the learning t...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>2042</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12230 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       left_citated_text  \\\n",
       "0      We conducted additional experiments on artific...   \n",
       "1      We conducted additional experiments on artific...   \n",
       "2      Reproducibility. All code, data, and experimen...   \n",
       "3      st like CWS and POS tagging, automatic prosody...   \n",
       "4      We begin by considering a document as the set ...   \n",
       "...                                                  ...   \n",
       "16662  With human annotation of data, significant int...   \n",
       "16663  An effective probabilistic approach to aggrega...   \n",
       "16665  This approach works reasonably well, but does ...   \n",
       "16667  Fig. 10 shows histograms of the execution time...   \n",
       "16668  As we can see from Eq. , the target of the lea...   \n",
       "\n",
       "                                      right_citated_text     target_id  \\\n",
       "0      andsyntactic parsing .Because RNNs make very f...  1606.03622v1   \n",
       "1      .Because RNNs make very few domain-specific as...  1606.03622v1   \n",
       "2      ; in a Pointer Network,the only way to generat...  1606.03622v1   \n",
       "3      . Recently, nsur .  have shown superior perfor...  1511.00360v1   \n",
       "4      model trained on the Google News dataset3.In a...  1705.10900v1   \n",
       "...                                                  ...           ...   \n",
       "16662  proposed a yesian EM framework for continuous-...  1503.06619v1   \n",
       "16663  . as is defined as the inverse of accuracy: It...  1503.06619v1   \n",
       "16665  on the POS sequences in PTB trainingset.The re...  1610.03946v1   \n",
       "16667  , but none of these methods can be applied dir...  1708.04033v1   \n",
       "16668  , we use multiple long short-term memory  laye...  1708.04033v1   \n",
       "\n",
       "          source_id  target_year  \\\n",
       "0       1409.3215v1         2016   \n",
       "1       1412.7449v1         2016   \n",
       "2      1506.03134v1         2016   \n",
       "3       1310.4546v1         2015   \n",
       "4       1310.4546v1         2017   \n",
       "...             ...          ...   \n",
       "16662  1512.02393v1         2015   \n",
       "16663  1512.02393v1         2015   \n",
       "16665   1301.3781v1         2016   \n",
       "16667  1603.02199v1         2017   \n",
       "16668   1312.5602v1         2017   \n",
       "\n",
       "                                           target_author  \\\n",
       "0                                  robin jia;percy liang   \n",
       "1                                  robin jia;percy liang   \n",
       "2                                  robin jia;percy liang   \n",
       "3       chuang ding;lei xie;jie yan;weini zhang;yang liu   \n",
       "4      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "...                                                  ...   \n",
       "16662  tingting zhu;nic dunkley;joachim behar;david a...   \n",
       "16663  tingting zhu;nic dunkley;joachim behar;david a...   \n",
       "16665                       jessica ficler;yoav goldberg   \n",
       "16667  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "16668  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "\n",
       "                                           source_author  \\\n",
       "0                 ilya sutskever;oriol vinyals;quoc v le   \n",
       "1      oriol vinyals;lukasz kaiser;terry koo;slav pet...   \n",
       "2           oriol vinyals;meire fortunato;navdeep jaitly   \n",
       "3      tomas mikolov;ilya sutskever;kai chen 0010;gre...   \n",
       "4      tomas mikolov;ilya sutskever;kai chen 0010;gre...   \n",
       "...                                                  ...   \n",
       "16662                  changbo zhu;huan xu;shuicheng yan   \n",
       "16663                  changbo zhu;huan xu;shuicheng yan   \n",
       "16665   tomas mikolov;kai chen;greg corrado;jeffrey dean   \n",
       "16667  sergey levine;peter pastor;alex krizhevsky;dei...   \n",
       "16668  volodymyr mnih;koray kavukcuoglu;david silver;...   \n",
       "\n",
       "                                               #1 String  \\\n",
       "0      le, recurrent neural networks  have made swift...   \n",
       "1      networks  have made swift inroads intomany str...   \n",
       "2      n-based copying can be seen as acombination of...   \n",
       "3      sing neural networks from raw text in a fully ...   \n",
       "4      s their usefulness for real-world tasks.As a f...   \n",
       "...                                                  ...   \n",
       "16662  ion of each expert annotator and the underlyin...   \n",
       "16663  reduce annotator inter- and intra-variability....   \n",
       "16665  ell in the CKY chart.3In both approaches,the P...   \n",
       "16667  l concept is shown in Fig. 1.Recent studies ha...   \n",
       "16668  work decision.Algorithm 2 shows the learning t...   \n",
       "\n",
       "                                               #2 String  #1 ID  #2 ID  \n",
       "0      andsyntactic parsing .Because RNNs make very f...  24142  12541  \n",
       "1      .Because RNNs make very few domain-specific as...  23803   4098  \n",
       "2      ; in a Pointer Network,the only way to generat...  11893    448  \n",
       "3      . Recently, nsur .  have shown superior perfor...  23709  15528  \n",
       "4      model trained on the Google News dataset3.In a...    287   5463  \n",
       "...                                                  ...    ...    ...  \n",
       "16662  proposed a yesian EM framework for continuous-...   5633   8530  \n",
       "16663  . as is defined as the inverse of accuracy: It...   2682  14024  \n",
       "16665  on the POS sequences in PTB trainingset.The re...  20866   6656  \n",
       "16667  , but none of these methods can be applied dir...  19226  15790  \n",
       "16668  , we use multiple long short-term memory  laye...   2042    525  \n",
       "\n",
       "[12230 rows x 11 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2017\n",
    "train_idx = df['target_year'][df['target_year'] < year].index\n",
    "test_idx = df['target_year'][df['target_year'] >= year].index\n",
    "train_df = df.loc[train_idx]\n",
    "test_df = df.loc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_citated_text</th>\n",
       "      <th>right_citated_text</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>target_year</th>\n",
       "      <th>target_author</th>\n",
       "      <th>source_author</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We begin by considering a document as the set ...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>s their usefulness for real-world tasks.As a f...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>287</td>\n",
       "      <td>5463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>With GMM and Aw2v+Ph, the F1-Score of clusteri...</td>\n",
       "      <td>. We observe that by themselves, Ph embeddings...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1408.5882v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>yoon kim</td>\n",
       "      <td>presented in Table 1. For comparison, we provi...</td>\n",
       "      <td>. We observe that by themselves, Ph embeddings...</td>\n",
       "      <td>1464</td>\n",
       "      <td>14906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We evaluate our method on the CSP dataset8. Th...</td>\n",
       "      <td>. We observe that Ph embeddings perform poorly...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1405.4053v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>quoc v le;tomas mikolov</td>\n",
       "      <td>e word vectors in that sentence. The results a...</td>\n",
       "      <td>. We observe that Ph embeddings perform poorly...</td>\n",
       "      <td>1563</td>\n",
       "      <td>19315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We perform document-level binary sentiment cla...</td>\n",
       "      <td>or purely word vector based methods  . .cument...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1405.4053v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>quoc v le;tomas mikolov</td>\n",
       "      <td>d recent attention owing to their utility in s...</td>\n",
       "      <td>or purely word vector based methods  . .cument...</td>\n",
       "      <td>22201</td>\n",
       "      <td>19816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In the lack of sufficient health status inform...</td>\n",
       "      <td>. Two most popular approaches of DGM rely on v...</td>\n",
       "      <td>1709.00845v1</td>\n",
       "      <td>1406.5298v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>andre s yoon;taehoon lee;yongsub lim;deokwoo j...</td>\n",
       "      <td>diederik p kingma;shakir mohamed;danilo jimene...</td>\n",
       "      <td>enerative models  have recently achieved state...</td>\n",
       "      <td>. Two most popular approaches of DGM rely on v...</td>\n",
       "      <td>6100</td>\n",
       "      <td>13092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16649</th>\n",
       "      <td>We now demonstrate cascading modern CNN archit...</td>\n",
       "      <td>, as a reference and construct a similar archi...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>1605.07146v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>edouard oyallon;eugene belilovsky;sergey zagor...</td>\n",
       "      <td>sergey zagoruyko;nikos komodakis</td>\n",
       "      <td>ication task. re we consider cascading the sca...</td>\n",
       "      <td>, as a reference and construct a similar archi...</td>\n",
       "      <td>1765</td>\n",
       "      <td>17047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16650</th>\n",
       "      <td>For the scattering transform we used J=2 which...</td>\n",
       "      <td>. Specifically we modify the WRN of 16 layers ...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>1605.07146v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>edouard oyallon;eugene belilovsky;sergey zagor...</td>\n",
       "      <td>sergey zagoruyko;nikos komodakis</td>\n",
       "      <td>on CIFAR-10, all based on end-to-end learned C...</td>\n",
       "      <td>. Specifically we modify the WRN of 16 layers ...</td>\n",
       "      <td>22417</td>\n",
       "      <td>14237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16651</th>\n",
       "      <td>We now consider the popular CIFAR-10 dataset c...</td>\n",
       "      <td>. Table 3 reports the accuracy in the unsuperv...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>1502.03167v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>edouard oyallon;eugene belilovsky;sergey zagor...</td>\n",
       "      <td>sergey ioffe;christian szegedy</td>\n",
       "      <td>on in both case. We utilize batch normalizatio...</td>\n",
       "      <td>. Table 3 reports the accuracy in the unsuperv...</td>\n",
       "      <td>22240</td>\n",
       "      <td>17408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16667</th>\n",
       "      <td>Fig. 10 shows histograms of the execution time...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1603.02199v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>sergey levine;peter pastor;alex krizhevsky;dei...</td>\n",
       "      <td>l concept is shown in Fig. 1.Recent studies ha...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>19226</td>\n",
       "      <td>15790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>As we can see from Eq. , the target of the lea...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1312.5602v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>volodymyr mnih;koray kavukcuoglu;david silver;...</td>\n",
       "      <td>work decision.Algorithm 2 shows the learning t...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>2042</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4783 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       left_citated_text  \\\n",
       "4      We begin by considering a document as the set ...   \n",
       "5      With GMM and Aw2v+Ph, the F1-Score of clusteri...   \n",
       "6      We evaluate our method on the CSP dataset8. Th...   \n",
       "7      We perform document-level binary sentiment cla...   \n",
       "8      In the lack of sufficient health status inform...   \n",
       "...                                                  ...   \n",
       "16649  We now demonstrate cascading modern CNN archit...   \n",
       "16650  For the scattering transform we used J=2 which...   \n",
       "16651  We now consider the popular CIFAR-10 dataset c...   \n",
       "16667  Fig. 10 shows histograms of the execution time...   \n",
       "16668  As we can see from Eq. , the target of the lea...   \n",
       "\n",
       "                                      right_citated_text     target_id  \\\n",
       "4      model trained on the Google News dataset3.In a...  1705.10900v1   \n",
       "5      . We observe that by themselves, Ph embeddings...  1705.10900v1   \n",
       "6      . We observe that Ph embeddings perform poorly...  1705.10900v1   \n",
       "7      or purely word vector based methods  . .cument...  1705.10900v1   \n",
       "8      . Two most popular approaches of DGM rely on v...  1709.00845v1   \n",
       "...                                                  ...           ...   \n",
       "16649  , as a reference and construct a similar archi...  1703.08961v1   \n",
       "16650  . Specifically we modify the WRN of 16 layers ...  1703.08961v1   \n",
       "16651  . Table 3 reports the accuracy in the unsuperv...  1703.08961v1   \n",
       "16667  , but none of these methods can be applied dir...  1708.04033v1   \n",
       "16668  , we use multiple long short-term memory  laye...  1708.04033v1   \n",
       "\n",
       "          source_id  target_year  \\\n",
       "4       1310.4546v1         2017   \n",
       "5       1408.5882v1         2017   \n",
       "6       1405.4053v1         2017   \n",
       "7       1405.4053v1         2017   \n",
       "8       1406.5298v1         2017   \n",
       "...             ...          ...   \n",
       "16649  1605.07146v1         2017   \n",
       "16650  1605.07146v1         2017   \n",
       "16651  1502.03167v1         2017   \n",
       "16667  1603.02199v1         2017   \n",
       "16668   1312.5602v1         2017   \n",
       "\n",
       "                                           target_author  \\\n",
       "4      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "5      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "6      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "7      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "8      andre s yoon;taehoon lee;yongsub lim;deokwoo j...   \n",
       "...                                                  ...   \n",
       "16649  edouard oyallon;eugene belilovsky;sergey zagor...   \n",
       "16650  edouard oyallon;eugene belilovsky;sergey zagor...   \n",
       "16651  edouard oyallon;eugene belilovsky;sergey zagor...   \n",
       "16667  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "16668  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "\n",
       "                                           source_author  \\\n",
       "4      tomas mikolov;ilya sutskever;kai chen 0010;gre...   \n",
       "5                                               yoon kim   \n",
       "6                                quoc v le;tomas mikolov   \n",
       "7                                quoc v le;tomas mikolov   \n",
       "8      diederik p kingma;shakir mohamed;danilo jimene...   \n",
       "...                                                  ...   \n",
       "16649                   sergey zagoruyko;nikos komodakis   \n",
       "16650                   sergey zagoruyko;nikos komodakis   \n",
       "16651                     sergey ioffe;christian szegedy   \n",
       "16667  sergey levine;peter pastor;alex krizhevsky;dei...   \n",
       "16668  volodymyr mnih;koray kavukcuoglu;david silver;...   \n",
       "\n",
       "                                               #1 String  \\\n",
       "4      s their usefulness for real-world tasks.As a f...   \n",
       "5      presented in Table 1. For comparison, we provi...   \n",
       "6      e word vectors in that sentence. The results a...   \n",
       "7      d recent attention owing to their utility in s...   \n",
       "8      enerative models  have recently achieved state...   \n",
       "...                                                  ...   \n",
       "16649  ication task. re we consider cascading the sca...   \n",
       "16650  on CIFAR-10, all based on end-to-end learned C...   \n",
       "16651  on in both case. We utilize batch normalizatio...   \n",
       "16667  l concept is shown in Fig. 1.Recent studies ha...   \n",
       "16668  work decision.Algorithm 2 shows the learning t...   \n",
       "\n",
       "                                               #2 String  #1 ID  #2 ID  \n",
       "4      model trained on the Google News dataset3.In a...    287   5463  \n",
       "5      . We observe that by themselves, Ph embeddings...   1464  14906  \n",
       "6      . We observe that Ph embeddings perform poorly...   1563  19315  \n",
       "7      or purely word vector based methods  . .cument...  22201  19816  \n",
       "8      . Two most popular approaches of DGM rely on v...   6100  13092  \n",
       "...                                                  ...    ...    ...  \n",
       "16649  , as a reference and construct a similar archi...   1765  17047  \n",
       "16650  . Specifically we modify the WRN of 16 layers ...  22417  14237  \n",
       "16651  . Table 3 reports the accuracy in the unsuperv...  22240  17408  \n",
       "16667  , but none of these methods can be applied dir...  19226  15790  \n",
       "16668  , we use multiple long short-term memory  laye...   2042    525  \n",
       "\n",
       "[4783 rows x 11 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_column = ['Quality', '#1 ID', '#2 ID', '#1 String', '#2 String', 'target_id', 'source_author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit_transform(df['source_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg_label': 0, 'pos_label': 1, 'sparse_output': False}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_argmax(df, lb):\n",
    "\n",
    "    y = df['source_id'].values\n",
    "    y = lb.transform(y)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    df['Quality'] = y\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1 = convert_argmax(train_df, lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_citated_text</th>\n",
       "      <th>right_citated_text</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>target_year</th>\n",
       "      <th>target_author</th>\n",
       "      <th>source_author</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "      <th>Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1409.3215v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>ilya sutskever;oriol vinyals;quoc v le</td>\n",
       "      <td>le, recurrent neural networks  have made swift...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>24142</td>\n",
       "      <td>12541</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1412.7449v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;lukasz kaiser;terry koo;slav pet...</td>\n",
       "      <td>networks  have made swift inroads intomany str...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>23803</td>\n",
       "      <td>4098</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reproducibility. All code, data, and experimen...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1506.03134v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;meire fortunato;navdeep jaitly</td>\n",
       "      <td>n-based copying can be seen as acombination of...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>11893</td>\n",
       "      <td>448</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>st like CWS and POS tagging, automatic prosody...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>1511.00360v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>chuang ding;lei xie;jie yan;weini zhang;yang liu</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>sing neural networks from raw text in a fully ...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>23709</td>\n",
       "      <td>15528</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stillation has been traditionally applied to n...</td>\n",
       "      <td>usedcompression to transfer knowledge from a d...</td>\n",
       "      <td>1511.06295v1</td>\n",
       "      <td>1312.6184v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>andrei a rusu;sergio gomez colmenarejo;caglar ...</td>\n",
       "      <td>jimmy ba;rich caruana</td>\n",
       "      <td>by cila . , whoproposed it as a means of comp...</td>\n",
       "      <td>usedcompression to transfer knowledge from a d...</td>\n",
       "      <td>10542</td>\n",
       "      <td>14649</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16660</th>\n",
       "      <td>An important inspiration for the proposed fram...</td>\n",
       "      <td>. Two important differenceswith the approach p...</td>\n",
       "      <td>1502.04156v1</td>\n",
       "      <td>1406.2751v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>yoshua bengio;dong-hyun lee;jorg bornschein;th...</td>\n",
       "      <td>j\\\"org bornschein;yoshua bengio</td>\n",
       "      <td>lgorithm  and which finds very interesting ins...</td>\n",
       "      <td>. Two important differenceswith the approach p...</td>\n",
       "      <td>24106</td>\n",
       "      <td>207</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16661</th>\n",
       "      <td>The proposal made here also owes a lot to the ...</td>\n",
       "      <td>and generativeadversarial networks ( ., 2014)...</td>\n",
       "      <td>1502.04156v1</td>\n",
       "      <td>1306.1091v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>yoshua bengio;dong-hyun lee;jorg bornschein;th...</td>\n",
       "      <td>yoshua bengio;eric laufer;guillaume alain;jaso...</td>\n",
       "      <td>about the same or better as was obtained for c...</td>\n",
       "      <td>and generativeadversarial networks ( ., 2014)...</td>\n",
       "      <td>23578</td>\n",
       "      <td>10752</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16662</th>\n",
       "      <td>With human annotation of data, significant int...</td>\n",
       "      <td>proposed a yesian EM framework for continuous-...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>1512.02393v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>tingting zhu;nic dunkley;joachim behar;david a...</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "      <td>ion of each expert annotator and the underlyin...</td>\n",
       "      <td>proposed a yesian EM framework for continuous-...</td>\n",
       "      <td>5633</td>\n",
       "      <td>8530</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16663</th>\n",
       "      <td>An effective probabilistic approach to aggrega...</td>\n",
       "      <td>. as is defined as the inverse of accuracy: It...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>1512.02393v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>tingting zhu;nic dunkley;joachim behar;david a...</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "      <td>reduce annotator inter- and intra-variability....</td>\n",
       "      <td>. as is defined as the inverse of accuracy: It...</td>\n",
       "      <td>2682</td>\n",
       "      <td>14024</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16665</th>\n",
       "      <td>This approach works reasonably well, but does ...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "      <td>1610.03946v1</td>\n",
       "      <td>1301.3781v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>jessica ficler;yoav goldberg</td>\n",
       "      <td>tomas mikolov;kai chen;greg corrado;jeffrey dean</td>\n",
       "      <td>ell in the CKY chart.3In both approaches,the P...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "      <td>20866</td>\n",
       "      <td>6656</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7447 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       left_citated_text  \\\n",
       "0      We conducted additional experiments on artific...   \n",
       "1      We conducted additional experiments on artific...   \n",
       "2      Reproducibility. All code, data, and experimen...   \n",
       "3      st like CWS and POS tagging, automatic prosody...   \n",
       "13     stillation has been traditionally applied to n...   \n",
       "...                                                  ...   \n",
       "16660  An important inspiration for the proposed fram...   \n",
       "16661  The proposal made here also owes a lot to the ...   \n",
       "16662  With human annotation of data, significant int...   \n",
       "16663  An effective probabilistic approach to aggrega...   \n",
       "16665  This approach works reasonably well, but does ...   \n",
       "\n",
       "                                      right_citated_text     target_id  \\\n",
       "0      andsyntactic parsing .Because RNNs make very f...  1606.03622v1   \n",
       "1      .Because RNNs make very few domain-specific as...  1606.03622v1   \n",
       "2      ; in a Pointer Network,the only way to generat...  1606.03622v1   \n",
       "3      . Recently, nsur .  have shown superior perfor...  1511.00360v1   \n",
       "13     usedcompression to transfer knowledge from a d...  1511.06295v1   \n",
       "...                                                  ...           ...   \n",
       "16660  . Two important differenceswith the approach p...  1502.04156v1   \n",
       "16661   and generativeadversarial networks ( ., 2014)...  1502.04156v1   \n",
       "16662  proposed a yesian EM framework for continuous-...  1503.06619v1   \n",
       "16663  . as is defined as the inverse of accuracy: It...  1503.06619v1   \n",
       "16665  on the POS sequences in PTB trainingset.The re...  1610.03946v1   \n",
       "\n",
       "          source_id  target_year  \\\n",
       "0       1409.3215v1         2016   \n",
       "1       1412.7449v1         2016   \n",
       "2      1506.03134v1         2016   \n",
       "3       1310.4546v1         2015   \n",
       "13      1312.6184v1         2015   \n",
       "...             ...          ...   \n",
       "16660   1406.2751v1         2015   \n",
       "16661   1306.1091v1         2015   \n",
       "16662  1512.02393v1         2015   \n",
       "16663  1512.02393v1         2015   \n",
       "16665   1301.3781v1         2016   \n",
       "\n",
       "                                           target_author  \\\n",
       "0                                  robin jia;percy liang   \n",
       "1                                  robin jia;percy liang   \n",
       "2                                  robin jia;percy liang   \n",
       "3       chuang ding;lei xie;jie yan;weini zhang;yang liu   \n",
       "13     andrei a rusu;sergio gomez colmenarejo;caglar ...   \n",
       "...                                                  ...   \n",
       "16660  yoshua bengio;dong-hyun lee;jorg bornschein;th...   \n",
       "16661  yoshua bengio;dong-hyun lee;jorg bornschein;th...   \n",
       "16662  tingting zhu;nic dunkley;joachim behar;david a...   \n",
       "16663  tingting zhu;nic dunkley;joachim behar;david a...   \n",
       "16665                       jessica ficler;yoav goldberg   \n",
       "\n",
       "                                           source_author  \\\n",
       "0                 ilya sutskever;oriol vinyals;quoc v le   \n",
       "1      oriol vinyals;lukasz kaiser;terry koo;slav pet...   \n",
       "2           oriol vinyals;meire fortunato;navdeep jaitly   \n",
       "3      tomas mikolov;ilya sutskever;kai chen 0010;gre...   \n",
       "13                                 jimmy ba;rich caruana   \n",
       "...                                                  ...   \n",
       "16660                    j\\\"org bornschein;yoshua bengio   \n",
       "16661  yoshua bengio;eric laufer;guillaume alain;jaso...   \n",
       "16662                  changbo zhu;huan xu;shuicheng yan   \n",
       "16663                  changbo zhu;huan xu;shuicheng yan   \n",
       "16665   tomas mikolov;kai chen;greg corrado;jeffrey dean   \n",
       "\n",
       "                                               #1 String  \\\n",
       "0      le, recurrent neural networks  have made swift...   \n",
       "1      networks  have made swift inroads intomany str...   \n",
       "2      n-based copying can be seen as acombination of...   \n",
       "3      sing neural networks from raw text in a fully ...   \n",
       "13      by cila . , whoproposed it as a means of comp...   \n",
       "...                                                  ...   \n",
       "16660  lgorithm  and which finds very interesting ins...   \n",
       "16661  about the same or better as was obtained for c...   \n",
       "16662  ion of each expert annotator and the underlyin...   \n",
       "16663  reduce annotator inter- and intra-variability....   \n",
       "16665  ell in the CKY chart.3In both approaches,the P...   \n",
       "\n",
       "                                               #2 String  #1 ID  #2 ID  \\\n",
       "0      andsyntactic parsing .Because RNNs make very f...  24142  12541   \n",
       "1      .Because RNNs make very few domain-specific as...  23803   4098   \n",
       "2      ; in a Pointer Network,the only way to generat...  11893    448   \n",
       "3      . Recently, nsur .  have shown superior perfor...  23709  15528   \n",
       "13     usedcompression to transfer knowledge from a d...  10542  14649   \n",
       "...                                                  ...    ...    ...   \n",
       "16660  . Two important differenceswith the approach p...  24106    207   \n",
       "16661   and generativeadversarial networks ( ., 2014)...  23578  10752   \n",
       "16662  proposed a yesian EM framework for continuous-...   5633   8530   \n",
       "16663  . as is defined as the inverse of accuracy: It...   2682  14024   \n",
       "16665  on the POS sequences in PTB trainingset.The re...  20866   6656   \n",
       "\n",
       "       Quality  \n",
       "0          139  \n",
       "1          174  \n",
       "2          232  \n",
       "3           90  \n",
       "13         102  \n",
       "...        ...  \n",
       "16660      127  \n",
       "16661       80  \n",
       "16662      319  \n",
       "16663      319  \n",
       "16665       69  \n",
       "\n",
       "[7447 rows x 12 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "anArray = train_df1['Quality'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1 = convert_argmax(test_df, lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_citated_text</th>\n",
       "      <th>right_citated_text</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>target_year</th>\n",
       "      <th>target_author</th>\n",
       "      <th>source_author</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "      <th>Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We begin by considering a document as the set ...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>s their usefulness for real-world tasks.As a f...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>23427</td>\n",
       "      <td>10153</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>With GMM and Aw2v+Ph, the F1-Score of clusteri...</td>\n",
       "      <td>. We observe that by themselves, Ph embeddings...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1408.5882v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>yoon kim</td>\n",
       "      <td>presented in Table 1. For comparison, we provi...</td>\n",
       "      <td>. We observe that by themselves, Ph embeddings...</td>\n",
       "      <td>13225</td>\n",
       "      <td>377</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We evaluate our method on the CSP dataset8. Th...</td>\n",
       "      <td>. We observe that Ph embeddings perform poorly...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1405.4053v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>quoc v le;tomas mikolov</td>\n",
       "      <td>e word vectors in that sentence. The results a...</td>\n",
       "      <td>. We observe that Ph embeddings perform poorly...</td>\n",
       "      <td>15416</td>\n",
       "      <td>1612</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We perform document-level binary sentiment cla...</td>\n",
       "      <td>or purely word vector based methods  . .cument...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1405.4053v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>quoc v le;tomas mikolov</td>\n",
       "      <td>d recent attention owing to their utility in s...</td>\n",
       "      <td>or purely word vector based methods  . .cument...</td>\n",
       "      <td>12091</td>\n",
       "      <td>22025</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In the lack of sufficient health status inform...</td>\n",
       "      <td>. Two most popular approaches of DGM rely on v...</td>\n",
       "      <td>1709.00845v1</td>\n",
       "      <td>1406.5298v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>andre s yoon;taehoon lee;yongsub lim;deokwoo j...</td>\n",
       "      <td>diederik p kingma;shakir mohamed;danilo jimene...</td>\n",
       "      <td>enerative models  have recently achieved state...</td>\n",
       "      <td>. Two most popular approaches of DGM rely on v...</td>\n",
       "      <td>9400</td>\n",
       "      <td>16385</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16649</th>\n",
       "      <td>We now demonstrate cascading modern CNN archit...</td>\n",
       "      <td>, as a reference and construct a similar archi...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>1605.07146v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>edouard oyallon;eugene belilovsky;sergey zagor...</td>\n",
       "      <td>sergey zagoruyko;nikos komodakis</td>\n",
       "      <td>ication task. re we consider cascading the sca...</td>\n",
       "      <td>, as a reference and construct a similar archi...</td>\n",
       "      <td>11129</td>\n",
       "      <td>1356</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16650</th>\n",
       "      <td>For the scattering transform we used J=2 which...</td>\n",
       "      <td>. Specifically we modify the WRN of 16 layers ...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>1605.07146v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>edouard oyallon;eugene belilovsky;sergey zagor...</td>\n",
       "      <td>sergey zagoruyko;nikos komodakis</td>\n",
       "      <td>on CIFAR-10, all based on end-to-end learned C...</td>\n",
       "      <td>. Specifically we modify the WRN of 16 layers ...</td>\n",
       "      <td>13604</td>\n",
       "      <td>7143</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16651</th>\n",
       "      <td>We now consider the popular CIFAR-10 dataset c...</td>\n",
       "      <td>. Table 3 reports the accuracy in the unsuperv...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>1502.03167v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>edouard oyallon;eugene belilovsky;sergey zagor...</td>\n",
       "      <td>sergey ioffe;christian szegedy</td>\n",
       "      <td>on in both case. We utilize batch normalizatio...</td>\n",
       "      <td>. Table 3 reports the accuracy in the unsuperv...</td>\n",
       "      <td>2229</td>\n",
       "      <td>1423</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16667</th>\n",
       "      <td>Fig. 10 shows histograms of the execution time...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1603.02199v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>sergey levine;peter pastor;alex krizhevsky;dei...</td>\n",
       "      <td>l concept is shown in Fig. 1.Recent studies ha...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>20485</td>\n",
       "      <td>21074</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>As we can see from Eq. , the target of the lea...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1312.5602v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>volodymyr mnih;koray kavukcuoglu;david silver;...</td>\n",
       "      <td>work decision.Algorithm 2 shows the learning t...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1391</td>\n",
       "      <td>22959</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4783 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       left_citated_text  \\\n",
       "4      We begin by considering a document as the set ...   \n",
       "5      With GMM and Aw2v+Ph, the F1-Score of clusteri...   \n",
       "6      We evaluate our method on the CSP dataset8. Th...   \n",
       "7      We perform document-level binary sentiment cla...   \n",
       "8      In the lack of sufficient health status inform...   \n",
       "...                                                  ...   \n",
       "16649  We now demonstrate cascading modern CNN archit...   \n",
       "16650  For the scattering transform we used J=2 which...   \n",
       "16651  We now consider the popular CIFAR-10 dataset c...   \n",
       "16667  Fig. 10 shows histograms of the execution time...   \n",
       "16668  As we can see from Eq. , the target of the lea...   \n",
       "\n",
       "                                      right_citated_text     target_id  \\\n",
       "4      model trained on the Google News dataset3.In a...  1705.10900v1   \n",
       "5      . We observe that by themselves, Ph embeddings...  1705.10900v1   \n",
       "6      . We observe that Ph embeddings perform poorly...  1705.10900v1   \n",
       "7      or purely word vector based methods  . .cument...  1705.10900v1   \n",
       "8      . Two most popular approaches of DGM rely on v...  1709.00845v1   \n",
       "...                                                  ...           ...   \n",
       "16649  , as a reference and construct a similar archi...  1703.08961v1   \n",
       "16650  . Specifically we modify the WRN of 16 layers ...  1703.08961v1   \n",
       "16651  . Table 3 reports the accuracy in the unsuperv...  1703.08961v1   \n",
       "16667  , but none of these methods can be applied dir...  1708.04033v1   \n",
       "16668  , we use multiple long short-term memory  laye...  1708.04033v1   \n",
       "\n",
       "          source_id  target_year  \\\n",
       "4       1310.4546v1         2017   \n",
       "5       1408.5882v1         2017   \n",
       "6       1405.4053v1         2017   \n",
       "7       1405.4053v1         2017   \n",
       "8       1406.5298v1         2017   \n",
       "...             ...          ...   \n",
       "16649  1605.07146v1         2017   \n",
       "16650  1605.07146v1         2017   \n",
       "16651  1502.03167v1         2017   \n",
       "16667  1603.02199v1         2017   \n",
       "16668   1312.5602v1         2017   \n",
       "\n",
       "                                           target_author  \\\n",
       "4      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "5      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "6      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "7      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "8      andre s yoon;taehoon lee;yongsub lim;deokwoo j...   \n",
       "...                                                  ...   \n",
       "16649  edouard oyallon;eugene belilovsky;sergey zagor...   \n",
       "16650  edouard oyallon;eugene belilovsky;sergey zagor...   \n",
       "16651  edouard oyallon;eugene belilovsky;sergey zagor...   \n",
       "16667  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "16668  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "\n",
       "                                           source_author  \\\n",
       "4      tomas mikolov;ilya sutskever;kai chen 0010;gre...   \n",
       "5                                               yoon kim   \n",
       "6                                quoc v le;tomas mikolov   \n",
       "7                                quoc v le;tomas mikolov   \n",
       "8      diederik p kingma;shakir mohamed;danilo jimene...   \n",
       "...                                                  ...   \n",
       "16649                   sergey zagoruyko;nikos komodakis   \n",
       "16650                   sergey zagoruyko;nikos komodakis   \n",
       "16651                     sergey ioffe;christian szegedy   \n",
       "16667  sergey levine;peter pastor;alex krizhevsky;dei...   \n",
       "16668  volodymyr mnih;koray kavukcuoglu;david silver;...   \n",
       "\n",
       "                                               #1 String  \\\n",
       "4      s their usefulness for real-world tasks.As a f...   \n",
       "5      presented in Table 1. For comparison, we provi...   \n",
       "6      e word vectors in that sentence. The results a...   \n",
       "7      d recent attention owing to their utility in s...   \n",
       "8      enerative models  have recently achieved state...   \n",
       "...                                                  ...   \n",
       "16649  ication task. re we consider cascading the sca...   \n",
       "16650  on CIFAR-10, all based on end-to-end learned C...   \n",
       "16651  on in both case. We utilize batch normalizatio...   \n",
       "16667  l concept is shown in Fig. 1.Recent studies ha...   \n",
       "16668  work decision.Algorithm 2 shows the learning t...   \n",
       "\n",
       "                                               #2 String  #1 ID  #2 ID  \\\n",
       "4      model trained on the Google News dataset3.In a...  23427  10153   \n",
       "5      . We observe that by themselves, Ph embeddings...  13225    377   \n",
       "6      . We observe that Ph embeddings perform poorly...  15416   1612   \n",
       "7      or purely word vector based methods  . .cument...  12091  22025   \n",
       "8      . Two most popular approaches of DGM rely on v...   9400  16385   \n",
       "...                                                  ...    ...    ...   \n",
       "16649  , as a reference and construct a similar archi...  11129   1356   \n",
       "16650  . Specifically we modify the WRN of 16 layers ...  13604   7143   \n",
       "16651  . Table 3 reports the accuracy in the unsuperv...   2229   1423   \n",
       "16667  , but none of these methods can be applied dir...  20485  21074   \n",
       "16668  , we use multiple long short-term memory  laye...   1391  22959   \n",
       "\n",
       "       Quality  \n",
       "4           90  \n",
       "5          136  \n",
       "6          120  \n",
       "7          120  \n",
       "8          130  \n",
       "...        ...  \n",
       "16649      397  \n",
       "16650      397  \n",
       "16651      185  \n",
       "16667      357  \n",
       "16668       99  \n",
       "\n",
       "[4783 rows x 12 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_df1[bert_column], test_df1[bert_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quality</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>23427</td>\n",
       "      <td>10153</td>\n",
       "      <td>s their usefulness for real-world tasks.As a f...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>136</td>\n",
       "      <td>13225</td>\n",
       "      <td>377</td>\n",
       "      <td>presented in Table 1. For comparison, we provi...</td>\n",
       "      <td>. We observe that by themselves, Ph embeddings...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>yoon kim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>120</td>\n",
       "      <td>15416</td>\n",
       "      <td>1612</td>\n",
       "      <td>e word vectors in that sentence. The results a...</td>\n",
       "      <td>. We observe that Ph embeddings perform poorly...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>quoc v le;tomas mikolov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>120</td>\n",
       "      <td>12091</td>\n",
       "      <td>22025</td>\n",
       "      <td>d recent attention owing to their utility in s...</td>\n",
       "      <td>or purely word vector based methods  . .cument...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>quoc v le;tomas mikolov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>130</td>\n",
       "      <td>9400</td>\n",
       "      <td>16385</td>\n",
       "      <td>enerative models  have recently achieved state...</td>\n",
       "      <td>. Two most popular approaches of DGM rely on v...</td>\n",
       "      <td>1709.00845v1</td>\n",
       "      <td>diederik p kingma;shakir mohamed;danilo jimene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16649</th>\n",
       "      <td>397</td>\n",
       "      <td>11129</td>\n",
       "      <td>1356</td>\n",
       "      <td>ication task. re we consider cascading the sca...</td>\n",
       "      <td>, as a reference and construct a similar archi...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>sergey zagoruyko;nikos komodakis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16650</th>\n",
       "      <td>397</td>\n",
       "      <td>13604</td>\n",
       "      <td>7143</td>\n",
       "      <td>on CIFAR-10, all based on end-to-end learned C...</td>\n",
       "      <td>. Specifically we modify the WRN of 16 layers ...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>sergey zagoruyko;nikos komodakis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16651</th>\n",
       "      <td>185</td>\n",
       "      <td>2229</td>\n",
       "      <td>1423</td>\n",
       "      <td>on in both case. We utilize batch normalizatio...</td>\n",
       "      <td>. Table 3 reports the accuracy in the unsuperv...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>sergey ioffe;christian szegedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16667</th>\n",
       "      <td>357</td>\n",
       "      <td>20485</td>\n",
       "      <td>21074</td>\n",
       "      <td>l concept is shown in Fig. 1.Recent studies ha...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>sergey levine;peter pastor;alex krizhevsky;dei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>99</td>\n",
       "      <td>1391</td>\n",
       "      <td>22959</td>\n",
       "      <td>work decision.Algorithm 2 shows the learning t...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>volodymyr mnih;koray kavukcuoglu;david silver;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4783 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Quality  #1 ID  #2 ID  \\\n",
       "4           90  23427  10153   \n",
       "5          136  13225    377   \n",
       "6          120  15416   1612   \n",
       "7          120  12091  22025   \n",
       "8          130   9400  16385   \n",
       "...        ...    ...    ...   \n",
       "16649      397  11129   1356   \n",
       "16650      397  13604   7143   \n",
       "16651      185   2229   1423   \n",
       "16667      357  20485  21074   \n",
       "16668       99   1391  22959   \n",
       "\n",
       "                                               #1 String  \\\n",
       "4      s their usefulness for real-world tasks.As a f...   \n",
       "5      presented in Table 1. For comparison, we provi...   \n",
       "6      e word vectors in that sentence. The results a...   \n",
       "7      d recent attention owing to their utility in s...   \n",
       "8      enerative models  have recently achieved state...   \n",
       "...                                                  ...   \n",
       "16649  ication task. re we consider cascading the sca...   \n",
       "16650  on CIFAR-10, all based on end-to-end learned C...   \n",
       "16651  on in both case. We utilize batch normalizatio...   \n",
       "16667  l concept is shown in Fig. 1.Recent studies ha...   \n",
       "16668  work decision.Algorithm 2 shows the learning t...   \n",
       "\n",
       "                                               #2 String     target_id  \\\n",
       "4      model trained on the Google News dataset3.In a...  1705.10900v1   \n",
       "5      . We observe that by themselves, Ph embeddings...  1705.10900v1   \n",
       "6      . We observe that Ph embeddings perform poorly...  1705.10900v1   \n",
       "7      or purely word vector based methods  . .cument...  1705.10900v1   \n",
       "8      . Two most popular approaches of DGM rely on v...  1709.00845v1   \n",
       "...                                                  ...           ...   \n",
       "16649  , as a reference and construct a similar archi...  1703.08961v1   \n",
       "16650  . Specifically we modify the WRN of 16 layers ...  1703.08961v1   \n",
       "16651  . Table 3 reports the accuracy in the unsuperv...  1703.08961v1   \n",
       "16667  , but none of these methods can be applied dir...  1708.04033v1   \n",
       "16668  , we use multiple long short-term memory  laye...  1708.04033v1   \n",
       "\n",
       "                                           source_author  \n",
       "4      tomas mikolov;ilya sutskever;kai chen 0010;gre...  \n",
       "5                                               yoon kim  \n",
       "6                                quoc v le;tomas mikolov  \n",
       "7                                quoc v le;tomas mikolov  \n",
       "8      diederik p kingma;shakir mohamed;danilo jimene...  \n",
       "...                                                  ...  \n",
       "16649                   sergey zagoruyko;nikos komodakis  \n",
       "16650                   sergey zagoruyko;nikos komodakis  \n",
       "16651                     sergey ioffe;christian szegedy  \n",
       "16667  sergey levine;peter pastor;alex krizhevsky;dei...  \n",
       "16668  volodymyr mnih;koray kavukcuoglu;david silver;...  \n",
       "\n",
       "[4783 rows x 7 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quality</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139</td>\n",
       "      <td>8103</td>\n",
       "      <td>7582</td>\n",
       "      <td>le, recurrent neural networks  have made swift...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>ilya sutskever;oriol vinyals;quoc v le</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174</td>\n",
       "      <td>818</td>\n",
       "      <td>21392</td>\n",
       "      <td>networks  have made swift inroads intomany str...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>oriol vinyals;lukasz kaiser;terry koo;slav pet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>10444</td>\n",
       "      <td>2237</td>\n",
       "      <td>n-based copying can be seen as acombination of...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>oriol vinyals;meire fortunato;navdeep jaitly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90</td>\n",
       "      <td>5440</td>\n",
       "      <td>14282</td>\n",
       "      <td>sing neural networks from raw text in a fully ...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>1511.00360v1</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>102</td>\n",
       "      <td>7301</td>\n",
       "      <td>3444</td>\n",
       "      <td>by cila . , whoproposed it as a means of comp...</td>\n",
       "      <td>usedcompression to transfer knowledge from a d...</td>\n",
       "      <td>1511.06295v1</td>\n",
       "      <td>jimmy ba;rich caruana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16660</th>\n",
       "      <td>127</td>\n",
       "      <td>19965</td>\n",
       "      <td>15181</td>\n",
       "      <td>lgorithm  and which finds very interesting ins...</td>\n",
       "      <td>. Two important differenceswith the approach p...</td>\n",
       "      <td>1502.04156v1</td>\n",
       "      <td>j\\\"org bornschein;yoshua bengio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16661</th>\n",
       "      <td>80</td>\n",
       "      <td>1787</td>\n",
       "      <td>20657</td>\n",
       "      <td>about the same or better as was obtained for c...</td>\n",
       "      <td>and generativeadversarial networks ( ., 2014)...</td>\n",
       "      <td>1502.04156v1</td>\n",
       "      <td>yoshua bengio;eric laufer;guillaume alain;jaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16662</th>\n",
       "      <td>319</td>\n",
       "      <td>6892</td>\n",
       "      <td>723</td>\n",
       "      <td>ion of each expert annotator and the underlyin...</td>\n",
       "      <td>proposed a yesian EM framework for continuous-...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16663</th>\n",
       "      <td>319</td>\n",
       "      <td>20408</td>\n",
       "      <td>15313</td>\n",
       "      <td>reduce annotator inter- and intra-variability....</td>\n",
       "      <td>. as is defined as the inverse of accuracy: It...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16665</th>\n",
       "      <td>69</td>\n",
       "      <td>2953</td>\n",
       "      <td>4794</td>\n",
       "      <td>ell in the CKY chart.3In both approaches,the P...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "      <td>1610.03946v1</td>\n",
       "      <td>tomas mikolov;kai chen;greg corrado;jeffrey dean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7447 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Quality  #1 ID  #2 ID  \\\n",
       "0          139   8103   7582   \n",
       "1          174    818  21392   \n",
       "2          232  10444   2237   \n",
       "3           90   5440  14282   \n",
       "13         102   7301   3444   \n",
       "...        ...    ...    ...   \n",
       "16660      127  19965  15181   \n",
       "16661       80   1787  20657   \n",
       "16662      319   6892    723   \n",
       "16663      319  20408  15313   \n",
       "16665       69   2953   4794   \n",
       "\n",
       "                                               #1 String  \\\n",
       "0      le, recurrent neural networks  have made swift...   \n",
       "1      networks  have made swift inroads intomany str...   \n",
       "2      n-based copying can be seen as acombination of...   \n",
       "3      sing neural networks from raw text in a fully ...   \n",
       "13      by cila . , whoproposed it as a means of comp...   \n",
       "...                                                  ...   \n",
       "16660  lgorithm  and which finds very interesting ins...   \n",
       "16661  about the same or better as was obtained for c...   \n",
       "16662  ion of each expert annotator and the underlyin...   \n",
       "16663  reduce annotator inter- and intra-variability....   \n",
       "16665  ell in the CKY chart.3In both approaches,the P...   \n",
       "\n",
       "                                               #2 String     target_id  \\\n",
       "0      andsyntactic parsing .Because RNNs make very f...  1606.03622v1   \n",
       "1      .Because RNNs make very few domain-specific as...  1606.03622v1   \n",
       "2      ; in a Pointer Network,the only way to generat...  1606.03622v1   \n",
       "3      . Recently, nsur .  have shown superior perfor...  1511.00360v1   \n",
       "13     usedcompression to transfer knowledge from a d...  1511.06295v1   \n",
       "...                                                  ...           ...   \n",
       "16660  . Two important differenceswith the approach p...  1502.04156v1   \n",
       "16661   and generativeadversarial networks ( ., 2014)...  1502.04156v1   \n",
       "16662  proposed a yesian EM framework for continuous-...  1503.06619v1   \n",
       "16663  . as is defined as the inverse of accuracy: It...  1503.06619v1   \n",
       "16665  on the POS sequences in PTB trainingset.The re...  1610.03946v1   \n",
       "\n",
       "                                           source_author  \n",
       "0                 ilya sutskever;oriol vinyals;quoc v le  \n",
       "1      oriol vinyals;lukasz kaiser;terry koo;slav pet...  \n",
       "2           oriol vinyals;meire fortunato;navdeep jaitly  \n",
       "3      tomas mikolov;ilya sutskever;kai chen 0010;gre...  \n",
       "13                                 jimmy ba;rich caruana  \n",
       "...                                                  ...  \n",
       "16660                    j\\\"org bornschein;yoshua bengio  \n",
       "16661  yoshua bengio;eric laufer;guillaume alain;jaso...  \n",
       "16662                  changbo zhu;huan xu;shuicheng yan  \n",
       "16663                  changbo zhu;huan xu;shuicheng yan  \n",
       "16665   tomas mikolov;kai chen;greg corrado;jeffrey dean  \n",
       "\n",
       "[7447 rows x 7 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df['#1 ID'] == 14050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_label_info = []\n",
    "for i in temp_df.groupby(['#1 ID', '#2 ID']):\n",
    "    instance_label = {}\n",
    "    instance_label['Quality'] = i[1]['Quality'].values\n",
    "    instance_label['index'] = i[1]['Quality'].index.values\n",
    "    multi_label_info.append(instance_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,len(multi_label_info)):\n",
    "    if multi_label_info[i]['index'].any() == 0:\n",
    "        print(multi_label_info[i])\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_label_info[0]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: HI \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenizer.encode_plus(\"Hi. How are you?\",\"Hi. I am fine and how are you?\", add_special_tokens = True, max_length = 100, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tokenizer.encode_plus(\"Hi. I am fine and how are you?\", None, add_special_tokens = True, max_length = 100, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 7632,\n",
       " 1012,\n",
       " 2129,\n",
       " 2024,\n",
       " 2017,\n",
       " 1029,\n",
       " 102,\n",
       " 7632,\n",
       " 1012,\n",
       " 1045,\n",
       " 2572,\n",
       " 2986,\n",
       " 1998,\n",
       " 2129,\n",
       " 2024,\n",
       " 2017,\n",
       " 1029,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS]', 101)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token, tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using pip version.Just like a watched pot never boils\n"
     ]
    }
   ],
   "source": [
    "context = \"You are using pip version.Just like a watched pot never boils\"\n",
    "print(\" \".join(context.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"andsyntactic parsing .Because RNNs make very few domain-specific assumptions,they have the potential to succeed at a wide variety of taskswith minimal feature engineering.wever, this flexibility also puts RNNs at a disadvantage compared tostandard semantic parsers, which can generalize naturallyby leveraging their built-in awareness of logical compositionality.In this paper, we introduce data recombination,a generic framework for declaratively injecting prior knowledgeinto a domain-general structured prediction model.In data recombination, prior knowledge about a taskis used to build a high-precision generative modelthat expands the empirical distributionby allowing fragments of different examples to be combined inparticular ways.mples from this generative model are then used to train a domain-general model.In the case of semantic parsing, we construct a generative modelby inducing a synchronous context-free grammar ,creating new examples such as those shown in Figure 1;our domain-general model is a sequence-to-sequence RNNwith a novel attention-based copying mechanism.Data recombination boosts the accuracyof our RNN model on three semantic parsing datasets. the o dataset, data recombinationimproves test accuracy by 4.3 percentage pointsover our baseline RNN, leading to newstate-of-the-art results for models that do not use aseed lexicon for predicates.ox: “what is the population of iowa ?”y: _answer  , _const    ATISx: “can you list all flights from chicago to milwaukee”y:  _lambda $0 e  _and  _flight $0  _from $0 chicago : _ci  _to $0 milwaukee : _ci   Overnightx: “when is the weekly standup”y:  call listValue  callgetProperty meeting.weekly_standup string start_time   We cast semantic parsing as a sequence-to-sequence task.The input utterance x is a sequence of words x1,...,xm∈Vin, the input vocabulary;similarly, the output logical form y isa sequence of tokens y1,...,yn∈Vout, the output vocabulary.A linear sequence of tokens might appear tolose the hierarchical structure of a logical form,but there is precedent for this choice:newcitevinyals2015grammarshowed that an RNN can reliably predict tree-structured outputsin a linear fashion.We evaluate our system on three existing semantic parsing datasets.Figure 2 shows sample input-output pairs from each of these datasets.oery o containsnatural language questions about US geographypaired with corresponding Prolog database queries.We use the standard split of 600 training examples and 280 test examplesintroduced by newcitezettlemoyer05ccg.We preprocess the logical forms to De Brujin index notationto standardize variable naming.ATIS ATIS containsnatural language queries for a flights databasepaired with corresponding database querieswritten in lambda calculus.We train on 4473 examples and evaluate on the 448test examples used bynewcitezettlemoyer07relaxed.Overnight Overnight containslogical forms paired with natural languageparaphrases across eight varied subdomains.newcitewang2015overnight constructed the datasetby generating all possible logicalforms up to some depth threshold,then getting multiple natural language paraphrasesfor each logical form from workers on Amazon Mechanical rk.We evaluate on the same train/test splits asnewcitewang2015overnight.In this paper, we only explore learning from logical forms.In the last few years, there has an emergence ofsemantic parsers learned from denotations.While our system cannot directly learn from denotations,it could be used to rerank candidate derivationsgenerated by one of these other systems.Our sequence-to-sequence RNN model is based on existingattention-based neural machine translation models,but also includes a novel attention-based copying mechanism.milar copying mechanisms have been exploredin parallel by newcitegu2016copying and newcitegulcehre2016pointing.Encoder. The encoder converts the input sequence x1,...,xminto a sequence of context-sensitive embeddingsb1,...,bm using a bidirectional RNN .First, a word embedding function φinmaps each word xi to a fixed-dimensional vector.These vectors are fed as input to two RNNs: a forward RNN and a backward RNN.The forward RNN starts with an initial hidden state hF0,and generates a sequence of hidden states hF1,...,hFm byrepeatedly applying the recurrenceThe recurrence takes the form of an LSTM .The backward RNN similarly generates hidden states hBm,...,hB1by processing the input sequence in reverse order.Finally, for each input position i, we definethe context-sensitive embeddingbi to be the concatenation of hFi and hDecoder. The decoder is an attention-based modelthat generates the output sequence y1,...,ynone token at a time. At each time step j,it writes yj based on thecurrent hidden state sj, then updates the hiddenstate to sj+1 based on sj and yj.Formally, the decoder is defined by the following equations:When not specified, i ranges over {1,...,m}and j ranges over {1,...,n}.Intuitively, the αji’s define a probabilitydistribution over the input words,describing what words in the input the decoder is focusing on at time j.They are computed from the unnormalizedattention scores eji.The matrices Ws, Wa, and U,as well as the embedding function φout, are parameters of the model.In the basic model of the previous section,the next output word yj is chosenvia a simple softmax over all words in the output vocabulary.wever, this model has difficulty generalizing to the long tail ofentity names commonly found in semantic parsing datasets.Conveniently, entity names in the input often corresponddirectly to tokens in the outpute.g., “iowa” becomes iowa in Figure 2.1To capture this intuition, we introducea new attention-based copying mechanism.At each time step j, the decodergenerates one of two types of actions.As before, it can write any word in the output vocabulary.In addition, it can copy any input word xi directly to the output,where the probability with which we copy xi is determined by theattention score on xi.Formally, we define a latent action ajthat is either Write for some w∈Voutor Copy for some i∈{1,...,m}.We then haveThe decoder chooses aj with a softmax over all these possible actions;yj is then a deterministic function of aj and x.ring training, we maximize the log-likelihood of y,marginalizing out a.Attention-based copying can be seen as acombination of a standard softmax output layer of an attention-based model and a Pointer Network ; in a Pointer Network,the only way to generate output is to copy a symbol from the input.Examples“what states border texas ?”,answerNV, stateV0, next_toV0, NV, constV0, stateidtexas“what is the highest mountain in ohio ?”,answerNV, highestV0, mountainV0, locV0, NV, constV0, stateidohioRules created by AbsEntitiesRoot →⟨ “what states border Id ?”,answerNV, stateV0, next_toV0, NV, constV0, stateidId ⟩Id →⟨ “texas”, texas ⟩Root →⟨ “what is the highest mountain in Id ?”,answerNV, highestV0, mountainV0, locV0, NV, constV0, stateidId ⟩Id →⟨“ohio”, ohio⟩Rules created by AbsWholePhrasesRoot →⟨ “what states border  ?”, answerNV, stateV0, next_toV0, NV,  ⟩ →⟨ “states border texas”, stateV0, next_toV0, NV, constV0, stateidtexas⟩Root →⟨ “what is the highest mountain in  ?”,answerNV, highestV0, mountainV0, locV0, NV,  ⟩Rules created by Concat-2Root →⟨textsct1 </s> textsct2,textsct1 </s> textsct2⟩t →⟨ “what states border texas ?”,answerNV, stateV0, next_toV0, NV, constV0, stateidtexas ⟩t →⟨ “what is the highest mountain in ohio ?”,answerNV, highestV0, mountainV0, locV0, NV, constV0, stateidohio⟩The main contribution of this paper is a novel data recombination frameworkthat injects important prior knowledge into our oblivious sequence-to-sequence RNN.In this framework, we induce a high-precisiongenerative model from the training data,then sample from it to generate new training examples.The process of inducing this generative modelcan leverage any available prior knowledge,which is transmitted through the generated examplesto the RNN model.A key advantage of our two-stage approach is that it allows us todeclare desired properties of the task which might be hard to capturein the model architecture.Our approach generalizes data augmentation,which is commonly employed to inject prior knowledge into a model.Data augmentation techniques focus on modelinginvariances—transformations liketranslating an image or adding noisethat alter the inputs x,but do not change the output y.These techniques have proven effective in areas likecomputer vision and speech recognition .In semantic parsing, however,we would like to capture more than just invariance properties.Consider an example with the utterance “what states border texas ?”.Given this example, it should be easy togeneralize to questions where “texas”is replaced by the name of any other state:simply replace the mention of Texas in the logical formwith the name of the new state.Underlying this phenomenon is a strong conditional independence principle:the meaning of the rest of the sentence is independent of thename of the state in question.Standard data augmentation is not sufficient to model such phenomena:instead of holding y fixed,we would like to apply simultaneous transformations to x and ysuch that the new x still maps to the new y.Data recombination addresses this need.In the general setting of data recombination,we start with a training set D of x,y pairs,which defines the empirical distribution ^px,y.We then fit a generative model ~px,y to ^pwhich generalizes beyond the support of ^p,for example by splicing together fragments of different examples.We refer to examples in the support of ~p as recombinant examples.Finally, to train our actual model pθy∣x,we maximize the expected value oflogpθy∣x, where x,y is drawn from ~p.For semantic parsing, we induce a synchronous context-free grammar to serve as the backbone of our generative model ~p.An SCFG consists of a set of production rulesX→⟨α,β⟩, where X is a category non-terminal,and α and β are sequences of terminal and non-terminal symbols.Any non-terminal symbols in α mustbe aligned to the same non-terminal symbol in β,and vice versa.Therefore, an SCFG defines a set of joint derivations ofaligned pairs of strings.In our case, we use an SCFG to represent joint derivationsof utterances x and logical forms y which for us is just a sequence of tokens.After we induce an SCFG G from D,the corresponding generative model ~px,yis the distribution over pairs x,ydefined by sampling from G,where we choose production rules to apply uniformly at random.It is instructive to compare our SCFG-based data recombination withWasp ,which uses an SCFG as the actual semantic parsing model.The grammar induced by Waspmust have good coverage in order to generalize to new inputsat test time.Wasp also requires the implementation of anefficient algorithm for computing the conditionalprobability py∣x.In contrast, our SCFG is only used to conveyprior knowledge about conditional independence structure,so it only needs to have high precision;our RNN model is responsible for boosting recallover the entire input space.We also only need to forward sample from the SCFG, which isconsiderably easier to implement than conditional inference.Below, we examine various strategies for inducinga grammar G from a dataset D.We first encode D as an initial grammarwith rules Root →⟨x,y⟩for each x,y∈D.Next, we will define each grammar induction strategyas a mapping from an input grammar Gin to a new grammar Gout.This formulation allows us to compose grammar induction strategiesSection 4.3.4.Our first grammar induction strategy, AbsEntities, simply abstracts entitieswith their types.We assume that each entity e e.g., texashas a corresponding type e.t e.g., state,which we infer based on the presence of certain predicates in the logical forme.g. stateid.For each grammar rule X→⟨α,β⟩ in Gin,where α contains a token e.g., “texas” thatstring matches an entity e.g., texas in β,we add two rules to Gout:i a rule where both occurrences are replaced with the type of the entitye.g., state,and ii a new rule that maps the type to the entity e.g.,textscId→⟨``{texas}'',texas⟩;we reserve the category name  for the next section.Thus, Gout generates recombinant examplesthat fuse most of one example with an entity found in a second example.A concrete example from the o domain is given inFigure 3.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFile[\"right_citated_text\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcn_data(train_df, test_df, save_dir):\n",
    "    with open('{}/gcn_pretrain.pkl'.format(save_dir), 'rb') as f:\n",
    "        embedding = pickle.load(f)\n",
    "        node2id = pickle.load(f)\n",
    "    gcn_train = np.array([[node2id[i]] for i in train_df['query_id'].values])\n",
    "    gcn_test = np.array([[node2id[i]] for i in test_df['query_id'].values])\n",
    "    return gcn_train, gcn_test, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = None\n",
    "node2id = None\n",
    "with open('/mnt/e/MS/2-1B/AP-FIR/bert-gcn/pre_train/gcn/PeerRead/PeerRead_gcn_pretrain_author.pkl', 'rb') as f:\n",
    "        embedding = pickle.load(f)\n",
    "        node2id = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quoc v le'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['source_author'].values[0].split(';')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = train_df['source_author'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teri maa ki  1111\n",
      "Teri maa ki  1112\n",
      "Teri maa ki  1264\n",
      "Teri maa ki  1265\n",
      "Teri maa ki  1266\n",
      "Teri maa ki  1267\n",
      "Teri maa ki  1358\n",
      "Teri maa ki  1359\n",
      "Teri maa ki  1360\n",
      "Teri maa ki  1361\n",
      "Teri maa ki  2168\n",
      "Teri maa ki  2169\n",
      "Teri maa ki  2698\n",
      "Teri maa ki  2840\n",
      "Teri maa ki  2850\n",
      "Teri maa ki  3943\n",
      "Teri maa ki  4738\n",
      "Teri maa ki  5316\n",
      "Teri maa ki  5317\n",
      "Teri maa ki  5318\n",
      "Teri maa ki  5430\n",
      "Teri maa ki  6831\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in arr:\n",
    "    cnt += 1\n",
    "    if i.split(';')[-1] == 'afroze ibrahim baqapuri':\n",
    "        print('Teri maa ki ', cnt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'afroze ibrahim baqapuri'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-b35a31d285c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnode2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'afroze ibrahim baqapuri'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'afroze ibrahim baqapuri'"
     ]
    }
   ],
   "source": [
    "node2id['afroze ibrahim baqapuri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.75661084e-02, -2.11767226e-01, -7.69936517e-02, -1.34367093e-01,\n",
       "       -1.32247269e-01,  9.25423503e-02, -1.23237289e-01, -2.50031084e-01,\n",
       "        1.83598384e-01, -1.87942892e-01,  1.05373263e-02, -3.16464901e-03,\n",
       "        2.89688170e-01,  1.16193756e-01,  3.08152884e-02, -1.64103135e-01,\n",
       "        3.85200605e-02,  1.93548799e-02,  1.04115993e-01,  1.59260869e-01,\n",
       "        3.75042222e-02, -4.13777083e-01, -1.40298188e-01, -7.97704756e-02,\n",
       "        1.69954151e-02,  9.29089710e-02,  5.71189970e-02, -9.63231102e-02,\n",
       "        2.62236208e-01,  2.40637809e-02,  1.57377809e-01, -1.58616498e-01,\n",
       "       -1.79514438e-01,  1.06614847e-02, -4.85762879e-02,  8.04453343e-02,\n",
       "       -7.72103742e-02, -1.38636418e-02, -5.83091676e-02,  3.78895551e-03,\n",
       "       -1.35671839e-01,  8.73243362e-02, -8.78408253e-02,  7.90649429e-02,\n",
       "        3.46747875e-01,  2.61593997e-01,  1.78148653e-02, -1.58512473e-01,\n",
       "        2.26705939e-01, -1.97524950e-01,  2.39470914e-01, -1.20129056e-01,\n",
       "        3.78371663e-02,  1.43525712e-02,  3.07499677e-01,  3.93205136e-02,\n",
       "        3.26521963e-01,  1.33485287e-01,  7.21615106e-02,  5.97223006e-02,\n",
       "       -1.21679194e-01, -1.88882463e-04,  8.32585096e-02,  2.37187371e-03,\n",
       "       -3.79623994e-02,  3.63164917e-02,  1.57881349e-01, -1.06942460e-01,\n",
       "        2.39275128e-01, -1.29011154e-01, -9.83724967e-02,  1.77602649e-01,\n",
       "        1.74250811e-01, -2.16639966e-01, -1.07145943e-02,  1.90985620e-01,\n",
       "       -8.94674510e-02, -1.25081211e-01, -7.06397817e-02, -8.83093253e-02,\n",
       "       -1.58514634e-01, -2.91686535e-01, -1.53919961e-02,  1.23052299e-03,\n",
       "       -8.97953957e-02, -1.59408286e-01,  4.09115627e-02,  5.01033664e-02,\n",
       "       -1.36052649e-02, -2.93312222e-02, -1.08291216e-01,  4.32716981e-02,\n",
       "        2.18032673e-02, -3.35435867e-02,  2.18881100e-01, -3.69883701e-02,\n",
       "       -1.82689846e-01, -2.95824502e-02,  2.63306294e-02,  7.04644844e-02,\n",
       "        2.15215653e-01,  8.89781713e-02, -1.25944898e-01, -1.45684928e-01,\n",
       "        1.77671283e-01,  3.18231195e-01, -3.87550116e-01,  1.38486981e-01,\n",
       "       -1.05099469e-01,  1.35701388e-01,  2.03291714e-01, -1.70869976e-01,\n",
       "        1.24309823e-01, -1.39292642e-01,  9.14501101e-02, -2.23273978e-01,\n",
       "        2.57606447e-01,  2.65174806e-01,  6.02744222e-02, -4.30236384e-02,\n",
       "       -1.11364447e-01,  5.96083365e-02, -8.72399881e-02,  1.57256201e-01,\n",
       "        1.98825896e-02,  5.03073931e-02, -3.17308068e-01,  5.25218621e-02,\n",
       "       -1.85600176e-01,  9.44966003e-02, -1.70663714e-01,  5.25478981e-02,\n",
       "        2.96930932e-02,  3.90870264e-03,  7.86724240e-02, -1.05689093e-03,\n",
       "       -6.82214051e-02,  8.38978961e-02, -3.35178711e-02,  2.84304589e-01,\n",
       "        2.36499161e-01,  1.47517145e-01,  8.66151601e-02, -1.81072459e-01,\n",
       "       -1.96698904e-02,  1.57579109e-01, -1.92496926e-03, -2.09096923e-01,\n",
       "       -1.58170134e-01,  5.24469987e-02, -1.35565549e-03,  1.33539885e-01,\n",
       "        1.78045243e-01, -1.06197000e-02, -4.36103493e-02,  3.63907933e-01,\n",
       "        3.28459620e-01, -1.43535599e-01,  1.16977558e-01,  1.54407874e-01,\n",
       "        2.84364820e-03, -3.21803659e-01, -1.41427889e-01, -1.41040727e-01,\n",
       "        1.16746262e-01, -7.78113678e-02, -3.88078317e-02, -2.05975980e-01,\n",
       "       -1.06396735e-01,  8.20268393e-02, -9.63085964e-02, -1.68748811e-01,\n",
       "        1.53609887e-01, -8.22425932e-02, -1.61294565e-02, -1.21433571e-01,\n",
       "       -1.49345025e-01, -2.24692188e-02,  2.30898291e-01,  9.56578739e-03,\n",
       "       -1.14168301e-01, -8.27038288e-03,  1.00167826e-01,  2.50400811e-01,\n",
       "       -7.10021630e-02, -5.84701374e-02, -9.67250019e-02, -1.63447440e-01,\n",
       "       -2.67274618e-01,  4.18512300e-02,  8.01496953e-02, -3.89249250e-02,\n",
       "       -1.09632500e-01,  1.33213878e-01, -1.98018625e-02,  1.20433904e-02,\n",
       "        1.79826140e-01,  2.21650973e-01,  5.44003583e-02, -1.59740448e-05,\n",
       "       -9.93075520e-02,  4.88293450e-03,  2.38708407e-03,  3.62814218e-01,\n",
       "        2.82487795e-02,  7.17111006e-02,  2.49472111e-02, -4.52988818e-02,\n",
       "       -3.81362140e-02, -5.15649468e-02,  2.57485330e-01, -9.33613256e-03,\n",
       "       -6.18604161e-02, -2.81135384e-02,  1.32598415e-01,  1.72094434e-01,\n",
       "        9.54440534e-02, -2.37655520e-01, -1.80550635e-01,  2.51775891e-01,\n",
       "       -4.10698615e-02, -1.87859088e-01, -1.35056712e-02, -2.08016485e-02,\n",
       "        2.80811697e-01, -1.64679930e-01,  2.17366546e-01,  1.58944771e-01,\n",
       "       -4.29793969e-02,  2.54787683e-01, -1.91771209e-01,  1.81889355e-01,\n",
       "        1.62545964e-01, -1.09660417e-01,  8.10352564e-02,  1.40040651e-01,\n",
       "        4.12505120e-03,  4.69538271e-02, -7.55614787e-02, -5.12436211e-01,\n",
       "        3.23276758e-01,  1.48093641e-01, -1.14473954e-01,  1.21638924e-02,\n",
       "       -1.14913434e-01,  2.81020492e-01,  6.87991604e-02, -2.10273132e-01,\n",
       "        2.14323848e-01, -2.72452205e-01,  6.00721240e-02, -5.16775064e-03,\n",
       "        2.71047294e-01,  2.09529966e-01,  1.19500108e-01, -2.06246078e-02,\n",
       "       -2.81132162e-01,  7.01486245e-02, -3.71465348e-02,  2.43571073e-01,\n",
       "        5.05249128e-02,  2.78368384e-01,  2.23756909e-01,  1.20734997e-01,\n",
       "        9.57269073e-02,  2.14078724e-02,  6.03707433e-02, -1.50995702e-01,\n",
       "        8.34118575e-02,  2.72654951e-01, -5.52794524e-02,  8.17072019e-02,\n",
       "       -2.57179439e-02,  1.21474534e-01,  3.06730419e-01, -1.83703601e-01,\n",
       "        1.31079406e-01,  2.59748459e-01, -1.71105772e-01,  1.12223893e-01,\n",
       "        1.75267428e-01, -1.28129721e-02,  1.92294538e-01,  5.61143309e-02,\n",
       "        4.48231325e-02,  2.87542194e-01,  1.11761123e-01,  9.83986109e-02,\n",
       "        2.07739323e-01, -1.20437980e-01,  1.17418468e-01,  2.90116742e-02,\n",
       "       -2.01227993e-01, -7.90366679e-02, -1.51033159e-02, -1.30126625e-02,\n",
       "       -9.51851085e-02,  1.25735700e-02, -2.08578393e-01,  4.85891476e-03,\n",
       "        8.88678581e-02,  2.06115097e-01,  4.28253375e-02,  4.29691859e-02,\n",
       "        2.26800963e-02,  2.23484248e-01,  1.05923101e-01, -5.47512621e-02,\n",
       "        2.77482904e-02, -9.09721851e-02,  2.01319069e-01,  7.00249895e-03,\n",
       "        1.93106294e-01,  4.41445373e-02, -1.37465615e-02, -2.96661705e-01,\n",
       "       -7.21250549e-02, -3.57294753e-02, -9.09278542e-02,  1.20054029e-01,\n",
       "        1.70667395e-01, -2.54131138e-01,  2.14243934e-01, -1.17644757e-01,\n",
       "        9.83435512e-02,  6.58015981e-02, -9.94444713e-02, -3.19661409e-01,\n",
       "        2.33328789e-01,  1.33826762e-01, -1.06701434e-01,  4.28169146e-02,\n",
       "        6.82872683e-02,  1.73110873e-01,  5.72999045e-02, -1.90228209e-01,\n",
       "       -1.47427678e-01,  2.27702737e-01,  3.44721600e-02, -5.07902578e-02,\n",
       "        7.02113509e-02, -1.72802687e-01, -9.85324234e-02,  3.26666459e-02,\n",
       "       -1.96010351e-01, -6.25237152e-02, -4.27084491e-02,  1.48135096e-01,\n",
       "       -2.82395899e-01,  1.12219669e-01, -9.71483141e-02,  1.08037397e-01,\n",
       "       -1.74854577e-01, -2.42618099e-02,  1.07621491e-01,  9.60434228e-02,\n",
       "        4.22468670e-02, -3.28735784e-02,  1.18553653e-01,  2.32916832e-01,\n",
       "        1.90050691e-01, -9.10579190e-02,  4.14975733e-01,  1.43490255e-01,\n",
       "        5.55760451e-02,  1.32838160e-01,  3.34881037e-01,  5.85687235e-02,\n",
       "        6.84086159e-02,  7.21669793e-02, -1.34318113e-01,  9.60158855e-02,\n",
       "       -1.59656391e-01, -6.69039786e-03, -7.15199411e-02, -1.07623562e-01,\n",
       "        1.28388375e-01, -1.67945713e-01,  3.35906208e-01,  1.98486745e-01,\n",
       "       -4.16123345e-02, -3.06370765e-01, -1.48574904e-01,  3.02260481e-02,\n",
       "       -7.15667605e-02,  1.02613159e-02, -2.19856814e-01,  1.21370010e-01,\n",
       "        2.09813535e-01, -1.49144068e-01, -2.27242149e-02, -9.19120014e-02,\n",
       "       -7.85933435e-02,  1.49443716e-01,  1.63351864e-01, -1.35934874e-01,\n",
       "       -1.27314702e-01, -1.66492537e-02, -7.07650930e-02, -2.32006237e-02,\n",
       "        1.56630874e-01, -1.38354942e-01,  1.09988973e-01, -8.46767649e-02,\n",
       "        2.33088240e-01, -2.66831875e-01,  1.18204169e-02,  1.00317568e-01,\n",
       "        7.27736205e-02, -2.43472397e-01,  1.15221672e-01,  1.30485862e-01,\n",
       "        1.80570439e-01,  8.97386856e-03,  5.50009869e-02,  5.39868735e-02,\n",
       "        1.56628177e-01,  1.84021026e-01,  9.18419361e-02,  2.07420737e-01,\n",
       "       -1.21091433e-01, -1.04261078e-01,  8.02391022e-02,  6.83686957e-02,\n",
       "        3.73614520e-01,  3.14177983e-02, -4.27574329e-02,  1.66034430e-01,\n",
       "        1.76892597e-02, -3.21981050e-02,  2.58408785e-02, -2.65410393e-02,\n",
       "        8.25869758e-03, -1.69332355e-01,  1.71659887e-01,  4.45934525e-03,\n",
       "       -3.14371511e-02, -5.14396466e-02, -1.17563799e-01, -6.15183190e-02,\n",
       "        9.24739242e-03,  4.38381843e-02, -1.71670377e-01, -6.54966459e-02,\n",
       "        1.89694911e-02, -1.13118708e-01, -1.68198377e-01, -7.04121590e-02,\n",
       "       -1.97749138e-02, -5.03671691e-02, -7.89410248e-03,  9.27044079e-03,\n",
       "       -1.23837747e-01,  1.49337232e-01, -1.34346187e-02, -4.64165173e-02,\n",
       "       -7.88813643e-03, -1.69540405e-01,  1.42077491e-01, -6.11776412e-02,\n",
       "       -3.19144815e-01,  2.58426294e-02, -1.78591400e-01, -2.39100158e-01,\n",
       "        6.44768327e-02,  2.07931176e-03, -4.72212769e-02, -3.38750303e-01,\n",
       "       -3.00750136e-04, -2.00204179e-02,  1.50527582e-01,  1.74028486e-01,\n",
       "       -3.14174712e-01, -3.18715461e-02,  2.35564619e-01,  8.39074627e-02,\n",
       "        1.25215784e-01,  9.07751173e-03,  1.37846142e-01,  1.86413303e-01,\n",
       "        1.10946596e-01,  4.42354605e-02, -1.68076336e-01, -6.87485486e-02,\n",
       "        1.69376642e-01,  3.30896676e-03, -2.31722221e-02, -7.85727799e-03,\n",
       "        1.52144386e-02,  7.43778199e-02, -3.22849602e-02,  1.03080317e-01,\n",
       "       -6.16932958e-02,  1.13012530e-02,  6.06167167e-02, -8.11293721e-03,\n",
       "       -1.51319169e-02, -3.40532660e-02, -2.31781863e-02, -1.44042656e-01,\n",
       "        1.76639855e-02, -2.34837145e-01,  5.16671427e-02, -3.83913703e-02,\n",
       "       -2.43441910e-02,  1.29225492e-01,  1.99231476e-01,  1.18984036e-01,\n",
       "        8.07112157e-02,  1.39053985e-01, -1.69619685e-04, -4.31291433e-03,\n",
       "       -1.62510380e-01,  8.47315788e-02,  2.22810134e-02, -9.79485139e-02,\n",
       "        2.25980237e-01, -1.89049676e-01,  1.21856987e-01, -1.45658189e-02,\n",
       "        5.72173446e-02,  1.22110918e-01, -3.43536027e-02,  2.31859237e-01,\n",
       "        3.58117148e-02, -4.16780338e-02,  2.19577476e-02, -1.33153200e-01,\n",
       "        1.55553222e-01,  6.58908337e-02,  1.78108886e-01,  4.52630334e-02,\n",
       "        1.06615096e-01,  3.19849774e-02,  3.79055858e-01,  3.68325599e-02,\n",
       "       -8.48008767e-02, -2.17905611e-01, -5.90476915e-02, -1.79626018e-01,\n",
       "       -3.06966364e-01, -4.69530523e-01, -2.45361984e-01,  2.30832234e-01,\n",
       "        1.02443472e-01,  1.54213667e-01,  3.66463028e-02, -2.70364881e-01,\n",
       "       -1.42036870e-01,  1.90208375e-01,  8.16864520e-02, -1.68174267e-01,\n",
       "       -1.18704155e-01,  7.30824247e-02, -1.08038515e-01, -1.45595461e-01,\n",
       "        1.35603249e-02,  8.77831578e-02, -2.90948063e-01, -4.36806977e-02,\n",
       "       -2.18599755e-02, -9.13985744e-02, -5.23272902e-02,  2.01560892e-02,\n",
       "       -5.90383112e-02, -1.54629424e-01, -7.28187710e-02,  3.56769562e-02,\n",
       "        4.71678227e-02, -1.65586919e-03,  7.41321817e-02, -3.38459045e-01,\n",
       "       -1.64768115e-01, -8.14933628e-02,  8.28793421e-02,  2.04840720e-01,\n",
       "        4.12821546e-02, -1.37941986e-02,  1.43288374e-01, -5.12973554e-02,\n",
       "        6.10718802e-02, -1.36533584e-02, -4.51214731e-01, -2.39280798e-02,\n",
       "        1.18384078e-01, -5.40161014e-01, -3.35807055e-01, -1.08482741e-01,\n",
       "       -6.57397509e-03,  8.17316771e-02, -9.08377394e-02,  9.48181897e-02,\n",
       "        3.49377543e-01, -1.25080690e-01, -2.96466947e-01,  7.89749324e-02,\n",
       "       -6.33802190e-02, -1.50898054e-01, -2.55509913e-01,  4.84517664e-02,\n",
       "        1.35174558e-01,  7.29764700e-02, -3.04877814e-02, -8.19886476e-02,\n",
       "       -1.70084462e-01,  1.94895193e-01,  2.32917480e-02,  7.96707049e-02,\n",
       "       -1.13909818e-01,  1.68204740e-01, -8.66214838e-03,  4.22080606e-03,\n",
       "        6.95541501e-03,  1.48016125e-01,  5.01022711e-02, -1.20396756e-01,\n",
       "        2.83337414e-01, -3.47635373e-02,  1.17587984e-01,  1.66253045e-01,\n",
       "       -8.45528916e-02,  3.55906785e-01, -2.99423970e-02,  4.71518934e-03,\n",
       "        2.27763653e-02,  1.52365386e-01, -4.28475849e-02, -7.15086088e-02,\n",
       "       -1.66595832e-01, -1.78473853e-02,  1.20108522e-01, -9.40418392e-02,\n",
       "        6.06597625e-02, -3.49631794e-02,  1.01526640e-02,  7.33784586e-02,\n",
       "        9.48322490e-02, -1.28458768e-01, -1.96678489e-01, -7.37394765e-02,\n",
       "       -1.53580993e-01, -5.91962188e-02, -1.78037360e-02,  2.01527268e-01,\n",
       "       -1.59936011e-01,  1.51845664e-02, -4.81478386e-02,  4.50078547e-02,\n",
       "        7.19841644e-02,  4.60779369e-02, -2.57471144e-01, -3.34817544e-02,\n",
       "       -3.13381314e-01,  5.87949604e-02,  7.55175203e-02,  1.17862493e-01,\n",
       "       -1.77163124e-01,  5.87680414e-02,  2.39789039e-02, -5.87395988e-02,\n",
       "        2.38105118e-01, -2.95949161e-01, -1.65956974e-01,  1.13459872e-02,\n",
       "       -9.43712741e-02, -2.59541497e-02, -3.91002744e-01,  9.69361514e-03,\n",
       "        4.58048247e-02,  8.27248245e-02, -1.60008237e-01, -1.59729600e-01,\n",
       "       -1.03350013e-01, -1.41877964e-01,  1.41779348e-01, -2.58843243e-01,\n",
       "        7.28308707e-02, -1.43969640e-01, -1.96585685e-01,  3.88807543e-02,\n",
       "        5.34303784e-02,  1.40571699e-01, -2.88314670e-02,  3.62033173e-02,\n",
       "       -1.43949851e-01,  1.16582915e-01,  3.25497389e-01, -7.86455423e-02,\n",
       "       -2.04120964e-01,  8.60311687e-02, -5.51437549e-02, -8.77598152e-02,\n",
       "        1.98504124e-02, -1.50903672e-01,  1.64051414e-01, -2.04029121e-02,\n",
       "        1.45213991e-01, -1.85651377e-01,  1.40388921e-01,  1.82447419e-01,\n",
       "       -3.02394062e-01,  1.02255300e-01,  1.59040838e-01, -7.98771977e-02,\n",
       "        9.97654274e-02,  1.10547230e-01, -1.68365628e-01, -1.83388889e-01,\n",
       "       -1.53088830e-02,  1.81984037e-01,  2.77941674e-01,  2.52358258e-01,\n",
       "       -2.49769241e-01,  1.99677587e-01,  2.17406541e-01, -3.97141650e-02,\n",
       "        1.67970523e-01, -3.47817317e-02, -1.90912649e-01,  3.32801700e-01,\n",
       "       -3.36467475e-03,  7.11534321e-02,  1.42974854e-01, -1.13277346e-01,\n",
       "        2.89390415e-01, -1.22241210e-02, -1.13263994e-01,  4.34265852e-01,\n",
       "        4.11671430e-01,  1.62717894e-01, -6.59641773e-02,  5.96385300e-02,\n",
       "        1.05811179e-01,  1.46182239e-01,  7.41886422e-02, -8.74630362e-02,\n",
       "       -8.36880654e-02, -1.28679082e-01,  2.60367811e-01,  1.08746782e-01,\n",
       "       -8.60203803e-02,  1.11941762e-01,  1.57412291e-02, -1.65263433e-02,\n",
       "        2.92397160e-02, -1.50352996e-02, -3.36176082e-02,  2.68480420e-01,\n",
       "       -9.78345275e-02,  2.07967963e-02,  1.27875507e-01, -4.08427790e-03,\n",
       "       -2.45908499e-02,  1.78359702e-01,  1.38917118e-02,  2.28043646e-05,\n",
       "        2.26909593e-02, -2.22606644e-01, -4.00838330e-02,  2.32521053e-02,\n",
       "       -2.22429603e-01, -3.04735936e-02, -1.33497760e-01,  2.69895270e-02,\n",
       "        1.00307256e-01, -7.80656189e-03,  1.66559637e-01,  7.01473653e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[3564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_train = np.zeros(shape=(1,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.append(gcn_train, [embedding[node2id[arr[0].split(';')[-1]]]], axis=0)\n",
    "b = np.append(b, [embedding[node2id[arr[1].split(';')[-1]]]], axis=0)\n",
    "b = np.append(b, [embedding[node2id[arr[2].split(';')[-1]]]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'afroze ibrahim baqapuri'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-b35a31d285c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnode2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'afroze ibrahim baqapuri'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'afroze ibrahim baqapuri'"
     ]
    }
   ],
   "source": [
    "node2id['afroze ibrahim baqapuri'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = zip([1,2], [9,10], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2013, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetMatrix = np.zeros(shape=(10, 3))\n",
    "targets = np.array([0,2,2,1,0,2,1,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0]\n",
      "1 [2]\n",
      "2 [2]\n",
      "3 [1]\n",
      "4 [0]\n",
      "5 [2]\n",
      "6 [1]\n",
      "7 [1]\n",
      "8 [0]\n",
      "9 [0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,targets.shape[0]):\n",
    "    print(i, targets[[i]])\n",
    "    targetMatrix[i,targets[[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 2, 1, 0, 2, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ('MM', ['hi', 'bye'], 'MM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MM', ['hi', 'bye'], 'MM')"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 5)\n",
    "y = torch.randint(5, (10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6558, -0.6311, -1.1523, -0.4526, -0.1082],\n",
       "        [-0.0242,  0.2569,  1.0637, -1.3420,  0.7725],\n",
       "        [-1.3376,  0.0537, -0.2791, -1.9397,  0.3444],\n",
       "        [-0.7768, -0.5065,  0.4023,  0.4000,  1.2417],\n",
       "        [ 0.8871,  0.8222,  1.1341,  0.0854, -0.6681],\n",
       "        [ 1.4926, -1.0424, -0.1974, -1.4365,  2.7164],\n",
       "        [ 0.4590,  1.7561, -0.7182, -0.6271, -0.7176],\n",
       "        [-0.5132,  1.5448,  1.0711,  0.4586,  0.7220],\n",
       "        [-0.3532,  0.9306,  0.5386, -1.3265, -1.6723],\n",
       "        [ 1.5191,  0.5804,  0.7835,  1.4071,  0.4309]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 1, 2, 3, 0, 3, 4, 3, 1])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8631)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "outputs = torch.randn(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5141089 ,  0.21601109,  0.3352844 , -0.47060046,  0.52831715,\n",
       "         0.8986401 , -1.3377849 , -0.21681648,  0.3495095 ,  0.04451937],\n",
       "       [ 0.82896525, -0.43477282, -0.6625765 , -1.1890138 ,  1.5161456 ,\n",
       "         0.7036071 ,  0.09192441,  0.15651953,  0.834813  ,  0.8876868 ],\n",
       "       [-1.6549444 ,  0.4601838 , -0.8080143 , -1.2026032 , -0.7485504 ,\n",
       "        -0.01099867, -0.15947303,  1.3419424 , -0.48343274, -1.5495147 ],\n",
       "       [-0.38402694,  0.20689583,  1.091941  ,  0.8155733 , -0.90273005,\n",
       "         0.06526163,  1.5922868 , -0.741216  , -0.12303007,  0.14203379],\n",
       "       [ 0.80925745, -0.8369664 ,  0.69213337,  0.41236097,  0.049534  ,\n",
       "        -1.89061   , -1.6556381 , -0.33929276, -0.18388875,  0.22856106],\n",
       "       [ 0.87471455,  0.5971452 ,  1.2274413 ,  0.62335396,  0.16428806,\n",
       "         0.7154189 , -0.5671112 , -1.0328497 ,  0.64456457,  0.32417497],\n",
       "       [ 0.35605755,  1.5302416 ,  0.04111671, -0.32558602, -1.2731838 ,\n",
       "        -0.6747714 ,  1.0610003 ,  1.7816477 ,  1.7482193 ,  0.56619126],\n",
       "       [ 1.10179   ,  0.41875717,  0.22219831,  1.0217499 ,  0.06200692,\n",
       "        -0.67268586,  0.34433922, -0.8202877 ,  0.33465925, -0.4524024 ],\n",
       "       [-1.2638752 ,  0.11909619, -0.22477137,  0.0295809 ,  0.2942076 ,\n",
       "         0.91019106, -0.2832178 , -0.83100337,  1.4686389 , -1.1637827 ],\n",
       "       [ 2.5199976 , -0.8440718 ,  0.08546928, -0.12223571,  1.0752045 ,\n",
       "         2.031304  , -0.7309149 , -0.01538379, -0.87354994,  0.7686104 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = outputs.cpu().detach().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.,  5.,  6.,  2.,  8.,  9.,  1.,  3.,  7.,  4.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.rankdata(temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "strr = 'Ashish'\n",
    "final = 'Hi '+strr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Ashish'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(anArray)):\n",
    "    if (anArray[i]).size > 1:\n",
    "        print(anArray[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "y_true = np.array([[0,0,0,1], [1,0,0,0], [0,0,1,0], [0,1,0,0]])\n",
    "y_scores = np.array([[0.2, 0.4, 0.1, 0.3],[0.2, 0.4, 0.1, 0.3],[0.2, 0.4, 0.1, 0.3],[0.2, 0.4, 0.1, 0.3]])\n",
    "average_precision_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = np.zeros(y_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-81-4ea62681798a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-81-4ea62681798a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    2.7238273e-06\t3.7461843e-06\t1.265971e-05\t3.3758e-07\t9.279866e-06\t1.6720429e-05\t1.824848e-06\t3.5331097e-06\t4.6212972e-05\t7.5340304e-06\t1.3375603e-05\t5.8317723e-06\t1.6728323e-05\t1.3344337e-05\t2.502062e-06\t7.686143e-06\t3.6247577e-06\t2.6900893e-06\t4.1882477e-06\t4.1786484e-06\t3.1038024e-05\t1.3613994e-05\t3.1531556e-05\t0.0001705549\t1.0515591e-06\t6.226058e-07\t1.3344617e-05\t1.32381165e-05\t5.647988e-06\t4.211426e-06\t1.9001794e-05\t1.533258e-06\t1.2448721e-05\t1.2053849e-06\t4.655062e-06\t2.5963097e-06\t8.927007e-06\t8.872375e-06\t9.8699e-06\t8.086413e-06\t5.334364e-06\t2.8298093e-05\t1.625008e-06\t1.2331382e-06\t2.8962766e-06\t4.027779e-06\t1.7039141e-05\t6.4360876e-07\t1.8116272e-05\t5.7249916e-05\t6.915115e-07\t9.615866e-06\t1.3205503e-06\t1.964617e-05\t1.5107306e-05\t3.810473e-06\t2.2752158e-06\t1.2288087e-06\t3.906334e-06\t2.5860532e-05\t2.0596874e-06\t1.1828263e-06\t2.0506744e-05\t2.4224755e-05\t1.623367e-05\t1.6677709e-06\t2.6616704e-05\t2.506845e-05\t2.0906598e-05\t0.9910842\t6.1014944e-06\t1.8171018e-05\t6.1250034e-07\t3.95839e-06\t1.0388141e-05\t1.2238004e-05\t3.2025584e-06\t1.8848534e-06\t2.1424653e-06\t6.491669e-06\t4.792133e-06\t3.0608521e-06\t3.2519274e-06\t2.6033983e-06\t1.59826e-05\t1.701162e-05\t4.849033e-06\t5.3221834e-06\t1.11324625e-05\t3.38234e-06\t0.0036893282\t4.2724996e-06\t1.7663771e-06\t1.3571179e-05\t7.145162e-06\t1.7716577e-06\t0.00015804224\t2.2444365e-06\t2.5698257e-06\t3.2936161e-06\t4.3347395e-06\t6.635959e-07\t5.559525e-07\t2.801928e-06\t1.1720612e-06\t2.484898e-06\t1.0942005e-06\t1.2080994e-05\t2.8027246e-06\t5.375704e-06\t7.0189267e-06\t2.9735172e-06\t1.5108423e-06\t1.7312082e-06\t1.1113765e-06\t4.139015e-06\t1.2285275e-06\t1.3966699e-05\t1.2151646e-06\t8.95448e-06\t0.00016236558\t5.6120713e-05\t3.328305e-06\t5.283385e-06\t1.569696e-06\t1.6863072e-06\t1.8713826e-06\t4.0135478e-07\t2.432473e-06\t2.240928e-05\t6.6182915e-06\t3.786779e-05\t1.4243064e-05\t2.8819934e-06\t1.0946801e-05\t1.9886891e-05\t7.146978e-05\t6.198873e-06\t3.0974584e-06\t3.9934654e-05\t1.7571782e-06\t0.00011273839\t3.4401944e-06\t5.5793666e-06\t1.1721043e-05\t6.095027e-06\t3.8086455e-06\t2.1973613e-06\t2.7911215e-06\t0.00034054212\t1.0138281e-05\t4.196884e-06\t4.3045025e-06\t3.9511447e-06\t2.5318346e-05\t1.0021572e-05\t1.7890284e-06\t3.3615922e-06\t1.5512798e-06\t4.0829076e-05\t1.5265042e-06\t2.811996e-06\t4.7768795e-06\t3.8691796e-06\t2.0826401e-05\t3.878061e-06\t2.885296e-06\t2.163705e-06\t1.8394718e-05\t4.822106e-06\t6.3337166e-06\t8.871595e-06\t4.2436534e-07\t3.9591337e-06\t1.2672767e-05\t1.1037383e-05\t2.8612105e-06\t5.0546273e-06\t0.00013643804\t1.941836e-05\t1.9276183e-06\t1.9652962e-06\t3.42998e-06\t1.0449043e-06\t2.6458098e-05\t7.3336625e-05\t2.1081835e-06\t1.3402098e-05\t3.6863858e-06\t5.6023996e-06\t2.1025637e-06\t5.5593557e-07\t1.8964129e-06\t6.493799e-06\t1.5424757e-06\t8.490561e-06\t1.281264e-05\t2.2847285e-06\t1.5798732e-05\t3.5605824e-06\t2.9933772e-06\t1.2019322e-06\t1.393215e-05\t2.867493e-06\t7.143479e-06\t6.7581806e-07\t3.9703823e-06\t0.00015692005\t4.695946e-06\t1.668537e-05\t1.3058633e-05\t1.4839759e-06\t2.2768536e-05\t1.2562994e-06\t1.0513851e-05\t8.585293e-07\t4.4472495e-06\t9.1685515e-06\t6.493644e-06\t3.5576975e-06\t5.8542984e-05\t4.4309988e-07\t1.3856297e-06\t6.731885e-05\t1.3337701e-06\t8.064987e-06\t5.346913e-06\t3.6019146e-06\t1.032206e-06\t1.251925e-05\t5.929818e-07\t1.2593727e-06\t3.5811786e-06\t8.891924e-06\t9.715371e-06\t1.6467964e-06\t4.0842956e-06\t5.6837366e-06\t4.1131743e-06\t3.6436813e-06\t7.802876e-07\t7.376766e-07\t4.1779138e-05\t5.816195e-07\t2.7639285e-06\t3.4708285e-06\t1.0104064e-05\t4.2286388e-06\t3.2156047e-06\t9.221359e-06\t2.9541416e-06\t7.26541e-06\t2.740413e-05\t1.130119e-06\t5.0945773e-06\t2.0548385e-05\t4.3916784e-06\t1.8623916e-06\t7.711595e-07\t2.2480547e-06\t3.3151773e-06\t6.49913e-07\t6.2591025e-06\t1.027924e-06\t1.9901272e-05\t6.172822e-06\t0.0001172476\t2.7378621e-06\t1.7660486e-06\t1.0851312e-05\t3.875824e-06\t1.3513829e-06\t2.7058231e-06\t3.5302317e-05\t4.988219e-07\t4.7270052e-05\t5.9562326e-06\t3.222417e-06\t1.2091021e-05\t3.9843785e-06\t2.6244627e-06\t1.2384281e-05\t2.8262532e-06\t3.178751e-06\t8.0118645e-07\t8.208152e-07\t7.792677e-06\t9.1134433e-07\t2.0987031e-06\t1.2583835e-06\t1.1799398e-06\t4.3159866e-06\t7.1084055e-06\t1.9460933e-06\t3.1469756e-07\t3.4626626e-06\t7.2766297e-06\t1.3315055e-06\t5.093431e-06\t1.5756879e-06\t2.9778932e-06\t4.3557657e-06\t9.443205e-06\t8.356261e-05\t1.344465e-07\t4.4268872e-06\t5.5227196e-07\t1.2906341e-06\t2.9184666e-06\t1.1081983e-06\t3.412703e-06\t5.0016456e-06\t1.9656393e-06\t4.6688918e-05\t1.0781821e-06\t7.947264e-07\t2.816556e-06\t1.3852955e-06\t2.6513111e-05\t4.488019e-06\t9.887852e-07\t3.110305e-06\t2.8196157e-05\t7.8284575e-07\t5.9861172e-06\t2.2876677e-06\t2.971411e-06\t3.2359117e-06\t1.7373989e-06\t7.6800325e-06\t4.421657e-07\t1.3316166e-05\t7.440354e-07\t1.2734597e-06\t1.5057596e-05\t6.0892517e-06\t3.3659294e-06\t9.7347365e-06\t5.019881e-06\t7.773734e-06\t7.09328e-07\t3.5385183e-06\t2.1494348e-05\t1.3230726e-06\t5.973366e-06\t1.0449627e-05\t1.5694035e-05\t4.340543e-06\t1.117573e-06\t2.2725203e-06\t5.3010825e-07\t6.320432e-07\t5.9384797e-06\t1.7516269e-06\t1.1202128e-06\t3.3830436e-06\t4.5659633e-07\t5.638637e-07\t2.5759944e-06\t4.955699e-06\t9.799922e-06\t4.163978e-06\t6.4187625e-06\t9.880985e-06\t2.7369508e-06\t1.361095e-06\t2.2143947e-06\t5.226326e-06\t3.1822908e-06\t1.8897094e-06\t2.447701e-06\t3.1976144e-06\t2.633137e-06\t6.6046596e-06\t4.4828557e-06\t2.3847434e-07\t3.4667828e-06\t1.9175507e-06\t3.7394632e-06\t5.7285883e-06\t1.3616514e-05\t1.3258807e-06\t1.4461924e-05\t9.299729e-06\t6.685927e-07\t4.7503618e-06\t2.1237793e-05\t2.2557679e-05\t9.4453804e-07\t2.1081312e-06\t7.424888e-05\t1.0252008e-05\t3.4135235e-06\t6.9209773e-06\t7.956493e-07\t3.4614077e-06\t1.3028865e-05\t1.4877635e-06\t6.7604597e-06\t3.0736378e-06\t1.4857231e-06\t1.909483e-06\t1.645997e-06\t2.7914941e-06\t2.7357898e-06\t2.3038834e-05\t4.173093e-06\t1.2752292e-06\t2.9050263e-06\t5.01973e-07\t4.3750997e-06\t3.6789822e-06\t7.597775e-05\t2.0351827e-06\t1.0940665e-05\t1.5544297e-06\t6.4727794e-07\t6.46587e-07\t1.6574297e-06\t9.78427e-06\t8.24387e-07\t4.8496895e-06\t2.5718475e-05\t3.3312995e-06\t2.6689463e-06\t3.713989e-07\t3.5276607e-05\t1.7916962e-05\t2.8609675e-06\t2.4095161e-06\t3.3994372e-06\t3.99127e-06\t2.996919e-06\t5.6677527e-06\t1.0631726e-06\t1.2816679e-06\t4.021588e-06\t7.376629e-06\t6.644265e-06\t4.8764446e-06\t2.5443837e-06\t1.2139513e-05\t6.895244e-06\t4.502492e-06\t1.8073251e-05\t3.741211e-06\t6.680194e-06\t2.0938094e-06\t1.3397147e-06\t1.2309504e-06\t1.478502e-06\t1.3526038e-06\t6.1143993e-07\t1.0583627e-05\t3.4522957e-06\t1.6293772e-05\t2.2315965e-06\t3.2097896e-06\t4.0552563e-06\t1.8213073e-05\t8.7823685e-07\t8.690003e-06\t1.6798787e-06\t2.3796147e-06\t2.0241063e-05\t1.1710645e-06\t2.2386007e-06\t1.928274e-05\t3.5217226e-06\t2.5107531e-06\t8.550464e-06\t9.1450823e-07\t1.4203158e-05\t1.6009677e-05\t2.6881223e-06\t1.5860505e-05\t1.6939028e-05\t1.4739188e-05\t3.5108676e-06\t4.051862e-06\t5.182524e-06\t1.5797512e-05\t2.1638629e-05\t4.6850714e-06\t2.3254412e-05\t1.8675077e-05\t8.621242e-06\t1.7273516e-05\t1.404945e-06\u001b[0m\n\u001b[0m                 \t^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
