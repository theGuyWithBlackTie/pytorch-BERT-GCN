{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3472, 0.4268, 0.2569, 0.5298, 0.6082, 0.8769, 0.6337, 0.6792, 0.1333,\n",
       "         0.0773]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.floor(x).type(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor([False, True, False])\n",
    "b = x._indices()[:,c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.LongTensor([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(i, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1, 2, 3, 4, 5, 6], shape=[3, 2])\n",
    "b = tf.constant([7, 8, 9, 10, 11, 12], shape=[2, 3])\n",
    "c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[ 27,  30,  33],\n",
       "       [ 61,  68,  75],\n",
       "       [ 95, 106, 117]], dtype=int32)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9,), dtype=int32, numpy=array([ 27,  30,  33,  61,  68,  75,  95, 106, 117], dtype=int32)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(c, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nonzero_elems = 5\n",
    "noise_shape = [num_nonzero_elems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tf.random.uniform(noise_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.floor(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = 0.7\n",
    "random_tensor = keep_prob\n",
    "temp = tf.random.uniform(noise_shape)\n",
    "print(temp)\n",
    "random_tensor += temp\n",
    "print(random_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile = pd.read_csv('../data/full_context_PeerRead.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>right_citated_text</th>\n",
       "      <th>left_citated_text</th>\n",
       "      <th>source_abstract</th>\n",
       "      <th>source_author</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_title</th>\n",
       "      <th>source_venue</th>\n",
       "      <th>source_year</th>\n",
       "      <th>target_id</th>\n",
       "      <th>target_author</th>\n",
       "      <th>target_abstract</th>\n",
       "      <th>target_year</th>\n",
       "      <th>target_title</th>\n",
       "      <th>target_venue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>Deep Neural Networks (DNNs) are powerful model...</td>\n",
       "      <td>ilya sutskever;oriol vinyals;quoc v le</td>\n",
       "      <td>1409.3215v1</td>\n",
       "      <td>Sequence to Sequence Learning with Neural Netw...</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>Modeling crisp logical regularities is crucial...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Data Recombination for Neural Semantic Parsing</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>Syntactic parsing is a fundamental problem in ...</td>\n",
       "      <td>oriol vinyals;lukasz kaiser;terry koo;slav pet...</td>\n",
       "      <td>1412.7449v1</td>\n",
       "      <td>Grammar as a Foreign Language</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>Modeling crisp logical regularities is crucial...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Data Recombination for Neural Semantic Parsing</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>Reproducibility. All code, data, and experimen...</td>\n",
       "      <td>We introduce a new neural architecture to lear...</td>\n",
       "      <td>oriol vinyals;meire fortunato;navdeep jaitly</td>\n",
       "      <td>1506.03134v1</td>\n",
       "      <td>Pointer Networks</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>Modeling crisp logical regularities is crucial...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Data Recombination for Neural Semantic Parsing</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>st like CWS and POS tagging, automatic prosody...</td>\n",
       "      <td>The recently introduced continuous Skip-gram m...</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>Distributed Representations of Words and Phras...</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1511.00360v1</td>\n",
       "      <td>chuang ding;lei xie;jie yan;weini zhang;yang liu</td>\n",
       "      <td>Prosody affects the naturalness and intelligib...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Automatic Prosody Prediction for Chinese Speec...</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>We begin by considering a document as the set ...</td>\n",
       "      <td>The recently introduced continuous Skip-gram m...</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>Distributed Representations of Words and Phras...</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>We investigate the pertinence of methods from ...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Does the Geometry of Word Embeddings Help Docu...</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>to create a sentence embedding. Second, from e...</td>\n",
       "      <td>For each word in the sentence we calculate var...</td>\n",
       "      <td>In this paper we compare different types of re...</td>\n",
       "      <td>junyoung chung;caglar gulcehre;kyunghyun cho;y...</td>\n",
       "      <td>1412.3555v1</td>\n",
       "      <td>Empirical Evaluation of Gated Recurrent Neural...</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1708.05582v1</td>\n",
       "      <td>sushant hiray;venkatesh duppada</td>\n",
       "      <td>This paper presents models for detecting agree...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Agree to Disagree: Improving Disagreement Dete...</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>for accelerating training. The network is opti...</td>\n",
       "      <td>For each Q-R pair we extract two sets of featu...</td>\n",
       "      <td>Training Deep Neural Networks is complicated b...</td>\n",
       "      <td>sergey ioffe;christian szegedy</td>\n",
       "      <td>1502.03167v1</td>\n",
       "      <td>Batch Normalization: Accelerating Deep Network...</td>\n",
       "      <td>ICML</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1708.05582v1</td>\n",
       "      <td>sushant hiray;venkatesh duppada</td>\n",
       "      <td>This paper presents models for detecting agree...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Agree to Disagree: Improving Disagreement Dete...</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>optimizer with learning rate of 0.001. We have...</td>\n",
       "      <td>For each Q-R pair we extract two sets of featu...</td>\n",
       "      <td>We introduce Adam, an algorithm for first-orde...</td>\n",
       "      <td>diederik p kingma;jimmy ba</td>\n",
       "      <td>1412.6980v1</td>\n",
       "      <td>Adam: A Method for Stochastic Optimization</td>\n",
       "      <td>iclr</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1708.05582v1</td>\n",
       "      <td>sushant hiray;venkatesh duppada</td>\n",
       "      <td>This paper presents models for detecting agree...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Agree to Disagree: Improving Disagreement Dete...</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>, and convolutional neural networks  have show...</td>\n",
       "      <td>Modeling textual or visual information with ve...</td>\n",
       "      <td>Deep Neural Networks (DNNs) are powerful model...</td>\n",
       "      <td>ilya sutskever;oriol vinyals;quoc v le</td>\n",
       "      <td>1409.3215v1</td>\n",
       "      <td>Sequence to Sequence Learning with Neural Netw...</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1606.01847v1</td>\n",
       "      <td>akira fukui;dong huk park;daylen yang;anna roh...</td>\n",
       "      <td>Modeling textual or visual information with ve...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Multimodal Compact Bilinear Pooling for Visual...</td>\n",
       "      <td>EMNLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>use concatenation and fully connected layers t...</td>\n",
       "      <td>In this paper, we propose to rely on ltimodal ...</td>\n",
       "      <td>We describe a very simple bag-of-words baselin...</td>\n",
       "      <td>bolei zhou;yuandong tian;sainbayar sukhbaatar;...</td>\n",
       "      <td>1512.02167v1</td>\n",
       "      <td>Simple Baseline for Visual Question Answering</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1606.01847v1</td>\n",
       "      <td>akira fukui;dong huk park;daylen yang;anna roh...</td>\n",
       "      <td>Modeling textual or visual information with ve...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Multimodal Compact Bilinear Pooling for Visual...</td>\n",
       "      <td>EMNLP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   right_citated_text  \\\n",
       "0   andsyntactic parsing .Because RNNs make very f...   \n",
       "1   .Because RNNs make very few domain-specific as...   \n",
       "2   ; in a Pointer Network,the only way to generat...   \n",
       "3   . Recently, nsur .  have shown superior perfor...   \n",
       "4   model trained on the Google News dataset3.In a...   \n",
       "..                                                ...   \n",
       "95  to create a sentence embedding. Second, from e...   \n",
       "96  for accelerating training. The network is opti...   \n",
       "97  optimizer with learning rate of 0.001. We have...   \n",
       "98  , and convolutional neural networks  have show...   \n",
       "99  use concatenation and fully connected layers t...   \n",
       "\n",
       "                                    left_citated_text  \\\n",
       "0   We conducted additional experiments on artific...   \n",
       "1   We conducted additional experiments on artific...   \n",
       "2   Reproducibility. All code, data, and experimen...   \n",
       "3   st like CWS and POS tagging, automatic prosody...   \n",
       "4   We begin by considering a document as the set ...   \n",
       "..                                                ...   \n",
       "95  For each word in the sentence we calculate var...   \n",
       "96  For each Q-R pair we extract two sets of featu...   \n",
       "97  For each Q-R pair we extract two sets of featu...   \n",
       "98  Modeling textual or visual information with ve...   \n",
       "99  In this paper, we propose to rely on ltimodal ...   \n",
       "\n",
       "                                      source_abstract  \\\n",
       "0   Deep Neural Networks (DNNs) are powerful model...   \n",
       "1   Syntactic parsing is a fundamental problem in ...   \n",
       "2   We introduce a new neural architecture to lear...   \n",
       "3   The recently introduced continuous Skip-gram m...   \n",
       "4   The recently introduced continuous Skip-gram m...   \n",
       "..                                                ...   \n",
       "95  In this paper we compare different types of re...   \n",
       "96  Training Deep Neural Networks is complicated b...   \n",
       "97  We introduce Adam, an algorithm for first-orde...   \n",
       "98  Deep Neural Networks (DNNs) are powerful model...   \n",
       "99  We describe a very simple bag-of-words baselin...   \n",
       "\n",
       "                                        source_author     source_id  \\\n",
       "0              ilya sutskever;oriol vinyals;quoc v le   1409.3215v1   \n",
       "1   oriol vinyals;lukasz kaiser;terry koo;slav pet...   1412.7449v1   \n",
       "2        oriol vinyals;meire fortunato;navdeep jaitly  1506.03134v1   \n",
       "3   tomas mikolov;ilya sutskever;kai chen 0010;gre...   1310.4546v1   \n",
       "4   tomas mikolov;ilya sutskever;kai chen 0010;gre...   1310.4546v1   \n",
       "..                                                ...           ...   \n",
       "95  junyoung chung;caglar gulcehre;kyunghyun cho;y...   1412.3555v1   \n",
       "96                     sergey ioffe;christian szegedy  1502.03167v1   \n",
       "97                         diederik p kingma;jimmy ba   1412.6980v1   \n",
       "98             ilya sutskever;oriol vinyals;quoc v le   1409.3215v1   \n",
       "99  bolei zhou;yuandong tian;sainbayar sukhbaatar;...  1512.02167v1   \n",
       "\n",
       "                                         source_title source_venue  \\\n",
       "0   Sequence to Sequence Learning with Neural Netw...         NIPS   \n",
       "1                       Grammar as a Foreign Language         NIPS   \n",
       "2                                    Pointer Networks         NIPS   \n",
       "3   Distributed Representations of Words and Phras...         NIPS   \n",
       "4   Distributed Representations of Words and Phras...         NIPS   \n",
       "..                                                ...          ...   \n",
       "95  Empirical Evaluation of Gated Recurrent Neural...        arxiv   \n",
       "96  Batch Normalization: Accelerating Deep Network...         ICML   \n",
       "97         Adam: A Method for Stochastic Optimization         iclr   \n",
       "98  Sequence to Sequence Learning with Neural Netw...         NIPS   \n",
       "99      Simple Baseline for Visual Question Answering        arxiv   \n",
       "\n",
       "    source_year     target_id  \\\n",
       "0        2014.0  1606.03622v1   \n",
       "1        2014.0  1606.03622v1   \n",
       "2        2015.0  1606.03622v1   \n",
       "3        2013.0  1511.00360v1   \n",
       "4        2013.0  1705.10900v1   \n",
       "..          ...           ...   \n",
       "95       2014.0  1708.05582v1   \n",
       "96       2015.0  1708.05582v1   \n",
       "97       2014.0  1708.05582v1   \n",
       "98       2014.0  1606.01847v1   \n",
       "99       2015.0  1606.01847v1   \n",
       "\n",
       "                                        target_author  \\\n",
       "0                               robin jia;percy liang   \n",
       "1                               robin jia;percy liang   \n",
       "2                               robin jia;percy liang   \n",
       "3    chuang ding;lei xie;jie yan;weini zhang;yang liu   \n",
       "4   paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "..                                                ...   \n",
       "95                    sushant hiray;venkatesh duppada   \n",
       "96                    sushant hiray;venkatesh duppada   \n",
       "97                    sushant hiray;venkatesh duppada   \n",
       "98  akira fukui;dong huk park;daylen yang;anna roh...   \n",
       "99  akira fukui;dong huk park;daylen yang;anna roh...   \n",
       "\n",
       "                                      target_abstract  target_year  \\\n",
       "0   Modeling crisp logical regularities is crucial...         2016   \n",
       "1   Modeling crisp logical regularities is crucial...         2016   \n",
       "2   Modeling crisp logical regularities is crucial...         2016   \n",
       "3   Prosody affects the naturalness and intelligib...         2015   \n",
       "4   We investigate the pertinence of methods from ...         2017   \n",
       "..                                                ...          ...   \n",
       "95  This paper presents models for detecting agree...         2017   \n",
       "96  This paper presents models for detecting agree...         2017   \n",
       "97  This paper presents models for detecting agree...         2017   \n",
       "98  Modeling textual or visual information with ve...         2016   \n",
       "99  Modeling textual or visual information with ve...         2016   \n",
       "\n",
       "                                         target_title target_venue  \n",
       "0      Data Recombination for Neural Semantic Parsing          ACL  \n",
       "1      Data Recombination for Neural Semantic Parsing          ACL  \n",
       "2      Data Recombination for Neural Semantic Parsing          ACL  \n",
       "3   Automatic Prosody Prediction for Chinese Speec...        arxiv  \n",
       "4   Does the Geometry of Word Embeddings Help Docu...        arxiv  \n",
       "..                                                ...          ...  \n",
       "95  Agree to Disagree: Improving Disagreement Dete...        arxiv  \n",
       "96  Agree to Disagree: Improving Disagreement Dete...        arxiv  \n",
       "97  Agree to Disagree: Improving Disagreement Dete...        arxiv  \n",
       "98  Multimodal Compact Bilinear Pooling for Visual...        EMNLP  \n",
       "99  Multimodal Compact Bilinear Pooling for Visual...        EMNLP  \n",
       "\n",
       "[100 rows x 14 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFile.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['left_citated_text', 'right_citated_text', 'target_id', 'source_id', 'target_year','target_author', 'source_author']\n",
    "frequency = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataFile[column]\n",
    "source_cut_data = df[['target_id', 'source_id']].drop_duplicates(subset=['target_id', 'source_id'])\n",
    "source_cut = source_cut_data.source_id.value_counts()[(source_cut_data.source_id.value_counts() >= frequency)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id = np.sort(source_cut.keys())\n",
    "source_id\n",
    "df = df.loc[df['source_id'].isin(source_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0705.4485v1', '0806.4686v1', '0812.4952v1', '0902.1284v1',\n",
       "       '0902.2206v1', '0907.1815v1', '0911.5708v1', '0912.0071v1',\n",
       "       '0912.3995v1', '1006.2588v1', '1010.3091v1', '1010.5511v1',\n",
       "       '1103.0398v1', '1104.4803v1', '1105.1033v1', '1105.5379v1',\n",
       "       '1106.2436v1', '1106.4574v1', '1107.4080v1', '1107.4557v1',\n",
       "       '1109.3701v1', '1109.3843v1', '1109.5647v1', '1109.6841v1',\n",
       "       '1112.6209v1', '1201.0292v1', '1201.0490v1', '1202.6258v1',\n",
       "       '1204.0136v1', '1204.2742v1', '1204.3514v1', '1204.3968v1',\n",
       "       '1204.6703v1', '1205.2661v1', '1206.1106v1', '1206.1270v1',\n",
       "       '1206.2944v1', '1206.3255v1', '1206.4657v1', '1206.4683v1',\n",
       "       '1206.6230v1', '1206.6380v1', '1206.6389v1', '1206.6392v1',\n",
       "       '1206.6398v1', '1206.6400v1', '1206.6417v1', '1206.6418v1',\n",
       "       '1206.6423v1', '1206.6426v1', '1206.6430v1', '1206.6448v1',\n",
       "       '1206.6487v1', '1207.4404v1', '1207.4747v1', '1209.3352v1',\n",
       "       '1210.5644v1', '1211.3966v1', '1211.5063v1', '1211.5590v1',\n",
       "       '1211.7012v1', '1212.0901v1', '1212.1824v1', '1212.4777v1',\n",
       "       '1212.5701v1', '1301.3224v1', '1301.3485v1', '1301.3584v1',\n",
       "       '1301.3666v1', '1301.3781v1', '1302.0723v1', '1302.4389v1',\n",
       "       '1305.0445v1', '1305.2982v1', '1305.3120v1', '1305.6663v1',\n",
       "       '1306.0160v1', '1306.0186v1', '1306.0543v1', '1306.0940v1',\n",
       "       '1306.1091v1', '1306.2119v1', '1306.3888v1', '1307.0032v1',\n",
       "       '1307.1493v1', '1307.1662v1', '1307.5101v1', '1307.7973v1',\n",
       "       '1308.0850v1', '1309.2375v1', '1310.4546v1', '1310.6343v1',\n",
       "       '1310.8499v1', '1311.1869v1', '1311.2495v1', '1311.4296v1',\n",
       "       '1312.3005v1', '1312.3393v1', '1312.4400v1', '1312.5602v1',\n",
       "       '1312.5851v1', '1312.6173v1', '1312.6184v1', '1312.6199v1',\n",
       "       '1312.6203v1', '1401.0514v1', '1401.3492v1', '1401.4082v1',\n",
       "       '1401.5390v1', '1402.0030v1', '1402.0119v1', '1402.0555v1',\n",
       "       '1402.1454v1', '1402.1869v1', '1402.3511v1', '1402.4102v1',\n",
       "       '1404.0736v1', '1404.2188v1', '1404.4641v1', '1405.3162v1',\n",
       "       '1405.4053v1', '1405.4273v1', '1405.5869v1', '1406.1078v1',\n",
       "       '1406.1822v1', '1406.2541v1', '1406.2572v1', '1406.2751v1',\n",
       "       '1406.3332v1', '1406.3676v1', '1406.5298v1', '1406.5679v1',\n",
       "       '1406.6247v1', '1407.0202v1', '1407.3068v1', '1408.5093v1',\n",
       "       '1408.5882v1', '1409.1458v1', '1409.2848v1', '1409.3215v1',\n",
       "       '1409.7495v1', '1410.0210v1', '1410.0759v1', '1410.1090v1',\n",
       "       '1410.1141v1', '1410.2455v1', '1410.8516v1', '1411.1147v1',\n",
       "       '1411.1792v1', '1411.4166v1', '1411.5654v1', '1411.6081v1',\n",
       "       '1412.0233v1', '1412.1058v1', '1412.1632v1', '1412.2007v1',\n",
       "       '1412.3555v1', '1412.4729v1', '1412.5068v1', '1412.5567v1',\n",
       "       '1412.6115v1', '1412.6550v1', '1412.6553v1', '1412.6564v1',\n",
       "       '1412.6568v1', '1412.6575v1', '1412.6583v1', '1412.6604v1',\n",
       "       '1412.6632v1', '1412.6651v1', '1412.6806v1', '1412.6980v1',\n",
       "       '1412.7024v1', '1412.7062v1', '1412.7449v1', '1412.7580v1',\n",
       "       '1412.7753v1', '1412.7755v1', '1501.02598v1', '1501.03796v1',\n",
       "       '1502.02367v1', '1502.02551v1', '1502.02761v1', '1502.02791v1',\n",
       "       '1502.03044v1', '1502.03167v1', '1502.03492v1', '1502.03508v1',\n",
       "       '1502.04390v1', '1502.04623v1', '1502.04681v1', '1502.05477v1',\n",
       "       '1503.00075v1', '1503.00185v1', '1503.01007v1', '1503.01070v1',\n",
       "       '1503.01838v1', '1503.02364v1', '1503.03167v1', '1503.03244v1',\n",
       "       '1503.03535v1', '1503.04069v1', '1503.04269v1', '1503.05671v1',\n",
       "       '1504.00548v1', '1504.04788v1', '1504.06580v1', '1504.06654v1',\n",
       "       '1505.00387v1', '1505.01809v1', '1505.02074v1', '1505.05008v1',\n",
       "       '1505.05612v1', '1505.05770v1', '1505.05899v1', '1505.08075v1',\n",
       "       '1506.00019v1', '1506.00333v1', '1506.01057v1', '1506.01066v1',\n",
       "       '1506.01070v1', '1506.01094v1', '1506.01900v1', '1506.02075v1',\n",
       "       '1506.02078v1', '1506.02142v1', '1506.02216v1', '1506.02438v1',\n",
       "       '1506.02516v1', '1506.02617v1', '1506.02626v1', '1506.03099v1',\n",
       "       '1506.03134v1', '1506.03340v1', '1506.03487v1', '1506.04089v1',\n",
       "       '1506.05254v1', '1506.05865v1', '1506.05869v1', '1506.06158v1',\n",
       "       '1506.06579v1', '1506.06714v1', '1506.06726v1', '1506.06863v1',\n",
       "       '1506.07190v1', '1506.07285v1', '1506.07503v1', '1506.07512v1',\n",
       "       '1506.07650v1', '1506.08909v1', '1506.08941v1', '1507.00210v1',\n",
       "       '1507.01127v1', '1507.01526v1', '1507.01839v1', '1507.02672v1',\n",
       "       '1507.03641v1', '1507.04808v1', '1507.08750v1', '1508.00305v1',\n",
       "       '1508.00657v1', '1508.01745v1', '1508.02096v1', '1508.03720v1',\n",
       "       '1508.04025v1', '1508.04112v1', '1508.05326v1', '1508.05508v1',\n",
       "       '1508.06615v1', '1508.07909v1', '1509.00685v1', '1509.00838v1',\n",
       "       '1509.01240v1', '1509.01626v1', '1509.02208v1', '1509.04219v1',\n",
       "       '1509.06461v1', '1509.06569v1', '1509.06664v1', '1509.06812v1',\n",
       "       '1509.08062v1', '1509.08967v1', '1509.09292v1', '1510.00726v1',\n",
       "       '1510.01722v1', '1510.03055v1', '1510.04935v1', '1510.09142v1',\n",
       "       '1511.00363v1', '1511.00561v1', '1511.01432v1', '1511.03677v1',\n",
       "       '1511.03729v1', '1511.04108v1', '1511.05644v1', '1511.05952v1',\n",
       "       '1511.06279v1', '1511.06295v1', '1511.06335v1', '1511.06342v1',\n",
       "       '1511.06350v1', '1511.06392v1', '1511.06422v1', '1511.06434v1',\n",
       "       '1511.06464v1', '1511.06530v1', '1511.06581v1', '1511.06709v1',\n",
       "       '1511.06732v1', '1511.06931v1', '1511.07289v1', '1511.07401v1',\n",
       "       '1511.08130v1', '1511.08198v1', '1511.08228v1', '1511.08308v1',\n",
       "       '1512.00103v1', '1512.01274v1', '1512.02167v1', '1512.02393v1',\n",
       "       '1512.02433v1', '1512.02595v1', '1512.05193v1', '1512.08849v1',\n",
       "       '1512.09300v1', '1601.00770v1', '1601.01073v1', '1601.01085v1',\n",
       "       '1601.01272v1', '1601.01280v1', '1601.01705v1', '1601.03896v1',\n",
       "       '1601.04811v1', '1601.06733v1', '1601.06759v1', '1602.00367v1',\n",
       "       '1602.01783v1', '1602.01925v1', '1602.02410v1', '1602.02644v1',\n",
       "       '1602.02830v1', '1602.02867v1', '1602.03609v1', '1602.04621v1',\n",
       "       '1602.06023v1', '1602.07332v1', '1602.07776v1', '1602.07868v1',\n",
       "       '1603.00391v1', '1603.00448v1', '1603.00748v1', '1603.00810v1',\n",
       "       '1603.01312v1', '1603.01354v1', '1603.01360v1', '1603.01417v1',\n",
       "       '1603.01547v1', '1603.02199v1', '1603.04351v1', '1603.04467v1',\n",
       "       '1603.05106v1', '1603.05643v1', '1603.06021v1', '1603.06042v1',\n",
       "       '1603.06059v1', '1603.06075v1', '1603.06147v1', '1603.06155v1',\n",
       "       '1603.06160v1', '1603.06270v1', '1603.06393v1', '1603.06598v1',\n",
       "       '1603.06744v1', '1603.07252v1', '1603.07954v1', '1603.08023v1',\n",
       "       '1603.08575v1', '1603.09025v1', '1604.00788v1', '1604.01485v1',\n",
       "       '1604.02201v1', '1604.03640v1', '1604.03968v1', '1604.06045v1',\n",
       "       '1604.06778v1', '1605.02097v1', '1605.02276v1', '1605.02688v1',\n",
       "       '1605.03209v1', '1605.03481v1', '1605.03705v1', '1605.04238v1',\n",
       "       '1605.04569v1', '1605.05273v1', '1605.06069v1', '1605.06676v1',\n",
       "       '1605.07110v1', '1605.07146v1', '1605.07272v1', '1605.07277v1',\n",
       "       '1605.07683v1', '1605.07736v1', '1605.09128v1', '1605.09186v1',\n",
       "       '1605.09304v1', '1606.00061v1', '1606.00709v1', '1606.00776v1',\n",
       "       '1606.01305v1', '1606.01540v1', '1606.01541v1', '1606.01549v1',\n",
       "       '1606.01847v1', '1606.01868v1', '1606.01933v1', '1606.02006v1',\n",
       "       '1606.02245v1', '1606.02270v1', '1606.02447v1', '1606.02492v1',\n",
       "       '1606.02689v1', '1606.02858v1', '1606.02891v1', '1606.02892v1',\n",
       "       '1606.02960v1', '1606.03126v1', '1606.03498v1', '1606.03622v1',\n",
       "       '1606.03657v1', '1606.04080v1', '1606.04155v1', '1606.04164v1',\n",
       "       '1606.04596v1', '1606.04640v1', '1606.04671v1', '1606.04695v1',\n",
       "       '1606.05250v1', '1606.05328v1', '1606.06031v1', '1606.06160v1',\n",
       "       '1606.06357v1', '1606.06565v1', '1606.07356v1', '1606.07419v1',\n",
       "       '1606.07947v1', '1607.04423v1', '1607.07086v1', '1608.04428v1',\n",
       "       '1608.04631v1', '1608.06993v1', '1608.07905v1', '1609.00150v1',\n",
       "       '1609.01704v1', '1609.03145v1', '1609.03499v1', '1609.04243v1',\n",
       "       '1609.05140v1', '1609.05473v1', '1609.05518v1', '1609.06773v1',\n",
       "       '1609.07061v1', '1609.07843v1', '1609.08667v1', '1610.01108v1',\n",
       "       '1610.02413v1', '1610.03017v1', '1610.04286v1', '1610.05256v1',\n",
       "       '1610.09038v1', '1610.09996v1', '1610.10099v1', '1611.00020v1',\n",
       "       '1611.00179v1', '1611.01587v1', '1611.01603v1', '1611.01604v1',\n",
       "       '1611.01874v1', '1611.03530v1', '1611.08669v1', '1611.09268v1',\n",
       "       '1611.09830v1', '1612.00837v1', '1612.03969v1', '1701.02810v1',\n",
       "       '1701.06547v1', '1701.08734v1', '1702.05800v1', '1705.03122v1',\n",
       "       '1708.06131v1'], dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_citated_text</th>\n",
       "      <th>right_citated_text</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>target_year</th>\n",
       "      <th>target_author</th>\n",
       "      <th>source_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1409.3215v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>ilya sutskever;oriol vinyals;quoc v le</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1412.7449v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;lukasz kaiser;terry koo;slav pet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reproducibility. All code, data, and experimen...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1506.03134v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;meire fortunato;navdeep jaitly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>st like CWS and POS tagging, automatic prosody...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>1511.00360v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>chuang ding;lei xie;jie yan;weini zhang;yang liu</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We begin by considering a document as the set ...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16664</th>\n",
       "      <td>The final  argument tells LLAMA whether low pe...</td>\n",
       "      <td>. For each instance, 36 features weremeasured....</td>\n",
       "      <td>1306.1031v1</td>\n",
       "      <td>1306.5606v1</td>\n",
       "      <td>2013</td>\n",
       "      <td>lars kotthoff</td>\n",
       "      <td>barry hurley;lars kotthoff;yuri malitsky;barry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16665</th>\n",
       "      <td>This approach works reasonably well, but does ...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "      <td>1610.03946v1</td>\n",
       "      <td>1301.3781v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>jessica ficler;yoav goldberg</td>\n",
       "      <td>tomas mikolov;kai chen;greg corrado;jeffrey dean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16666</th>\n",
       "      <td>In addition to the symmetry and replacement si...</td>\n",
       "      <td>and nia Treebank7.When evaluating on the PTB, ...</td>\n",
       "      <td>1610.03946v1</td>\n",
       "      <td>1606.02529v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>jessica ficler;yoav goldberg</td>\n",
       "      <td>jessica ficler;yoav goldberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16667</th>\n",
       "      <td>Fig. 10 shows histograms of the execution time...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1603.02199v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>sergey levine;peter pastor;alex krizhevsky;dei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>As we can see from Eq. , the target of the lea...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1312.5602v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>volodymyr mnih;koray kavukcuoglu;david silver;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16669 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       left_citated_text  \\\n",
       "0      We conducted additional experiments on artific...   \n",
       "1      We conducted additional experiments on artific...   \n",
       "2      Reproducibility. All code, data, and experimen...   \n",
       "3      st like CWS and POS tagging, automatic prosody...   \n",
       "4      We begin by considering a document as the set ...   \n",
       "...                                                  ...   \n",
       "16664  The final  argument tells LLAMA whether low pe...   \n",
       "16665  This approach works reasonably well, but does ...   \n",
       "16666  In addition to the symmetry and replacement si...   \n",
       "16667  Fig. 10 shows histograms of the execution time...   \n",
       "16668  As we can see from Eq. , the target of the lea...   \n",
       "\n",
       "                                      right_citated_text     target_id  \\\n",
       "0      andsyntactic parsing .Because RNNs make very f...  1606.03622v1   \n",
       "1      .Because RNNs make very few domain-specific as...  1606.03622v1   \n",
       "2      ; in a Pointer Network,the only way to generat...  1606.03622v1   \n",
       "3      . Recently, nsur .  have shown superior perfor...  1511.00360v1   \n",
       "4      model trained on the Google News dataset3.In a...  1705.10900v1   \n",
       "...                                                  ...           ...   \n",
       "16664  . For each instance, 36 features weremeasured....   1306.1031v1   \n",
       "16665  on the POS sequences in PTB trainingset.The re...  1610.03946v1   \n",
       "16666  and nia Treebank7.When evaluating on the PTB, ...  1610.03946v1   \n",
       "16667  , but none of these methods can be applied dir...  1708.04033v1   \n",
       "16668  , we use multiple long short-term memory  laye...  1708.04033v1   \n",
       "\n",
       "          source_id  target_year  \\\n",
       "0       1409.3215v1         2016   \n",
       "1       1412.7449v1         2016   \n",
       "2      1506.03134v1         2016   \n",
       "3       1310.4546v1         2015   \n",
       "4       1310.4546v1         2017   \n",
       "...             ...          ...   \n",
       "16664   1306.5606v1         2013   \n",
       "16665   1301.3781v1         2016   \n",
       "16666  1606.02529v1         2016   \n",
       "16667  1603.02199v1         2017   \n",
       "16668   1312.5602v1         2017   \n",
       "\n",
       "                                           target_author  \\\n",
       "0                                  robin jia;percy liang   \n",
       "1                                  robin jia;percy liang   \n",
       "2                                  robin jia;percy liang   \n",
       "3       chuang ding;lei xie;jie yan;weini zhang;yang liu   \n",
       "4      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "...                                                  ...   \n",
       "16664                                      lars kotthoff   \n",
       "16665                       jessica ficler;yoav goldberg   \n",
       "16666                       jessica ficler;yoav goldberg   \n",
       "16667  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "16668  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "\n",
       "                                           source_author  \n",
       "0                 ilya sutskever;oriol vinyals;quoc v le  \n",
       "1      oriol vinyals;lukasz kaiser;terry koo;slav pet...  \n",
       "2           oriol vinyals;meire fortunato;navdeep jaitly  \n",
       "3      tomas mikolov;ilya sutskever;kai chen 0010;gre...  \n",
       "4      tomas mikolov;ilya sutskever;kai chen 0010;gre...  \n",
       "...                                                  ...  \n",
       "16664  barry hurley;lars kotthoff;yuri malitsky;barry...  \n",
       "16665   tomas mikolov;kai chen;greg corrado;jeffrey dean  \n",
       "16666                       jessica ficler;yoav goldberg  \n",
       "16667  sergey levine;peter pastor;alex krizhevsky;dei...  \n",
       "16668  volodymyr mnih;koray kavukcuoglu;david silver;...  \n",
       "\n",
       "[16669 rows x 7 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-104-4693a8e9f77e>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['#1 String'] = df['left_citated_text'].str[-128:]\n",
      "<ipython-input-104-4693a8e9f77e>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['#2 String'] = df['right_citated_text'].str[:128]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_citated_text</th>\n",
       "      <th>right_citated_text</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>target_year</th>\n",
       "      <th>target_author</th>\n",
       "      <th>source_author</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1409.3215v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>ilya sutskever;oriol vinyals;quoc v le</td>\n",
       "      <td>le, recurrent neural networks  have made swift...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1412.7449v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;lukasz kaiser;terry koo;slav pet...</td>\n",
       "      <td>networks  have made swift inroads intomany str...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reproducibility. All code, data, and experimen...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1506.03134v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;meire fortunato;navdeep jaitly</td>\n",
       "      <td>n-based copying can be seen as acombination of...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>st like CWS and POS tagging, automatic prosody...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>1511.00360v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>chuang ding;lei xie;jie yan;weini zhang;yang liu</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>sing neural networks from raw text in a fully ...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We begin by considering a document as the set ...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>s their usefulness for real-world tasks.As a f...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16662</th>\n",
       "      <td>With human annotation of data, significant int...</td>\n",
       "      <td>proposed a yesian EM framework for continuous-...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>1512.02393v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>tingting zhu;nic dunkley;joachim behar;david a...</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "      <td>ion of each expert annotator and the underlyin...</td>\n",
       "      <td>proposed a yesian EM framework for continuous-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16663</th>\n",
       "      <td>An effective probabilistic approach to aggrega...</td>\n",
       "      <td>. as is defined as the inverse of accuracy: It...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>1512.02393v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>tingting zhu;nic dunkley;joachim behar;david a...</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "      <td>reduce annotator inter- and intra-variability....</td>\n",
       "      <td>. as is defined as the inverse of accuracy: It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16665</th>\n",
       "      <td>This approach works reasonably well, but does ...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "      <td>1610.03946v1</td>\n",
       "      <td>1301.3781v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>jessica ficler;yoav goldberg</td>\n",
       "      <td>tomas mikolov;kai chen;greg corrado;jeffrey dean</td>\n",
       "      <td>ell in the CKY chart.3In both approaches,the P...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16667</th>\n",
       "      <td>Fig. 10 shows histograms of the execution time...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1603.02199v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>sergey levine;peter pastor;alex krizhevsky;dei...</td>\n",
       "      <td>l concept is shown in Fig. 1.Recent studies ha...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>As we can see from Eq. , the target of the lea...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1312.5602v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>volodymyr mnih;koray kavukcuoglu;david silver;...</td>\n",
       "      <td>work decision.Algorithm 2 shows the learning t...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12230 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       left_citated_text  \\\n",
       "0      We conducted additional experiments on artific...   \n",
       "1      We conducted additional experiments on artific...   \n",
       "2      Reproducibility. All code, data, and experimen...   \n",
       "3      st like CWS and POS tagging, automatic prosody...   \n",
       "4      We begin by considering a document as the set ...   \n",
       "...                                                  ...   \n",
       "16662  With human annotation of data, significant int...   \n",
       "16663  An effective probabilistic approach to aggrega...   \n",
       "16665  This approach works reasonably well, but does ...   \n",
       "16667  Fig. 10 shows histograms of the execution time...   \n",
       "16668  As we can see from Eq. , the target of the lea...   \n",
       "\n",
       "                                      right_citated_text     target_id  \\\n",
       "0      andsyntactic parsing .Because RNNs make very f...  1606.03622v1   \n",
       "1      .Because RNNs make very few domain-specific as...  1606.03622v1   \n",
       "2      ; in a Pointer Network,the only way to generat...  1606.03622v1   \n",
       "3      . Recently, nsur .  have shown superior perfor...  1511.00360v1   \n",
       "4      model trained on the Google News dataset3.In a...  1705.10900v1   \n",
       "...                                                  ...           ...   \n",
       "16662  proposed a yesian EM framework for continuous-...  1503.06619v1   \n",
       "16663  . as is defined as the inverse of accuracy: It...  1503.06619v1   \n",
       "16665  on the POS sequences in PTB trainingset.The re...  1610.03946v1   \n",
       "16667  , but none of these methods can be applied dir...  1708.04033v1   \n",
       "16668  , we use multiple long short-term memory  laye...  1708.04033v1   \n",
       "\n",
       "          source_id  target_year  \\\n",
       "0       1409.3215v1         2016   \n",
       "1       1412.7449v1         2016   \n",
       "2      1506.03134v1         2016   \n",
       "3       1310.4546v1         2015   \n",
       "4       1310.4546v1         2017   \n",
       "...             ...          ...   \n",
       "16662  1512.02393v1         2015   \n",
       "16663  1512.02393v1         2015   \n",
       "16665   1301.3781v1         2016   \n",
       "16667  1603.02199v1         2017   \n",
       "16668   1312.5602v1         2017   \n",
       "\n",
       "                                           target_author  \\\n",
       "0                                  robin jia;percy liang   \n",
       "1                                  robin jia;percy liang   \n",
       "2                                  robin jia;percy liang   \n",
       "3       chuang ding;lei xie;jie yan;weini zhang;yang liu   \n",
       "4      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "...                                                  ...   \n",
       "16662  tingting zhu;nic dunkley;joachim behar;david a...   \n",
       "16663  tingting zhu;nic dunkley;joachim behar;david a...   \n",
       "16665                       jessica ficler;yoav goldberg   \n",
       "16667  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "16668  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "\n",
       "                                           source_author  \\\n",
       "0                 ilya sutskever;oriol vinyals;quoc v le   \n",
       "1      oriol vinyals;lukasz kaiser;terry koo;slav pet...   \n",
       "2           oriol vinyals;meire fortunato;navdeep jaitly   \n",
       "3      tomas mikolov;ilya sutskever;kai chen 0010;gre...   \n",
       "4      tomas mikolov;ilya sutskever;kai chen 0010;gre...   \n",
       "...                                                  ...   \n",
       "16662                  changbo zhu;huan xu;shuicheng yan   \n",
       "16663                  changbo zhu;huan xu;shuicheng yan   \n",
       "16665   tomas mikolov;kai chen;greg corrado;jeffrey dean   \n",
       "16667  sergey levine;peter pastor;alex krizhevsky;dei...   \n",
       "16668  volodymyr mnih;koray kavukcuoglu;david silver;...   \n",
       "\n",
       "                                               #1 String  \\\n",
       "0      le, recurrent neural networks  have made swift...   \n",
       "1      networks  have made swift inroads intomany str...   \n",
       "2      n-based copying can be seen as acombination of...   \n",
       "3      sing neural networks from raw text in a fully ...   \n",
       "4      s their usefulness for real-world tasks.As a f...   \n",
       "...                                                  ...   \n",
       "16662  ion of each expert annotator and the underlyin...   \n",
       "16663  reduce annotator inter- and intra-variability....   \n",
       "16665  ell in the CKY chart.3In both approaches,the P...   \n",
       "16667  l concept is shown in Fig. 1.Recent studies ha...   \n",
       "16668  work decision.Algorithm 2 shows the learning t...   \n",
       "\n",
       "                                               #2 String  \n",
       "0      andsyntactic parsing .Because RNNs make very f...  \n",
       "1      .Because RNNs make very few domain-specific as...  \n",
       "2      ; in a Pointer Network,the only way to generat...  \n",
       "3      . Recently, nsur .  have shown superior perfor...  \n",
       "4      model trained on the Google News dataset3.In a...  \n",
       "...                                                  ...  \n",
       "16662  proposed a yesian EM framework for continuous-...  \n",
       "16663  . as is defined as the inverse of accuracy: It...  \n",
       "16665  on the POS sequences in PTB trainingset.The re...  \n",
       "16667  , but none of these methods can be applied dir...  \n",
       "16668  , we use multiple long short-term memory  laye...  \n",
       "\n",
       "[12230 rows x 9 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['#1 String'] = df['left_citated_text'].str[-128:]\n",
    "df['#2 String'] = df['right_citated_text'].str[:128]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.  .  quantized the weights and activations of pre-trained deep networks using 8-bit fixed-point representation to improve infer': 0,\n",
       " 'en z, the decoder then generates an output sequence  of symbols one element at a time. At each step the model is auto-regressive': 1,\n",
       " 'g. For SNLI, we use the binary Stanford PCFG Parser parses that are included with the corpus. We did not find scheduled sampling': 2,\n",
       " '. All the initial optimizee parameters used in the experiments are generated independently from the Gaussian distribution N.All ': 3,\n",
       " 'argue that in some structured prediction problems whenone can draw samples from π∗τ, optimizing  ismore effective than , since n': 4,\n",
       " ',image captioning  . , machine translation and summarization  . .Recently,  .  also proposed a reward-based learning framework f': 5,\n",
       " ', the authors generate the representations of questions and answers separately, and score a QA pair using a similarity metric on': 6,\n",
       " '. We suspect that a much more extensive hyperparameter search over context sizes, word and sentence embedding sizesas well as in': 7,\n",
       " 'global optimum. This is a very interesting result butis based on a number of unrealistic assumptions .It has recently been shown': 8,\n",
       " 'trongpositive correlation between outlier class and both OPP scores and accuracy.Finally, we used the non-English ltiCCA vectors': 9,\n",
       " '.  introduce an end-to-end sequence model to generate captions for videos.By applying attention mechanism  to visual recognition': 10,\n",
       " 'with learning rate 0.01and batch-size 64.Gradients norms are clipped to 5.0  . .We use single-layer LSTMs with dropout of 0.3  o': 11,\n",
       " 'where the high-level CNN features over an image sequence were extracted, and a RNN is learned on top of the extracted CNN featur': 12,\n",
       " 'or passive  and action-conditioned  motion prediction. Because these work historically have not been compositional in nature, th': 13,\n",
       " ', which consists of about 1/2 million natural language dialogues extracted from the #Ubuntu Internet Relayed Chat  channel.The t': 14,\n",
       " ' of the framework and construct a new algorithm derived from it. In particular, we consider incremental methods g , Svrg  and ga': 15,\n",
       " 'g scheme.A separate CNN was trained for each of the studies described below. All models were trained in the same way, using ADAM': 16,\n",
       " '.Tweet: Tweets from 10 different authors. Classification involves classifying which Tweet belongs to which author9. This dataset': 17,\n",
       " 'an LSTM network were trained on Penn Treebank, each of them having 5 million parameters . Finally we report results published in': 18,\n",
       " '.re, the SNLI “premise” is the input (context C), and the SNLI “hypothesis” is the output (hypothesis H).We employ two different': 19,\n",
       " 'with a learning rate of 0.002 for at least 100 epochs until the validation error stopped improving.The models are four-layer net': 20,\n",
       " ', an important predecessor and inspiration for this work, showed that one fruitful path to learning longer-term dependencies was': 21,\n",
       " 'on character-level language modeling we select a more conventional architecture – a stack of two LSTM cells .Softmax: The last s': 22,\n",
       " ', there isnot a clear winner. In many tasks both architectures yieldcomparable performance and tuning hyperparameters like layer': 23,\n",
       " 'ained embeddings we use are publicly available. the English datasets, following previous works that are based on neural networks': 24,\n",
       " 'by leveraging semantic denotations during structural search.Third, to train from weak supervision and directly maximize the expe': 25,\n",
       " 'as our optimizing algorithm.For the hyperparameters of Adam optimizer, we set the learning rate α=0.001, two momentum parameters': 26,\n",
       " 'to procedurally generate the data. We compare the performance in table 2 and show samples from this model in figure 3. This data': 27,\n",
       " ', Syntactic Parsing , Text mmarization  and alogue Systems . Seq2Seq is essentially an encoder-decoder model, in which the encod': 28,\n",
       " '. The precisereason for this is unclear, as it is very difficult to isolate themost important difference between very different ': 29,\n",
       " 'learningwith structured prediction but the main learning and inference problemsfor general multi-relational settings remain open': 30,\n",
       " 's  . 2013;  . ;  . ;  . ; Ioannou . .Other approaches to reduce the convolutional overheads include using FFT based convolutions': 31,\n",
       " 'aluating dialogue systems is difficult. Metrics such as BLEU and perplexity have been widely used fordialogue quality evaluation': 32,\n",
       " 'are the results in  considering convex loss functions.In , early stopping of a  batch subgradient method is analyzed, whereas in': 33,\n",
       " '.5 These word-embeddings are generated by first training word-embeddings in a monolingual space. Then, these representations are': 34,\n",
       " 'to using fixed paths. Firstly, a possibility is that mutable paths provide a more useful form of diverse exploration in RL tasks': 35,\n",
       " '. While sequence-to-sequence models arepowerful, recent work has shown the necessity of incorporatingan external memory to perfo': 36,\n",
       " 'motion prediction. Because these work historically have not been compositional in nature, they have had limited flexibility to t': 37,\n",
       " 'ynthetic samples with estimated models to avoid degradation from modelling errors.See recent work on model-based learning, e.g.,': 38,\n",
       " 'combine the individual embeddings. The most straightforward approach is to average the embeddings. This was used for instance in': 39,\n",
       " 'gual models interms of the form of cross-lingual supervision required by them. Forexample, lBOWA  and cross-lingual Auto-encoder': 40,\n",
       " 'algorithm to minimize the sum of the loss for pasv and pa,F, with a learning rate of 10−3. arning is stopped when the minimum va': 41,\n",
       " 'proved important results for deep neural networks. Whereas  . ; effele &   approached deep learning from the perspective of gene': 42,\n",
       " 'was used.Sequence neration as an RL problem. As described in the previous section, captioning systems are traditionally trained ': 43,\n",
       " 'ning Natural guage Processing with Computer Vision for high-level scene interpretation is a recent trend, e.g., image captioning': 44,\n",
       " 'nt years, reinforcement learning  algorithms have been successfully combined with deep neural networks as function approximators': 45,\n",
       " 'and its one-way variations, where the one-way model gives better results10.Note that residual learning significantly helps on We': 46,\n",
       " 'among the annotators were calculated using Fleiss’ kappa .Note that we do not choose BLEU  as an evaluation metric, because  u .': 47,\n",
       " 'process is shown in Figure 2.Next, we discuss our language-based models using the audience chat text. Word-level LSTM-RNN models': 48,\n",
       " 'and used in  to achieve the current best performance on the slot filling task till now.Instead of using the softmax activation f': 49,\n",
       " 'roposed to incorporate an adversarial discriminator into the procedure of generative modeling. More recently, LAPGAN  and DC-GAN': 50,\n",
       " ', long short-term memoryunits chreiter & huber  or a simple logistic function, parametrized withthe parameters θr. This formulat': 51,\n",
       " '∈Rd×n. There exist two types of 1d convolution operations. e is called Time Delay ral Networks . The other one was introduced by': 52,\n",
       " '. If storingthe parameter vector takes ∼1GB, and the parameter vector is updated tensof thousands of times  then storing the lea': 53,\n",
       " 'andimproved upon using attention-based variants  . ; ong . , NMT has now become a widely-applied technique for machinetranslatio': 54,\n",
       " '.The HRED model’s encoder RNN uses a bidirectional GRU RNN encoder, where the forward and backward RNNs each have 1000 hidden un': 55,\n",
       " 'showed that a simple cascade of convolutions and ReLUs is enough to achieve good performances on standard datasets. The question': 56,\n",
       " ', we perform next word prediction with an LSTM network . We set the inner dimensionality to 250 and train the network with SGD. ': 57,\n",
       " 'ect of dropout is difficult for complex models, we can still gain an insight by looking at an approximate surrogate. Previously,': 58,\n",
       " ', where the internal covariate shift and thevanishing or exploding gradients may be especially severe, and whichwould allow us t': 59,\n",
       " 'to the dictionary.The final vocabulary size is 335,323.Because the vocabulary size is very huge, we adopt the sampled Softx loss': 60,\n",
       " 'bility to predict classes from images - is preserved. With its strongcommunity and fast training for deep CNNs, Caffe created by': 61,\n",
       " ', withβ1=β2=0.9, and initial learning rate η0=10−3.The learning rate η is annealed at a rate of 0.5 every 10 epochs zat and nnin': 62,\n",
       " 'dy to the problem of adapting vision-based models trained in a source domain to a previously unseen target domain see, e.g., , ,': 63,\n",
       " 'moval: We removed all stop-words and set the vocabulary size to top-15000 most frequent words in the corpus.We used the word2vec': 64,\n",
       " 'following experiments, we compare PLCCA/NCCA with linear CCA, two kernel CCA approximations using random Fourier features FKCCA,': 65,\n",
       " 'extends Conditional GRU to make it capable of receiving image information as input.The first GRU computes intermediate represent': 66,\n",
       " 'ery deep convolutional networks as acoustic model for  Vocabulary Continuous Speech Recognition , extending our earlier work .In': 67,\n",
       " 'norm, the more so the more the variates are antithetically correlated, leading to possible improvements in algorithmic stability': 68,\n",
       " '.e of the simplest recurrent networks is the Elman network , wherewhere σ is a non-linearity such as the logistic or tanh functi': 69,\n",
       " 'is an open-source Python library fordeveloping complex algorithms via mathematicalexpressions. It is often used forfacilitating ': 70,\n",
       " '. This indicates that a character based model indeed performs good on nese summary task. Moreover, when evaluating with F1 score': 71,\n",
       " '. Architectures employed for NLP applications differ in that they typically involve temporal ratherthan spatial convolutions.t C': 72,\n",
       " 't normalize energy output  until it does in CRF layer. Thus, our model is gobally normalized, which can solve label bias problem': 73,\n",
       " '. In bothcases one maximizes a variational lower bound on the log-likelihood that isrewritten as two terms: one that is log-like': 74,\n",
       " 'introduce a probabilistic treatment for triplets and learn an adaptive crowd kernel. milar work has been generalized to multiple': 75,\n",
       " ', tuning E in back-propagation to induce discriminative embeddings.We now report results. For sentence classification, we use th': 76,\n",
       " '. This is because our mechanism is not only about reducing the vocabulary itself for each sentence or batch, it also brings impr': 77,\n",
       " ' reward functions. The detail of the retrain process is shown in Algorithm 1. In practice, θ is updated using the Adam Optimizer': 78,\n",
       " '. t c:X→Y be a classifier.For a given instance x∈X anda given “distance” function d:X×X→R+, the optimal evasion problemis define': 79,\n",
       " 'dimensional case where we also observe a greater overall success rate .These observations are consistent with the intuition from': 80,\n",
       " 'pendix F of  and  .We conduct an empirical comparison of oac with the following active learning algorithms.iwal0: Algorithm 1 of': 81,\n",
       " 'as features.For semantic similarity of CIFAR-100 and CIFAR-10, we compute the WordNet path distance,and also used word2vec tools': 82,\n",
       " 'respect to a different loss in every turn. lti-task learning in the contextof language-processing is introduced and discussed in': 83,\n",
       " 'bilitiesthat tch Normalization potentially enables. Our future work includesapplications of our method to Recurrent ral Networks': 84,\n",
       " ' composed of 8 sentences. We do not perform any regularization other than dropout  . . All experiments are implemented in Theano': 85,\n",
       " ', with the ith element of F defined asOur feature extractor can be viewed as a hierarchical feature extractor, aiming for extrac': 86,\n",
       " 'ng temporal dependencies. While the CTS model is rather impoverished in comparison to state-of-the-art density models for images': 87,\n",
       " 'prior to forming the inner product with intermediate layers; this alleviates the need for very low learning rates, which was the': 88,\n",
       " 'for images, but also to localize different segments of the sentence to their corresponding image regions. The multimodal RNN  by': 89,\n",
       " '.We also plan to apply these models to other language pairs, such asthe recently released PASCAL 1K Japanese slations dataset.La': 90,\n",
       " 'are presented to the LSTM sentence generator as the first input, before the special start word.fferent from the previously cited': 91,\n",
       " 'nd Torch, with Caffe lagging relatively far behind. Wereproduce the relevant results in Table 4.Lastly, we review the results of': 92,\n",
       " '; a promising method, currentlylacking support for tree structures and domain-specific constraints.We release a new argument min': 93,\n",
       " 'true agreement based on speech understanding.The details of our conversational speech recognition system are described elsewhere': 94,\n",
       " ',image processing , speech recognition , etc.Compared with traditional machine learning approaches,deep learning has the followi': 95,\n",
       " '. All parameters were left at their recommended values Separation quality is assessed using the BSSEVAL toolkit . Note that, in ': 96,\n",
       " 'eling the embedding-based features from  .  alongside unigrams, bigrams and trigrams with an SVM. GoogleNews word2vec embeddings': 97,\n",
       " 'rom the final classifier.Experiments on two benchmark data sets, the Stanford timent Treebank  .  and the AG English news corpus': 98,\n",
       " '.MP3 songs corresponding to two different samplesfrom the best DMM model in the main paper learned on each of the four polyphoni': 99,\n",
       " 'pproaches in NER.ny NLP tasks, such as NER, chunking and part-of-speech  tagging can be formulated as sequence labeling tasks.In': 100,\n",
       " 'computation of the output layer is directly affected by the size of vocabulary V,which is typically set around tens of thousands': 101,\n",
       " ' spaces, RdimS and RdimT, are induced separately in each of the two languages using a standard monolingual WE model such as SGNS': 102,\n",
       " 'ion we briefly describe the LSTM unit which is the basic building block ofour model. The unit is shown in Fig. 1 reproduced from': 103,\n",
       " 'is applied to scale down the gradients. nce the information from the future frames helps making better decisions for current fra': 104,\n",
       " 'es consists of the FC, the bidirectional gated recurrent unit  , and the output layer consisting of one node of the maxout layer': 105,\n",
       " ', and we use a single dropout mask to apply dropout across all LSTM time-steps as proposed by Gal &  . Hidden layers in the feed': 106,\n",
       " 'llow us to do so. All the models, which are publicly available together with our tutorial 12, are implemented in ras  and Theano': 107,\n",
       " 'layer activations. It was considered problematic with recurrent networks until the introduction of recurrent batch normalisation': 108,\n",
       " '.According to e . : “If deep learning could lead to significantly more robust and efficient mobile sensor inference, it would re': 109,\n",
       " ' is a representational advantage of sequential processing of image parts over a single pass over the whole image see for example': 110,\n",
       " 'ry depending on the domain. These details are described in the Appendix.Our algorithm is based on asynchronous n-step Q-learning': 111,\n",
       " 'on , adding noise to the discriminator input , as well as modifying or adding regularization to the training objective functions': 112,\n",
       " 'propose the Shotgun CDN  method for l1-regularized logistic regression by directly parallelizing the updates of features based o': 113,\n",
       " 'parameters of the system  have been optimized on the development set.The proposed system, which has been implemented with Theano': 114,\n",
       " ', speech recognition, and syntactic parsing , without the use ofalmost any domain-specific tuning.In this section we describe th': 115,\n",
       " 'f the earlier layers  of Deep CNNs. wever, understanding the precise operations performed by those early layers is a complicated': 116,\n",
       " ') after the last average pooling layer of Inception-V1, with 512 hidden units. A fully connected layer is added on top for the c': 117,\n",
       " ', which performs importance-weightedsampling of labels and maintains an unbiased estimate of classification error.  every new ex': 118,\n",
       " ' is a recent method tha t solves the gradient exploding/vanishing problem and guarantees near-optimal learning regime for the la': 119,\n",
       " '} respectively. The learning rate is multiplied by the decay parameter every 1M steps. Our implementation is based on TensorFlow': 120,\n",
       " 'that are trained on Google News dataset to initialize the word embeddings. These word embeddings are fine-tuned during model tra': 121,\n",
       " ' is not invariant to linear transformations ofthe embeddings’ basis, whereas the bases in word embeddingsare generally arbitrary': 122,\n",
       " 'with linear, polynomial and radial basis function  kernels. All of SVM experiments are conducted under the clean condition.To be': 123,\n",
       " '. An architecture with 2 convolutional layers, each 3x3 with 32 filters followed by a max pooling layer, and 2 fully connected l': 124,\n",
       " 'and outlined in Table 1. The  represents two convolution layers with each having 128 feature maps and 3×3 convolution kernels. M': 125,\n",
       " ', the two most popular CNN systems, usingCaffe’s reference  model.ltiple machines: we compare Omnivore toMXNet  and SINGA , two ': 126,\n",
       " '. Words not present in the set of pre-trained words are initializedfrom the uniform distribution . We fix the word vectors and l': 127,\n",
       " 'with initiallearning rate 0.001. The two momentum parameters were set to 0.99 and0.999 respectively. We performed mini-batch tra': 128,\n",
       " 'oposed convolutional sentence model takes simple architectures such as  essentially the same convolutional architecture as SENNA': 129,\n",
       " 'by representing the weight matrix as a low rank product of two smaller matrices without changing the original number of filters ': 130,\n",
       " ';  . , where they report from R@1 up to R@20 on the entire image caption words e.g. R@1=0.16 on Flickr30K dataset by nerating ke': 131,\n",
       " 'work over horizon τ or less . Any setting with discount factor α can be learned for τ∝−1.e appealing feature of UCRL2  and REGAL': 132,\n",
       " 'ite2015 and  . shortcite2015 use attention based recurrent neural network to generate abstractive summarization.Image captioning': 133,\n",
       " '] for multiple tasks thereby enabling transfer between related tasks that could possibly have different state-action spaces. A h': 134,\n",
       " ' idea to build a first-order graph-based  ensemble parser  that seeks consensus among 20 randomly-initialized stack LSTM parsers': 135,\n",
       " 'to obtain a d dimensional vector representation of each single word. nce, we have a context representation H∈Rd×C and a query re': 136,\n",
       " 'iggest difference from the highway networks  in its current implementation.Perhaps the closet work to this research is Grid LSTM': 137,\n",
       " 'to recreate the neral Inquirer lexicon Stone .  with valence score ∈R from Warriner .  lexicon to compare our results to milton ': 138,\n",
       " 'll-known form uses the Euclidean gradients with a varying learning rate to optimize the weights. In this regard, the recent work': 139,\n",
       " 'ion, how the input should be normalized, and whichactivation function to use. Some more techniques are proposed bylimans et. al.': 140,\n",
       " ', andbinarization .In particular,binarization only requires one bit for each weight value. This can significantly reducestorage,': 141,\n",
       " 'odel proposed in this section shares similarities with previous work on word embeddings and unsupervised neural retrieval models': 142,\n",
       " 'th linear layers.Interesting compositionality properties have been observed from models based on the addition of representations': 143,\n",
       " 'to suppress the initial transient that would result if the initial estimation error was large.We trained the model with the ADAM': 144,\n",
       " 'pool the image and question with element-wise product and sum, attending to part of the image and question with an Episodic Memo': 145,\n",
       " '. GRU has a simpler structure and can be computed faster. The three gates from LSTM are combined into two gates, respectively up': 146,\n",
       " 'ht models as all previous models.An interesting observation was found when we were investigating the publicly available code for': 147,\n",
       " ', image captioning . In these tasks, while the input and output data are from different modalities  e.g. spoken and written lang': 148,\n",
       " 'beddings in a machine-learned vector space. Wepresent an ensemble method that combines embeddings produced by GloVe and word2vec': 149,\n",
       " ') Deep arning toolbox to efficiently execute the data generation and model training tasks. Spark will be extended to incorporate': 150,\n",
       " 'e want to explore more fine-grained latent variables for neural machine translation, such as the Recurrent Latent Variable Model': 151,\n",
       " 'predicts all context given the central word. For the approximation algorithm, hierarchical softmax is borrowed from  and differe': 152,\n",
       " ', yielding an approach that issimple to implement but works on low-resource bandit machine translation.Even with substantially r': 153,\n",
       " ' benchmarks .We regularize the model with Dropout .The square hinge loss is minimized with the ADAM adaptivelearning rate method': 154,\n",
       " 'have shown that SVM, a very widely used machine learning algorithm, is very susceptible to poisoning attacks.In this paper, we e': 155,\n",
       " 'ention helps VQA algorithms. In ablation studies, when attentive mechanisms are removed from models it impairs their performance': 156,\n",
       " ' strong non-identifiability inthis learning scenario.Other models like deep linear networks ,leaky rectifiers  ormaxout networks': 157,\n",
       " ' already proposed in the literature under different names, e.g Deep Sequential ral Networks DBLP:journals/corr/G14 , ral Fabrics': 158,\n",
       " 'lly, we describe image reconstruction from features and eye fixation prediction.ny probabilistic image models have been proposed': 159,\n",
       " '; word char feat concat concatenates the word-level and character-level representations along with the features described in Sec': 160,\n",
       " '; compute attention over the context document for every word in the query  or use two-way context-query attention mechanism for ': 161,\n",
       " 'but has been widely used for translation tasks ever since. e should note that it slightly differs from  .  .  where their attent': 162,\n",
       " 'and parsing , thanks to their ability to learn longer-range interactions. For acoustic modeling though, the difference between a': 163,\n",
       " '.In this paper we propose the use of multi-label convolutional recurrent neural network for polyphonic, scene-independent sound ': 164,\n",
       " 'which makes it very difficult for them to learn long sequences of input. Few methods like gradient clipping have been proposed t': 165,\n",
       " 'corpora can be effectively exploited to learn high-qualityvector-based representations of their meaning in an unsupervisedmanner': 166,\n",
       " 'ever, it also suffers from several limitations, including a difficult temporal credit assignment problem, high sample complexity': 167,\n",
       " 'communications applications such compression techniques can be used to store and transmit models efficiently.Other authors e.g.,': 168,\n",
       " '. As already noted by , the ability of REINFORCE to estimate the derivative of stochastic units can be straightforwardly combine': 169,\n",
       " 'formulate a single LP whose solution resolves the exactly separable NMF problem. An extension is also developed for noise-robust': 170,\n",
       " 'and  for instance, are in this line but, again, they limit their study at sentence-length spans. The latter is, also, due to har': 171,\n",
       " 'ment learning has been previously applied toextractive summarization g and kawa , informationextraction  . , dialogue generation': 172,\n",
       " 'd from MMD-GAN and WGAN.To quantitatively measure the quality and diversity of generated samples, we compute the inception score': 173,\n",
       " ' of non-linear classifiers such as neural nets or SVMs. In contrast to this prior work, the methods presented in our recent work': 174,\n",
       " 'ctively. We use recurrent networks with 100 gated recurrent units  for both unigram and n-gram models, respectively. We use Adam': 175,\n",
       " ' models, for example, in . In all experiments we apply a margin m=0.1, which has been shown to work well on word-retrieval tasks': 176,\n",
       " '. We also investigated the slope annealingtrick  when training networks with stochastic binary activations. From our experience,': 177,\n",
       " 'fined in  as the expected minimum time required to reach any state starting from any other state.A related result is reported in': 178,\n",
       " 'A common on-policy algorithm, known as DAgger, is shown in Alg. 1. DAgger operates for N iterations where at each iteration M tr': 179,\n",
       " '.games – at the end, there are many games that don’t have tutorials nor walkthroughs. We downloaded a big collection of games, d': 180,\n",
       " 'demonstrate that it is beneficial to pre-train a decoder LSTM for image captioning before fine-tuning it to video description. .': 181,\n",
       " ', and writing styles  .  for deception detection are suitable for satirical news detection. These works consider features at doc': 182,\n",
       " 'is one feasible solution that enables RNN to predict out-of-vocabulary words by selecting appropriate words from the source text': 183,\n",
       " 'n image as part of the question. Some recent work has started to look at the problem of question-answering for images.linowski .': 184,\n",
       " 'ator can be realized in numerous ways without changing the broader algorithm. For instance, we could use a convolutional network': 185,\n",
       " 'g mechanism in their seminal work on LSTMs. Later, this technique was adopted by other models such as the Gated Recurrent Units ': 186,\n",
       " 'nt mechanisms for adapting the model. We used an LSTM with coupled input and forget gates for a 20% reduction in computationtime': 187,\n",
       " 'llel computation facilitated by GPU hardware has led to a surge of successful neural network architectures for sequence labeling': 188,\n",
       " '.ving shown excellent ability to capture and learn complex linguistic phenomena, RNN architectures are prone to overfitting. Amo': 189,\n",
       " '. In the early experiment, we search ReLU and batch normalization directly, but it cost a low performance. As shown in Figure 3,': 190,\n",
       " 'to reduce “internal covariate shift” tch nhattan We were first motivated by looking at how BP could tolerate noisy operations th': 191,\n",
       " 'is notable among these since it is the onlymodel that has, thus far, been able to learn to correctly multiplyintegers of length ': 192,\n",
       " '. The GPU based parallel implementation showed very good speedups. As commonly known that the IFFT × FFT=kernel∗fmap, the kernel': 193,\n",
       " 'penalty and ‘M‘ for Nesterov momentum.We evaluate our models on the four polyphonic music datasets of varying complexity used in': 194,\n",
       " 'cluding machine translation , recognizing textual entailment , sentence summarization , image captioning  and speech recognition': 195,\n",
       " 'cut sizes of neural networks in half. XNOR-Net , ReFa-Net  and network binarization  use aggressively quantized weights, activat': 196,\n",
       " 'rform comparably to or better than the existing phrase-basedstatistical systems in many language pairs including En-Fr  . ,En-De': 197,\n",
       " '.Finally, the use of demonstration data has played an important role in robot learning, both as a means to obtain suitable cost ': 198,\n",
       " 'to preprocess each sentence to obtain a more accurate representation of the information.We take the pre-trained word embeddings2': 199,\n",
       " 'e commonly usedCNN models, i.e., AlexNet ,VGG-16  and Net-50 . All of these models aredownloaded from Berkeley’s Caffe model zoo': 200,\n",
       " '. It is worth mentioning that these zero-shot learning approaches  and also the aforementioned work  assume that during training': 201,\n",
       " 'Other layer parameters are all quantized to ternary values. We train our model on ImageNet from scratch using an Adam optimizer ': 202,\n",
       " '.The mixed word-character model is similar to the word model, except theout-of-vocabulary  words are converted into sequences of': 203,\n",
       " 'o our knowledge.We developed this method independently in a summer workshop and later knew the works in . In highway networks in': 204,\n",
       " ' Net, we have replaced the dropout layer after the convolutional layer with batch-normalization; this is common in recent papers': 205,\n",
       " ',the authors train neural networks with floating-point, fixed-point anddynamic fixed-point formats and demonstrate that fixed-po': 206,\n",
       " 'our environment to make the emergent language more interpretable.Others e.g., the SHRLDU program of Winograd 1971 or the game in': 207,\n",
       " 'tomatically. Recently, with the rapid development of deep neural networks, the neural machine translation  and  ;  . ;  . ;  . ;': 208,\n",
       " 'andfor transfer learning .More recent examples of unsupervised pre-training are the Skip-grammodel  and its generalization to se': 209,\n",
       " 'e, whose evaluations may not necessarily be exact. GBS is known to yield arbitrarily suboptimal results in the presence of noise': 210,\n",
       " 'ementation is following the general idea of training RNNs for LM tasks presented in  , but is rather using Gated Recurrent Unit ': 211,\n",
       " 'is a neural architecture combining an RNN and a differentiable stack.In another paper  .  authors considerextending an LSTM with': 212,\n",
       " 'mentum based improvements of classical SGD, notably Nesterov’s Accelerated Gradient Nesterov1983 ; 2013 , and the Adam optimizer': 213,\n",
       " 'omparison should be treated with caution,because our log-likelihood baseline is 1.6 BLEU points strongerthan its equivalent from': 214,\n",
       " ', a well-known Python library for machine learning and deep learning. The NVIDIA CUDA Deep ral Network4  accelerated our trainin': 215,\n",
       " 'offer an alternative to SMT systems, and have been applied successfully to a variety of tasks including machine translation. In ': 216,\n",
       " '* broke the general feedforward architecture to use ”short connections” to connect non-consecutive layers.All feed-forward archi': 217,\n",
       " ',and reinforcement learning have been proposed . Finally,abandoning greedy search in favor of approximate global search offersan': 218,\n",
       " 'certain optimization schemes have the potential to get trapped in the latters. We ran two tests to ensure that this was not the ': 219,\n",
       " 'or also known as Word2Vec is a highly popular language modeling approach that learns continuous vector representations of words ': 220,\n",
       " 'resentational power of the block. We thus experiment with the following combinations  is similar to effective Network-in-Network': 221,\n",
       " 'important ingredient for modern deep RNN architectures. We first define their update equations in the commonly-used version from': 222,\n",
       " '. These approximators consist of an RNNthat can write to and read from an external memory. This allows the network toclearly car': 223,\n",
       " '. All these models are able to induce algorithmic behavior from training data. Our work differs in that our differentiable abstr': 224,\n",
       " 'trained with 3-fold cross validation on the data from the two academic years prior to the most recent academic year. We used a n': 225,\n",
       " 'aracter sequence to fixed dimensional vector, which is called word embedding, and feed this vector to the word-level RNN LMs. In': 226,\n",
       " 'have shown that neural sequence models can be used for input representation, attention and response mechanisms.Sequence models n': 227,\n",
       " ', which can help achieve higher diversity by increasing search errors.9chine slation is the phrase-based MT system described in.': 228,\n",
       " 's hidden state. All the methods the same CNN architecture, input pre-processing, and an action repeat of 4.We use the A3C method': 229,\n",
       " ' that predicts cat.To tackle these problems, we modify the discriminator objective to also predict the class labels, inspired by': 230,\n",
       " 'ation, and then compute similarities between the two vectors to output the matching score. Examples include DSSM , CDSMM , ARC-I': 231,\n",
       " ', and necraft, all of which have allowed researchers to train deeplearning models with imitation learning, reinforcement learnin': 232,\n",
       " ' for text classification are used for initializing architectures such as convolutional and recurrent networks. The works of  and': 233,\n",
       " ' type there is a set of possible answers.Using mpleestions, an existing open-domain question answering dataset based on Freebase': 234,\n",
       " 'CoNLL-2009 datasets with the standard split into training, test and development sets.For English, we used external embeddings of': 235,\n",
       " '. The size of the beam was set to 6 in all experiments.line learning hyperparameters were estimated on the Europarl validation s': 236,\n",
       " '. Through effective definition of the reward function, the deep-RL model motivated the planner to output appropriate continuous ': 237,\n",
       " 'r ASR have recently been investigated by severalresearchers, and found to produce significant gains when training datais limited': 238,\n",
       " ' extensions such as LSTMs or GRUs showed their success in many different tasks such as language modeling  or machine translation': 239,\n",
       " ': with a certain probability, the embedding for aword is replaced with a zero vector. We do not apply word dropout to the extern': 240,\n",
       " 'entire training set.We train each model with stochastic gradient descent on the negative log-likelihood using the Adam optimizer': 241,\n",
       " 'optimizer with a learning rate of 0..In the recognition phase, we generated transcriptions with best-1  search from the decoder.': 242,\n",
       " '  sum of average and max pooling, and skipping pooling at all, replacing it with strided convolutions proposed by Springenberd .': 243,\n",
       " 'ion when =I,Fv=I. In the multi-label learning problem, M represents the label matrix and  corresponds to examples typically Fv=I': 244,\n",
       " 'us to compare the performance of end-to-end systems performant on all our tasksto a standard QA benchmark.We chose the method of': 245,\n",
       " 'model the characters and words hierarchically, by building word-level representations on top of character-level representations.': 246,\n",
       " 'hing inherent to generalcompositional models that prevents more complicated dependence on environmentstate.Indeed, previous work': 247,\n",
       " 'and Overfeat  with PELU all had better performances than with ELU. We finally show that our PELUs in the CNNs adopt different no': 248,\n",
       " ' ;  . ; integratingsuch models is an important next step. Meanwhile, a new direction in argument miningexplores pointer networks': 249,\n",
       " '.The privacy result relies on a more specific smoothness assumption.Assume that for any parameter θ∈Rd, and X∈XN the ellipsoid E': 250,\n",
       " ' benchmark RNN models because oftheir high dimensionality and the complex temporal dependenciesinvolved at different time scales': 251,\n",
       " 'trol network indirectly from signals backpropagated from the task cost. For stochastic gradient descent optimization we use Adam': 252,\n",
       " 'might help addressing this issue.An interesting case is Gradius III, a side-scrolling, flight-shooter game. While the trained ag': 253,\n",
       " ' approaches, a property often useful in cases where the objective function needs to be minimized with a relatively high accuracy': 254,\n",
       " '. This avoids the need to collect supervised labels on a large scale, which can be prohibitively expensive.wever, automatically ': 255,\n",
       " 'l language models  have been shown to be very effective in solving a number of real world problems, such as machine translation ': 256,\n",
       " 'was used after every hidden layer, and Dropout  was used before each hidden layer. We used cross-validation to optimize learning': 257,\n",
       " '.Their effectiveness has been validated on reinforcement learning tasks, such as Atari and 3D maze game playing.Modular reinforc': 258,\n",
       " '.Experimental results show that our single model achieves competitive results for all of the five different tasks, demonstrating': 259,\n",
       " '.ral generation models are standardly trained by maximizing the likelihood of target sequences given source sequences in a train': 260,\n",
       " '. It consists of m neurons each associated with a filter K∈Rt×d, where t is a window size, typically 2 – 5. The filter processes': 261,\n",
       " 'n AdaGrad is that at some point, the S will grow too large, effectively making −1/2≈0 and therefore slowing or halting training.': 262,\n",
       " 'so makes fewer mispronunciations than a standard multi-layer RNN encoder .We use a content-based tanh attention decoder see e.g.': 263,\n",
       " 'm Memory   based encoders trained using encoder-decoder framework have been proposed to learn representations of video sequences': 264,\n",
       " '.We call this training approach the joint strategy.Because the controller is separate from the recognizer, we can pretrain itin ': 265,\n",
       " 'model are builton the premise that, given a certain target word, they should serve to predict itssurrounding words in a text. I.': 266,\n",
       " '2. For this dataset, we adapt pylearn2  to apply the same global contrast normalization and ZCA whitening as was used by  et. al': 267,\n",
       " '. Given a model that outputs a sequence of discrete distributions over the token classes  augmented with a special “blank” token': 268,\n",
       " 's.These networks have the same architecture but do not share parameters.sed on the strong empirical performance of xout Networks': 269,\n",
       " 'zing the “incorrect” classification loss of the discriminator—in our setting, this loss is given by −1K∑klogDk). As suggested in': 270,\n",
       " '.The CTC cost function that we use implicitly depends on the length of the utterance,where gn is the set of all possible alignme': 271,\n",
       " 'are actually optimizing the DataGrad objective but withr=1 and λ0 and λ1 carefully chosen to eliminate the ∂L0∂wij term.Finally,': 272,\n",
       " 'and minibatches of size 40, where each training instance consists of one English sentence, one rman sentence and one image.We ap': 273,\n",
       " 'for the task of passage-based question answering, and re-purposed for the task of document summarization as proposed in  for ext': 274,\n",
       " 'n the video. In our case, we obtained vi from a pretrained model ψ which jointly models visual and semantic embedding for images': 275,\n",
       " 'em is based on the sequence-to-sequence architecture with attention and we applied sub-word segmentation with byte pair encoding': 276,\n",
       " 'units to control information flow. There are many kinds of gated RNNs, such as long short-term memory,  and gated recurrent unit': 277,\n",
       " ', and three proposed models, RecAtt, RNNAtt and CondDec. We skip HybridAtt1 because we have HybridAtt2 as an improved version.We': 278,\n",
       " '. Other approaches to combining evolution and learning have involved parameter copying, whereas there is no such copying in the ': 279,\n",
       " '.We investigate several variations of this model, including one that takes into account paragraph- rather than sentence-level in': 280,\n",
       " ' between the train and test environments is mostly in the observation , but not in the task goal . Indeed, the recent work of  .': 281,\n",
       " '; see also    for an introduction to cnns, and  and   and  .For Wikipedia comments, we use a ‘narrow’ convolution layer, with ke': 282,\n",
       " 'and ras llet .me to the previous researches that are related to zero pronoun resolution, we evaluate our system performance in t': 283,\n",
       " 'to endow the model with a richer encoding of substructures prior to the alignment step.Asymptotically our approach does the same': 284,\n",
       " '. t it doesn’t perform exceptionally better than the existing reverse dictionaries If we are to ignore the ordering of words in ': 285,\n",
       " 'in some cases. These prior studiesmotivate us to explore combining multiple versions of wordembeddings, treating each of them as': 286,\n",
       " 'ltsfor machine translation , roughly the same model and traininghave also proven to be useful for sentence compression , parsing': 287,\n",
       " ' a stochastic policy. The neural network parameters θ are updated using stochastic gradient descent. Compared to Deep Q Network ': 288,\n",
       " 'alization schemes have beenproposed to downplay the frequency/hubness effect when computingnearest neighbors in the vector space': 289,\n",
       " '. To generate an answer, the most common approach is to treat VQA as a classification problem. In this framework, the image and ': 290,\n",
       " '. At test time, beam search is used to enhance fluency inthe question-generation output.1 The decoder also keeps an explicit his': 291,\n",
       " 'tum 0.9 and mini-batch size was 256. 50 epochs of training were carried out. The network was implemented using the Caffe package': 292,\n",
       " 'which recently demonstrated impressive results on various text modeling tasks. LSTM-Networks  select a previous hidden state via': 293,\n",
       " 's for training.The first approach that optimizes only a cross-lingual objective is the bilingual compositional sentence model by': 294,\n",
       " 'imize the weights of the trained network. A preliminary support for gradient noise  is available for Adam.Gradient norm clipping': 295,\n",
       " 'across all question types.The DCN, like other models, is adept at “when” questions and struggles with the more complex “why” que': 296,\n",
       " 'ormalizing step. To improve the running time to O, onecan only update the sketch every m rounds similar to the blockpower method': 297,\n",
       " 'rning libraries and platforms have been developed and widely adopted, including cuDNN tlur ., , TensorFlow Abadi ., 2016, Theano': 298,\n",
       " 'h tasks.Perhaps owing to our efficient gradient calculation  and simpler recurrence relation, our model runs faster than that of': 299,\n",
       " 'he number of floating point multiplication operations.Recently, the same idea is extended to the activations of the neurons also': 300,\n",
       " 'hich generates words as a sequence of characters and constructs a “word embedding” by encoding a character sequence with an LSTM': 301,\n",
       " '. When uniform noise is added , then the log-likelihoods of continuous and discrete models are directly comparable . In our case': 302,\n",
       " ' spatial relation captioner module.Real-world or human-created data essentially has to be obtained again for every change/update': 303,\n",
       " 'ing er  and wman ,lti-Perspective Context tching  ngle version  . ,RaSOR  . ,Dynamic nk er   . ,and the gistic Regression seline': 304,\n",
       " 'p−k. wever, for networks that are at most p=O deep, we can bound the probability of the lth neuron being active on the kth layer': 305,\n",
       " ', we can let the model translate a given sentence by finding a translation that maximizeswhere θ=.This is, however, computationa': 306,\n",
       " 'foracoustic modeling only extend this training pipeline.The current state of the art on brispeech  uses this approach too, with ': 307,\n",
       " '.re, weuse the square root of this v, which is readily available in Adam, to constructD=ag,where Dl is the approximate diagonal ': 308,\n",
       " 'or an information retrieval based approach  . . Although joint intent  determination and slot-filling has been widely studied to': 309,\n",
       " ', we benchmark the performance of our method against LEML. Also both LEML and MoM has similar model complexity due to similar nu': 310,\n",
       " '. In thissection we define explicit reference readers more specifically by equation  below. We first present the Stanford er as ': 311,\n",
       " 'have achieved state of the art results across a wide variety of tasks, including ral chine slation    . ;  . , text summarizatio': 312,\n",
       " ' word embeddings.While originating from computer vision , CNN models haverecently been very successfully applied in NLP problems': 313,\n",
       " 'the number of tokens is approximately the same as the number of tokens in the mpWiki sentences.3We use paragram-sl999 embeddings': 314,\n",
       " 's in a decorrelated initial state so as to condition the network training to converge into better representations. For instance,': 315,\n",
       " '.IfPθn is a consistent estimator of the true conditionaldistribution PandTn defines anergodic rkov chain,thenas n→∞, the asympto': 316,\n",
       " 'ation of the previous hidden state ht−1 and the current input It.The evolution of the LSTM layer named GRU Gated Recurrent Units': 317,\n",
       " ', the use of attention-based models has proven to be very effective. As attention is learned by the iterative finding of the hig': 318,\n",
       " '. Secondly, despite focusing on ways to augment an HMM,  clustering and systems inspired by it perform very well. They aim to ma': 319,\n",
       " ' and report results for the Ano and for both alternative approaches.Implementation details As opposed to historical attempts,  .': 320,\n",
       " ', Selective Receptive Fields  , and scriminative m-Product Networks  . This also indicates that spatial information preserving u': 321,\n",
       " '.Given a sentence, a CNN with m filters and a filter size of n extracts a m-dimension feature vectorfrom every n-gram phrase of ': 322,\n",
       " 'erms of the F-measure.Therefore, we employ a single threshold of 0.2 for all labels, which we determined empirically.We use Adam': 323,\n",
       " 'n data for open domain dialogue . For example,  .  employed a phrase-based machine translation model for response generation. In': 324,\n",
       " 'eligibility traces and return-based methods in reinforcement learning.Our implementation of A3C was along the lines mentioned in': 325,\n",
       " 'what would be learnable under implicit learning conditions.We obtained semantic representations using the skip-gram architecture': 326,\n",
       " '. Table 1 givesmore examples.Our combined new effort resulted in MXNet , intending toblend advantages of different approaches. D': 327,\n",
       " 'ally private training algorithms forNaive yes classification , decision trees , logistic regression  and support vector machines': 328,\n",
       " 'd gradient descent like network weights and biases. For thisstudy we ignore training hyper-parameters and use the adam optimizer': 329,\n",
       " 'atives to gradient-based methods for learning neural networks. wever, these algorithms are not widely used in practice. Finally,': 330,\n",
       " 'rmalizing flows  and  ; limans . ; ma .  that aim at closing the gap between true and approximated posteriors, DRAW  .  and DARN': 331,\n",
       " 'also have many commonalities with Memory Networks, differing in word representation choicesand attention procedures.th  .  and O': 332,\n",
       " ' in a decoupled manner, and alleviates the overestimation issue of DQN. This generally results in a more stable learning process': 333,\n",
       " 'corresponds to biasedstochastic gradients of the glsbemb objective.For convenience, these acronyms are in 1.Example 1: ral data ': 334,\n",
       " '.In recent times hybrid approaches, based on the principle of compositional distributional semantics, have been adopted as well ': 335,\n",
       " 'only use sequence features such as words and word positions. CR-CNN can achieve 84.1% in F1 with special treatment for noisy Oth': 336,\n",
       " 'est/dev splits of with goldpart-of-speech tags, also following.When using external word embeddings, we also use the same data as': 337,\n",
       " 'addressed this problem by applying batch normalization to the input of each layer.Dropout  and Dropconnect  techniques are widel': 338,\n",
       " 'ize the vector representation of each acoustic unit with a set of pre-trained vectors , we apply the skip-gram model of word2vec': 339,\n",
       " 'ng their neighborhood . Deep learning techniques have also improved graph kernels for graph structure learning. Recently, pert .': 340,\n",
       " '.The attention models used for problems such as image captioning typically depend on the single image under consideration and th': 341,\n",
       " 'developed the Deep Adaptation Network  architecture for convolutional neural networks that embed hidden representations of all t': 342,\n",
       " 'mples of unsupervised pre-training are the Skip-grammodel  and its generalization to sentences, theSkip-thought vectors model of': 343,\n",
       " 'are a recently introduced variant of RNNs, and are designed to prevent vanishing gradients, thus being able to cope with longer ': 344,\n",
       " 'codethe entire input into the network’s hidden state vector and then, in a secondstep, decode the entire output from this vector': 345,\n",
       " 'models are also based .Specifically, we focus on the VGG11 model as it is deep  and relatively memory efficienttrains with Caffe': 346,\n",
       " '. The results on 8 evaluation tasks are promising with no finetuning on the encoder, and some of the results reach other supervi': 347,\n",
       " 'k of   leverages neural networks to generate news headline, where input documents are limited to 50 word tokens, and the work of': 348,\n",
       " 'l Networks   have recently been shown to learn powerful image representations that support state of the art image classification': 349,\n",
       " 'make use of a bi-directional attention mechanisms, whereas the GNR is more lightweight and achieves similar results without this': 350,\n",
       " ' inputs while tensor networks often assume fixed-length inputs.We notice that the very recently proposed attentive pooling model': 351,\n",
       " ', may further improve the performance.This work implements four models, BLSTM, BLSTM-Att, BLSTM-2, and BLSTM-2DCNN. Table 2 pres': 352,\n",
       " ', which maintains and samples from a posterior distribution over MDPs and is a direct application of Thompson sampling to RL.PSR': 353,\n",
       " 'over all 2D CNN features and 3D CNN features, to generate two feature vectors . The representation of each video, v, is produced': 354,\n",
       " 'and fastText , and many works that study syntax and semantics of languages,see a recent example in semantic role labeling .Some ': 355,\n",
       " 'to adapt learning rates in stochastic gradient descent for both MLE and MRT. We utilize no dropout or regularization, but we tak': 356,\n",
       " '. They allow for either replacing Vanilla, GRU, LSTM, keeping GRU, LSTM or additively aggregating LSTM features in every hidden ': 357,\n",
       " 'ext data been used here but we believe more untranscribed data definitely can further improve the results.The AdaDelta algorithm': 358,\n",
       " 'tion systemsat its core, and therefore inherits their shortcomings.Other proposed approaches for learning phrase representations': 359,\n",
       " 'ng the question representation is illustrated in Figure 3. For a given question with each word represented as the word embedding': 360,\n",
       " 'have implemented various execution strategies miliagkas2016async . As a consequence,systems like paleo  have been developed to m': 361,\n",
       " '.  the other hand the parameter search for stochastic gradient descentand variants such as Adagrad and Adam can be quite tedious': 362,\n",
       " ', quantitative analysis of mixing times does not exist, hence analysis of convergence can be difficult even for experts; student': 363,\n",
       " ',  applied Recursive ral Networks as a variant of the standard RNN structured by syntactic trees to the sentiment analysis task.': 364,\n",
       " ', a toolkit that includes a collection ofbenchmark problems such as classic Atari games using Arcade arningEnvironment  , classi': 365,\n",
       " 'Error rate  on MNIST The ambiguous data set is generated by adding a relative small perturbation to the original MNIST data sets': 366,\n",
       " 'at AAAI .Exciting achievements abound:differentiable neural computer ,asynchronous methods ,dual learning for machine translatio': 367,\n",
       " 'red recently: highway LSTMs  employ highway connections to train a stacked LSTM with multiple layers; recurrent highway networks': 368,\n",
       " 'function can be simple ones such as element-wise addition or multiplication .More complex ones such as recurrent neural networks': 369,\n",
       " 'has started to look at more flexible and reliable evaluation metricssuch as human-rating prediction  .  and next utterance class': 370,\n",
       " 'ize the sum of the cross-entropy loss on each term in Eqn. 4. We train the network via backpropagation, using the Adam Optimizer': 371,\n",
       " 'now how the resulting performancecompares to GPU implementations of ConvNet learning. Therefore, webenchmarked ZNN against Caffe': 372,\n",
       " 'for the following parameters: initial weight scaling, learning rate, and learning rate decay half life. For each dataset, 100 re': 373,\n",
       " ',question answering , andalgorithm-learning , among many otherapplications see  . 2015 for a comprehensive review.This approach ': 374,\n",
       " ', the same group explored using algorithms to dynamically select the best layout for the given question from a set of automatica': 375,\n",
       " '.Feedforward layer: Feedforward layer is another method to learn information from input words. It inputs two word embedding and ': 376,\n",
       " '. We observe that by themselves, Ph embeddings are not useful at predicting the sentiment of each sentence. Aw2v gives reasonabl': 377,\n",
       " ' with an initial learning rate of 0.001 and a 0.02 learning rate decay every 100 steps. Gradients were clipped to . Tensorflow T': 378,\n",
       " '.The following baseline methods were implemented for comparison:Utterances are always classified as the majority class, NonChat.': 379,\n",
       " 'ses deghi and  -------171-Viske deghi . -------6500--DAQUAR linowski and  1,449--------12,468COCO QA  . 123,287--------117,684du': 380,\n",
       " 'abeled, and build a ‘corpus’ where each sentence is akin to a document. We train a word embedding model using skip-gram word2vec': 381,\n",
       " 'proposes a compositional model for learning word vectors from characters. milar to word lookup tables, a word string sj is mappe': 382,\n",
       " 'tion layer and a non-linear hidden layer to jointly learn the word vector representations and a statistical language model.  and': 383,\n",
       " 'n approach where they randomly sample thousands of architectures and choose promising ones for further training. In recent work,': 384,\n",
       " ' due to their recent success as encoders for images and words, respectively. We propose two models for learning facts, denoted b': 385,\n",
       " 'L with the same HVP trick.These computations are easy to implement in auto-grad systems like TensorFlow Abadi ., 2015 and Theano': 386,\n",
       " ', dependency-based  and randomly initialized embeddings.We used publicly available word2vec vectors that were trained on Google ': 387,\n",
       " 't aspresented by  . . It won in anumber of categories in ILSVRC14. We slightly adapted theimplementation provided with the Caffe': 388,\n",
       " 'and contains 19,544 such questions, divided into a semantic subset and a syntactic subset. The 8,869 semantic questions are anal': 389,\n",
       " 'such as long short-term memory  orgated recurrent unit .We can add more hidden layers in advance or subsequent to the RNN to inc': 390,\n",
       " ',learning textual similarity  and Thyagarajan , and discourse relation sense classification Rutherford . .Our model is inspired ': 391,\n",
       " 'rameterization.The standard Exponential near Unit  is defined as identity for positive arguments and a−1) for negative arguments': 392,\n",
       " 'augment the directed model with an encoder, which is also kind of inference network.Another direction is to make the posterior p': 393,\n",
       " 'tical models, such asHidden rkov Models  and Conditional Random Fields   . ; o . .Recently, several neural network architectures': 394,\n",
       " 'm and RMSprop with fixed learning rate.The input is preprocessed by a fully-connected layer with ELU  as the activation function': 395,\n",
       " '. In , the GRU was found to achieve better performance than the LSTM on some tasks. The GRU is formulated as:From these formulae': 396,\n",
       " ' challenging tasks involving inputs and outputs with variable length, including machine translation , parsing , image captioning': 397,\n",
       " ' the hidden states of earlier part of the sequence vanishes in long sequences . ng Short-term Memory   and Gated Recurrent Unit ': 398,\n",
       " 'and possibly intractable task. In this work we investigate if it is possible to replace these early layers, by simpler cascades ': 399,\n",
       " '. In our work, we employ the Professor-Forcing algorithm  which was originally proposed to close the gap between teacher-forcing': 400,\n",
       " 'with momentum of 0.5. We search for optimal learning rates in {0.,0.,0.001,0.003}, generally finding 0. to work well with weight': 401,\n",
       " '.ndwidth parameters are selected to be close to the median distance  of feature vectors encoded from real sentences.λr and λm ar': 402,\n",
       " 'the modified environments; adding motor noise to the same set of environments ; simulated grasping and stacking using a Jaco arm': 403,\n",
       " 'rds, showing its aspect of abstractive summarization. In this experiment we follow the work on neural dialogue model proposed in': 404,\n",
       " 'ding vector can be further used for different language processing applications, such as machine translation , sentiment analysis': 405,\n",
       " ', which also introduced some language features such as POS and NE into their model.Besides, we tried to increase the memory dime': 406,\n",
       " 't source segments, but also distracts them in the decoding step in order to better grasp the overall meaning of input documents.': 407,\n",
       " '.5Our dataset is an evolved version of the CNN dataset first collectedby newcitesvore-emnlp07 for highlightgeneration. newcitesv': 408,\n",
       " ', taking 36920 videos for training, 4950 videos for validation, and 4717 videos for testing.The MSR-VTT is a newly collected lar': 409,\n",
       " ' we can even formalize the price we payfor these constraints, in terms of degraded sample complexity or regretguarantees. wever,': 410,\n",
       " 'oost, and Random Forests are adopted to evaluate our extracted features. We test each classification algorithm with scikit-learn': 411,\n",
       " ' this part, we compared our model against the following art-of-the-state baseline approaches:LSTM enc: 100D LSTM encoders + MLP.': 412,\n",
       " 'use a recurrent neural network  with LSTM units to embed shortest dependency paths without typed dependency relations, while a c': 413,\n",
       " 'tems typically represent a word by a single, context-agnostic n-dimensional word embedding  . ; Pennington . . Following studies': 414,\n",
       " 'models can also be easily adapted to other tasks such as dialogsystems , question answering systems  and image captiongeneration': 415,\n",
       " 'with a threshold of 10 and mini-batches of size 32.ring training we randomly shuffle all examples within each epoch. To speed up': 416,\n",
       " 'ial epoch through the training set.In this experiment we compare the traditional GRU with the  AM-GRU using conditional encoding': 417,\n",
       " 'to the best validation loss as well. t us note that the log-loss is computed per data point.We have run experiments with GRU-RNN': 418,\n",
       " 'epresentations are learned from word alignments, sentence alignments, or, more rarely, from aligned, comparable documents vy . .': 419,\n",
       " 'where the probability of a target word is conditionedon source sentence representations and convolutional image features .More s': 420,\n",
       " 'and , which inspired our model mostly,AC-BLSTM can achieve better performance which show that deeper model actually has better p': 421,\n",
       " 'tive way to create low-dimensional word vectors without explicitly constructing M is to use the negative sampling algorithm SGNS': 422,\n",
       " 'the loss function of SGNS. The modelis originally proposed as an ad hoc objective function using the negative sampling technique': 423,\n",
       " ';  . .wever, the previously proposed models introduce side-effects:1 mixing word-level and sense-level tokens achieves efficient': 424,\n",
       " 'were designed for better remembering andmemory accesses.Along with the sequence-based  or thetree-structured  models, RNNs have ': 425,\n",
       " 'policy gradient theorem from.In the special case of deterministic transition functions,this theorem simplifies as shown belowsee': 426,\n",
       " 'showed that NMT makes significantly fewer reordering errors, and also is able to select correct word forms more often than PBMT ': 427,\n",
       " '.To overcome this problem, we introduce dynamic data selection, in which we vary the selected data subsets during training. Unli': 428,\n",
       " '. Videos of our experiment will be available at https://sites.google.com/site/invariantfeaturetransfer/ For details of the reinf': 429,\n",
       " '.Description:e of the optimizations suggested is to sub-sample the training set words to achieve a speed-up in model training.Gi': 430,\n",
       " 'rocess, i.e., do not constrain their weights to be equal when we perform back-propagation; use linguistic resources to retro-fit': 431,\n",
       " '.We back-translate the 145k rman descriptions in the M30kC into English and include the triples  as additional training data.We ': 432,\n",
       " '. Thus we have a context character representation M∈Rf×C and a query representation N∈Rf×Q, where C is the sequence length of th': 433,\n",
       " 'and preforming better gradient updates with parallel batch approaches . Some work on adaptation to the continuous control domain': 434,\n",
       " 'score on the test split was only 0.387. top of OBJ2TEXT we additionally experimented with the global attention model proposed in': 435,\n",
       " 'map image features to a conceptual word space and are able to classify between seen and unseen concepts.The unseen concepts can ': 436,\n",
       " 'ame scale as recent datasets for solving problems such as question answering and analysis of microblog services, such as Twitter': 437,\n",
       " 'showed that the inference can be very efficient for fully connected CRF and particularly effective in the context of semantic se': 438,\n",
       " 'd systems for question answering and dialogue have been the focus of both academic and industrial research. In dialogue systems,': 439,\n",
       " 'for neural machine translation. This could be achieved by a classification layer independent of the size of the label set as in ': 440,\n",
       " ' the likelihood-ratio method tend to be significantly less effective than biased estimators, such as the straight-through method': 441,\n",
       " 'to do initial trainingof the deep CNNs. Using Adadelta has two main advantages. Firstly, in our experience the optimizationprobl': 442,\n",
       " ' task,as it represents each word with a distributed vector,and words appearing in similar contexts tend to have similar meanings': 443,\n",
       " ' normalization on the output scores fv and ft along the batch-wise direction. It can be viewed as a mixture of tch Normalization': 444,\n",
       " 'and question answering memn2n .To translate a natural language description into a program, we would like to locatethe words in t': 445,\n",
       " 'e, neural machine translation has successfully proven itself to be capable of handling subword-level representation of sentences': 446,\n",
       " 'in Table III.We further synthesized preferred input images for the Places-CNN by using the image synthesis technique proposed in': 447,\n",
       " 'is shown as a degenerated special case of nshNN. Extensions of nshNN with dual space hashing and multi-hops are also discussed. ': 448,\n",
       " '.We evaluate these techniques on the tasks of machine translation  and part-of-speech  tagging and compare them against morpholo': 449,\n",
       " 'or subwords nrich . ; tnis and DeNero  can be applied to suppress the vocabulary size,but these methods also make for longer seq': 450,\n",
       " 'ones as the opponent’s strategy in the harsh, intermediate, and benign settings, respectively.We compared Random, FeedExp3 , CBP': 451,\n",
       " 'aining  . ; g .Another work inspired by the recursive nature of the human perceptual system is Deep Attention Selective Network ': 452,\n",
       " ',hashing techniques , circulant projection ,and tensor train decomposition  were proposed and showedbetter compression capabilit': 453,\n",
       " 'N+, to two state of the art question answering architectures: the end to end memory network   and the neural reasoner framework ': 454,\n",
       " 'd using large corpora and a sense inventory such asWordNet have been shown to achieve state-of-the-art results forsupervised WSD': 455,\n",
       " 'e-choice settings. We can see that our approach improves the state of art from 60.4%  to 62.1%  on open-ended and from 64.2% FDA': 456,\n",
       " 'consists of query logs, web documents and crowd-sourced answers.Answer sentence selection is studied with the TREC QA Voorhees a': 457,\n",
       " ', one network is used to encode the input modality, and a different network to decode into the output modality, with the decoder': 458,\n",
       " ' domain-invariant representations:  .  .  first employed stacked Denoising Auto-encoders  to extract meaningful representations.': 459,\n",
       " 'An adaptive optimizer, Adam, is used to optimize the parameters of the neural networks.Default hyper-parameters of Adam are used': 460,\n",
       " '.In fact, our definition reduces to that of  .   in the case of uninformative edge labels:∑j∈NΘljiXl−1=Θl∑j∈NXl−1 if Θlji=Θl∀∈E.': 461,\n",
       " 'n has drawn more and more attention. Most of the existing approaches and models mainly focus ondesigning better attention models': 462,\n",
       " 'nt DCNN architectures with all convolutional layers1. For training on MNIST and SVHN, we set our network architecture similar to': 463,\n",
       " 'with β1=0.9, β2=0.99, and ε=10−8.Convolutional layers are presented as l×,o,h×w filters, which means that there are l convolutio': 464,\n",
       " 'elyon recurrent neural network for end-to-end encoder-decodersystem for tasks such as machine translation and textreconstruction': 465,\n",
       " 'nt for the thresholds of 1, 3, 5, and 10 for any of the five tasks.Gradient normalization has a better theoretical justification': 466,\n",
       " 'ed dropout as a regularizer on the non-recurrent connections of all bi-LSTMs.We employed the Adam optimizer for gradient descent': 467,\n",
       " ', and MISO iral  instead take update steps in the average gradient direction. For each update step, they evaluate the gradient o': 468,\n",
       " 'showed that using the RCNN object detection framework  combined with the decaf features significantly improves the performance.I': 469,\n",
       " 'presented a comparison of numerous distributed SGD variants, and although their work also employed the linear scaling rule, it d': 470,\n",
       " 'chitecture has been successfully applied tomulti-task learning in NLP such as part-of-speech tagging andnamed-entity recognition': 471,\n",
       " 'n, we consider the CIFAR-10 data set of natural images  and  . The model we are using is based on the Conv-CNN-C architecture of': 472,\n",
       " 'and DQN-PixelCNN , since these are not competing HRL methods and their advantageous features could easily be integrated into our': 473,\n",
       " 'and  showed that LSTM-based embeddings can be used for transfer learning across diverse tasks, including semantic relatedness, p': 474,\n",
       " ' set to ×10 results in 3.4 times larger mean absolute value of the biases in the first layer. This is due to batch normalization': 475,\n",
       " ' literature: SCH: an SVM that also uses character n-grams, among other stylometric features ; LSTM-2: an LSTM trained on bigrams': 476,\n",
       " 'ed convolution to improve performance. -linear interpolation anddeconvolutional methods are also very popular these days such as': 477,\n",
       " 'report a log-likelihood bound of -244±54 forRBMs and 138±2 for a 2-hidden layer DBN, using the same setup. We havealso evaluated': 478,\n",
       " '. While we do not study these models in the present work, the CBT would be ideally suited for testing this class of model on sem': 479,\n",
       " ', as it has shown to perform very effectively for a variety of datasets. To train these networks, we used the default train and ': 480,\n",
       " 'ain CRF model has 400 dimensional hidden layer.We use dropout  with a probability of 0.50 for all models. We also use batch norm': 481,\n",
       " ', who were also testing their proposed algorithm for fully connected deep nets.ring each iteration, for each real image one synt': 482,\n",
       " 'to obtain 10000 samples from this set in 4 categories. Other statistics of the datasets are mentioned in table III.We compare th': 483,\n",
       " 'rward structure motivated in Section 5. In brief, we build a 20-layer network with residual connections  and batch normalization': 484,\n",
       " ':this is a sequential approach using UCB with √β=2.Gaussian process - tch Upper Confidence und  :this is a batch BO utilizing th': 485,\n",
       " '.We used 40 mel filter bank energies as features along with theirfirst and second order derivatives. Decoding and evaluation was': 486,\n",
       " '. Therefore compressing deep models intosmaller networks has been an active area of research. As deep learningsystems obtain bet': 487,\n",
       " 'described in Section V,the adversarial examples crafted using the substitute are also misclassified bythe defended model. This a': 488,\n",
       " ', the standard and unrolled solvers are very unstable and collapse at this higher rate. wever, as shown in Figure d, & a, traini': 489,\n",
       " ' of the learned low-level controllers.We implemented our experiments using the asynchronous actor-critic framework introduced in': 490,\n",
       " 'y training the models, and the latter two have been used in previous work by wman .  and  . .HRED : We compare to the HRED model': 491,\n",
       " ', which automatically adapts the learning rate. The size of minibatches is 80; the size of word embeddings is 512; the size of h': 492,\n",
       " 'sion, thus improving performance and enhancing interpretability. This informed our choice of baseline CNN variants: standard CNN': 493,\n",
       " 'uses observation-prediction models not for planning, but for improving exploration.A key distinction from these prior works is t': 494,\n",
       " ', dictionary learning , tensor decompositions for unsupervised learning , and so on. Our current work on the analysis of non-con': 495,\n",
       " ', can be used as additional features in a supervised WSD system e.g. the SVM-based IMS . Indeed Iacobacci . shortcitesupervised_': 496,\n",
       " '. suggests that CTC training could be suffering from optimization issues and could be made more stable by providing alignment in': 497,\n",
       " '-grained parts of visual or textual inputs, and has been successfully applied to various multimodal tasks, such as image caption': 498,\n",
       " '. In comparison, GSA is more robust on a variety of datasets.We also note that adadelta runs slower than GSA because its per-dim': 499,\n",
       " 'hypothesis about maxout power in the final layers is confirmed and combined ELU  + maxout  shows the best performance among non-': 500,\n",
       " ' a multi-layer architecture, where each layer encodes the same document, but the attention is updated from layer to layer. Epier': 501,\n",
       " 'ontrast to previous memory models with a variety of different functions for memory attention retrieval and representations, DMNs': 502,\n",
       " ' in a wide range of supervised learning applications, such as computer vision , speech recognition , natural language processing': 503,\n",
       " 'nsion  are too small for training data-intensive deep learning models, while those that are sufficiently large for deep learning': 504,\n",
       " '. Word embeddings are initialized using word2vec skip-gram embeddings  trained on allcomments from the corresponding subreddit. ': 505,\n",
       " 'and from the output  . .We used a two-layer LSTM with a hidden size of H=200, and word embeddings with size 200. The number of l': 506,\n",
       " 'has also been explored in the image recognition domain. .  presents several enhancements to the standard seq2seq architecture th': 507,\n",
       " '. The best results for NER were obtained with the relaxed greedy decoder with annealed α which yielded an F1 gain of +3.1 over t': 508,\n",
       " 'using large unlabeled corpora to train the model. wever, every single dimension in the vector has no obvious meaning. We are not': 509,\n",
       " ' by the best bidirectional LSTM models described by  . , which both make useof Polyglot word vector representations published by': 510,\n",
       " 'nof the first 15 frames while the decoder predicts the last 5 framesusing the hidden context learnt by the encoder. fferent from': 511,\n",
       " 't other kinds of machine learning models such as decision trees can also be attacked by using the substitute network to fit them': 512,\n",
       " 'for our implementation.Training Parameters Model 1 and Model2 were trained by back propagation with stochastic gradient descent.': 513,\n",
       " ', a modification of D-DQN’s architecture in which the Q-function is modeled using a state  dependent estimator and an action dep': 514,\n",
       " 'with α=β=0.75, step size 0.01, and minibatch size one.We terminate training after 20000 iterations.In this section, we evaluate ': 515,\n",
       " '.There has been some recent work onmini-batched coordinate descent methods for l1-regularizedproblems , similar to the SVM dual.': 516,\n",
       " 'with a learning rate of 1e-3.After training a DAE, as detailed in the previous section6, a β-VAEDAE was trained with perceptual ': 517,\n",
       " 'an auto-encoder as a measure of interesting states to explore. visitation counts have also been investigated for exploration . .': 518,\n",
       " ' benefit of very deep convolutional networks.We present our results on eight freely available large-scale data setsintroduced by': 519,\n",
       " 'and Lasagne .And the models were trained end-to-end using the ADAM  optimizer with learning rate of 4e-3.The cost annealing tric': 520,\n",
       " ', optimization is playing an increasinglyimportant role in deep architectures. For instance, recent work on structuredprediction': 521,\n",
       " '. Despite their effectiveness in downstream applications, embeddings have limited practical value as standalone items. Consequen': 522,\n",
       " ' besides the ral-Image-QA approach, the VSE model is also compared on “single word”. Moreover, some of the methods introduced in': 523,\n",
       " 'that synthesizes waveform samples directly.We use the Griffin-m algorithm  to synthesize waveform from the predicted spectrogram': 524,\n",
       " 'rained weight in Viterbi decoding can give improvement in certain conditions.The DNN tool we use is python based software Theano': 525,\n",
       " 'uage sentences is known to be constructed recursively according to a tree structure wty, 2007, i.a.. TreeRNNs have shown promise': 526,\n",
       " 'and , we employed perplexity as an evaluation metric. Perplexity is defined by Equation . It measures how well the model predict': 527,\n",
       " ' build on some of these techniques in this work.For instance, we use some of the “DCGAN” architectural innovations proposedin  .': 528,\n",
       " ',the authors use a biLTSM architecture that employs unidirectional attention.th AP-CNN and AP-biLSTM outperform the state-of-the': 529,\n",
       " 'ients are propagated all the way back to the encoder RNN .ch anapproach was shown to be surprisingly effective for chine slation': 530,\n",
       " '.In an oracle attack, an adversary tries to disturb or even remove thewatermark embedded in a medium. The attack setting is clos': 531,\n",
       " 'with identical networks encoding Q-R pair.For each Q-R pair we extract two sets of features. First, GloVe word embeddings are fe': 532,\n",
       " 'averaging network  , recurrent neural network  and LSTM; as well as the state-of-the-art unsupervised model skip-thought vectors': 533,\n",
       " 'to 1.26 bpc  .This may be due to differences in the experimental setup: we have appliedtruncated BPTT without subsequence shuffl': 534,\n",
       " 'ystems able to keepinformation from the broader context into memory, and possibly evenperform simple forms of reasoning about it': 535,\n",
       " '.For our main MT results, we set Δ1:t to 1−SB^yKr+1:t,yr+1:t, where r is the last margin violation and SB denotes smoothed, sent': 536,\n",
       " ' BLEU on news-test2016, which in the English-rman WMT’16 evaluation is among the best systems  which do not use back-translation': 537,\n",
       " 'and Actor-mic Networks  fall into this category. These works train k task-specific expert networks  and then distill the individ': 538,\n",
       " ', paragraph vector  and bilingual distributed representations  features.2013 proposed to use bilingual word embeddings to detect': 539,\n",
       " '.To establish a stronger baseline, we explore several decode-time heuristics to improve the quality of the generated story. The ': 540,\n",
       " ', address the first challenge but fall short at the second challenge . milarly, Google’s deep learning platform, called TensorFl': 541,\n",
       " 'ariants for non-convex problems. There are also some other theoretical results for some particular non-convex problems, like PCA': 542,\n",
       " 'in a different way. newciteluong2015effective feed their original attention vectors1,into the next RNN step by concatenating the': 543,\n",
       " 'applied to various tasksin the field of natural language processing ,including language modeling ,caption generation  andparsing': 544,\n",
       " 'tories into batches of size 16, for a total of 80 images on each iteration of training. We use the Adam optimizer  in TensorFlow': 545,\n",
       " ', we use a squared exponential kernel with lengthscale parameter 0.2 for both GP-Sparring and rnelSelfSparring, and use a square': 546,\n",
       " ', we trained a skip-gram model4 on the same COCO captions as we trained VIEW  and concatenated it to the original SpRL features ': 547,\n",
       " '. As mentionedearlier, in contrast to existing neural QA models that point to the startand end boundaries of extractive answers,': 548,\n",
       " ', or both 2017 . A number of other recent studies haveapproached embeddingsfrom another angle, by trying to analyze exactly what': 549,\n",
       " 'utput width too much is harmful to networkâ\\x80\\x99s representational power. rthermore, by visualizing DenseNet’s weight norms, ang .': 550,\n",
       " 'which cover severalclassification tasks such as sentiment analysis, topic classification or newscategorization . The number of t': 551,\n",
       " ' samples, resulting in an input shape of 96×1366.The model is built with ras  and Theano . We use ADAM for learning rate control': 552,\n",
       " 'on  and image captioning .In natural language processing they have been used for machine translation  and sentence summarization': 553,\n",
       " ' ensemble model is made up of four best models, which are trained using different random seed.Implementation is done with Theano': 554,\n",
       " 'imated gradient becomesIt is shown  ;  and   that any number bsi will yield an unbiased estimation. re, we adopt the strategy of': 555,\n",
       " 'evaluate a similar network but propose a novel method for encoding lexicon matches, presenting results on CoNLL and toNotes NER.': 556,\n",
       " ', and implements the attentional RNN encoder-decoder  with several implementation differences . We borrowed the training setting': 557,\n",
       " '. ny of these models have employedLSTM chreiter & huber, 1997 based RNNs and have shown good results inlearning sequences.Attent': 558,\n",
       " 'n. Existing work along this line includes retrieval-based methods  . ;  . ;  . ;  . ;  . ;  . ;  .  and generation-based methods': 559,\n",
       " ', trained with asequence criterion . wever these models arecomputationally expensive, and thus take a long time to train. Compar': 560,\n",
       " '. wever, Oja-SON providesgood performance experimentally.In many applications, examples and hence gradients are sparse in thesen': 561,\n",
       " 'se the log likelihood aslogpx=∑ilogpθxi|x<i.Auto-regressive models have been highly effective in both image and audio generation': 562,\n",
       " 'e error improvement on CIFAR-10, and a 1.5% relative error improvement on CIFAR-100.For SVHN we follow the procedure depicted in': 563,\n",
       " '.Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has bee': 564,\n",
       " 't .  .  propose a dynamic chunk reader to extract and rank a set of answer candidates from a given document to answer questions.': 565,\n",
       " 'with a norm of 1 on all algorithms. Unless otherwise specified 100 trials were performed to search over the following hyper-para': 566,\n",
       " 'ethods have been proposed for addressing this problem. Structural Correspondence arning  and marginalized denoising autoencoders': 567,\n",
       " 'es an encoder-decoder approach.We also report the scores from Klementiev . who introduced the task and the CVM model scores from': 568,\n",
       " 'ge processing, e.g.,part-of-speech tagging, chunking, named entityrecognition  . , sentimentanalysis  . , documentclassification': 569,\n",
       " ', a method where the output of each neuron  is normalized by the mean and standard deviation of the outputs calculated over the ': 570,\n",
       " 'on 100 billion words from various news articles. The entity embeddings can also be learnt from Wikipedia hyperlinks or Freebase ': 571,\n",
       " '. To capture long-term dependencies of the input, the authors combineconvolutional layers with recurrent layers. The model is ev': 572,\n",
       " '. We model the probability of a description as a product of the conditional next-word probabilities. More formally, for each NL ': 573,\n",
       " 'ural networks  models have been proposed.End-to-end speech recognition methods based on RNNs trained with CTC objective function': 574,\n",
       " '. The authors utilized a critic network to predict the value of a token, i.e., the expected score following the sequence predict': 575,\n",
       " ', uble DQN  , Deep Advantage Actor-Critic   and a version of DA2C initialized with supervised learning 1 similar idea to  . 2016': 576,\n",
       " 'mbedded jointly into a shared vector space, the varioustasks reap benefits from each other and often performance improves forall': 577,\n",
       " ' dimensionality or aggregation, since one can feed word after word in the system and in the end arrive at a text representation ': 578,\n",
       " ', however we can show that it is suboptimal.We show to improve the bound to O(sr(A)gap2) by using a better variance analysis. In': 579,\n",
       " ' issues in NMT ong . ; nrich . .In addition, it has been shown that NMT performance drops tremendously in low-resource scenarios': 580,\n",
       " '.For STL-10, we closely follow the model presented by . Instead of using deterministic autoencoders, we replace it with standard': 581,\n",
       " 'earn to produce realistic images through competition between an image generator and a real/fake discriminator. Professor forcing': 582,\n",
       " 'es among their users.Visual estion Answering.A number of recent works have proposed visual question answering datasets andmodels': 583,\n",
       " 'hat phrase. Recurrent neural networks have been used for many tasks such as language modeling , machine translation  and parsing': 584,\n",
       " 'hem. There are 6,670 context-response pairs left in the test set.    We considered the following baselines:sic models: models in': 585,\n",
       " 'σγ control the “scale” of the Gaussian kernels.Crucially, this model is amenable to efficient approximate probabilisticinference': 586,\n",
       " 's and then form phrase-embeddings. Our approach is different from skip thoughts, where universal phrase embeddings are generated': 587,\n",
       " 'crowdsourced.For example, Belz and   find that human experts assign low rankings to their original corpus text. Again, weighting': 588,\n",
       " 'instead of words,which shrinks the vocabulary to 40k sub-word symbols for both source and target.For comparison, we also run a w': 589,\n",
       " 'ithms.Recent research in robot exploration and mapping has focused on developing adaptive sampling and active sensing algorithms': 590,\n",
       " ' the decoder. Current NMT models differ in their ways of calculating ci from the hidden states from the encoder. Please refer to': 591,\n",
       " ' a set of training inputs to outputs by maximizing a conditionallog-likelihood,very much like neural machine translation systems': 592,\n",
       " 't sentence si+1.In order to make the comparison fair, we choose to use a recurrent neural network with the gated recurrent unit ': 593,\n",
       " 'ar convolutional layer typically consists of a very large number of parameters. Methods such as Dropout , DropConnect , and xout': 594,\n",
       " '. ny recentstate-of-the-art results in sequence processing are based on LSTM,which learned to control robots ,and was used to se': 595,\n",
       " 'ent from monolingual multi-task settings, e.g., jointlytraining a chunker and a tagger for the transfer of syntactic information': 596,\n",
       " ' enabled learning these vectors from billions of words, which makes them much more semantically accurate. As a result, these mod': 597,\n",
       " 'and SAD .erful statistical models, including deep learning models , have been proposed for RC, most of which employ the followin': 598,\n",
       " ' score in the context as an answer. Note that this can only be used for the questions with a unigram answer.Attention sum reader': 599,\n",
       " ', and a more general version in . Recently, its variant for L1-regularized objectives was introduced in .The CoCoA framework for': 600,\n",
       " '. In this paper, we focus on theretrofitting approach of newcitefaruqui:2015:Retro.Retrofitting works by optimizing an objective': 601,\n",
       " 'essful application of e algebra symmetries to neural nets was in mard . . The recent crop of spatial attention nets Jadeberg . ,': 602,\n",
       " 'ected Improvement EI Močkus 1974 as acquisition function as this combination has been shown to outperform comparable approaches': 603,\n",
       " ' bdep are the weight and bias parameters including those in θchk, and Echk is the set of the chunking label embeddings.Following': 604,\n",
       " ' Imagenet Russakovsky . . Second, the features from the low to mid-levels have been shown to transfer well to a variety of tasks': 605,\n",
       " 'y results on several tasks.stributional Semantics Objective: We implement distributional semantics using the ral guage Model  of': 606,\n",
       " ', while our proposedvirtual batch normalization is a direct extension of batch normalization.e of the primary goals of this work': 607,\n",
       " 'ions and upsampling ,hypercolumns , and, autoencoder-like, hourglass orU-shaped networks that reduce and re-expand spatial grids': 608,\n",
       " 'explicitly showed that a deeper neural network can be trained to perform much better than a comparatively shallow network.Althou': 609,\n",
       " 'the word2vec vectors, our model can achieve better results than  which combines multiple word embedding methods such as word2vec': 610,\n",
       " 'and the ADGM . We also trained a supervised AAE model on all the available labels, and obtained the error rate of 0.85%. In comp': 611,\n",
       " 'e the costs for deploying the model, which is clearly different from some previous works focusing on training, e.g., naryConnect': 612,\n",
       " 'mentioned an idea of using immediate reward as a guidance of Reinforcement arning, which inspired us to add collectible items to': 613,\n",
       " ' a sequence-to-sequence scorer. wever, their training procedure is more involved, and we did not implement it in this work.MIXER': 614,\n",
       " ' the model i.e. the LSTM to extract the question embeddingwith the average embedding of the words in the question using word2vec': 615,\n",
       " 'inferred implicitly.To resolve this problem, we further propose Model III,which is an encoder-decoder-pointer framework Figure 4': 616,\n",
       " '.earchers have also explored training neural networks directlywith fixed-point weights. In mmerstrom , the authorpresents a hard': 617,\n",
       " 'o represent and process the subtle structure of inter-locking dependencies in English auxiliary verbs, as the SP system can do ,': 618,\n",
       " ', and improving specificity .Concurrent with this work, citeauthorluan-16  improve topic consistency by feeding into the model t': 619,\n",
       " 'which has recently been applied to a real-world robotics problem  can be viewed as a DDPG variant where the Q-function is quadra': 620,\n",
       " '.Given that we are using rectified linear units, it bears asking whether or not our implementation is improving substantially ov': 621,\n",
       " 'le hand-tuned potentials that were used in previous work : the edge-based costs defined by  and the count-based costs defined by': 622,\n",
       " 'suggested a reliableway to estimate the local curvature in the stochastic setting by keeping track of the variance andaverage of': 623,\n",
       " ', machine translation , caption generation , and dialog generation , among others.In some real-world scenarios, additional infor': 624,\n",
       " 'after each convolution, which was necessary for the learning to converge. Interestingly, we had no success with other feature no': 625,\n",
       " 'o encode token representations from embeddings of their characters, which similarly perform a pooling operation over characters.': 626,\n",
       " 'el to back-translate the 145k rman descriptions in the lti30kC into English and include the triples  as additional training data': 627,\n",
       " 'A specialised form of location based soft attention mechanism, well suited for 2D images was developed for the DRAW architecture': 628,\n",
       " ' they exploited an image attention mechanism to help determine the relevance between original questions and updated ones. Before': 629,\n",
       " ' variations, we perform a simple binary classificationexperiment to predict a single digit  using the MNIST dataset.We ran CoCoA': 630,\n",
       " ', we treat the problem ofgenerating maximum mutual information responseas a reinforcement learning problem in which a reward of ': 631,\n",
       " ';  . ;  &  .  .  proposes a set of techniques to stablize GANs, including using batch normlization, dropping pooling layers, red': 632,\n",
       " 'oise for each example.In this paper we investigate new training procedures such that the adversarial examples generated based on': 633,\n",
       " 'ability can be directly inferred from the visual observation I of current state and proposed action a=.Inspired by previous work': 634,\n",
       " 'in their analysis of the CNN/ly il dataset. Types are as follows, in ascending order of difficulty:Word tching: Important words ': 635,\n",
       " '. Interestingly, RNNsperform poorly when trained with classification losses, but become competitivewith the feature-engineered s': 636,\n",
       " ' promise working with character-level input, including state-of-the-art performance on a Wikipedia text classification benchmark': 637,\n",
       " 'showedsurprisingly that the same convergence rate holds for a simple variantof rror Descent with the seemingly minor modificatio': 638,\n",
       " 'gives recovery bound for the resulting estimator ^W, as described in the text .With probability at least 1−2−1,where C is absolu': 639,\n",
       " 'explanation on how they are used for venue recommendation is presented.Word2Vec is a group of models which is introduced by  . ,': 640,\n",
       " 'aps both the state and the embedding to the parameters of a Gaussian.The state decoder is similar to a conditional WaveNet model': 641,\n",
       " 'ermanet . . Recently, many research also tries to apply it on text classification problem. 2014 proposed a model similar to ’s .': 642,\n",
       " ', for visual recognition as well as audio and text processing tasks, employ elaborate connectivity schemes.The question we ask i': 643,\n",
       " ', each with three modules , as shown in Fig. 3.We argue that the multi-scale nature of the filters within the competitive module': 644,\n",
       " 'able. -of-the-art approaches to extreme multi-label learning fall broadly under two classes: 1) embedding based methods, e.g. ml': 645,\n",
       " ' to a 300-dimensional representation linearly, where the embeddings of word types are initialized using newswire embeddings from': 646,\n",
       " ', by using an approximate lesky factorization of the inverse Fisher matrix FANG,  and lakhudinov 2015, or by whitening the input': 647,\n",
       " 'and has the capacity to learn a low-entropy distribution that is similar to hard attention.-of-the-art question answering models': 648,\n",
       " 'banana is its object.In order to produce syntax-aware feature representations of words, we exploit graph-convolutional networks ': 649,\n",
       " '2015adversarial .To build adversarial images for classification, one can maximize the misdirection towards a certain wrong label': 650,\n",
       " 'superior performance to previous information retrieval e.g. nearest neighbour approaches . This idea was further developed by  .': 651,\n",
       " 'optimizer with learning rate ε=0.5 and β1=β2=0.95.The target network update interval is 10,000, mini-batch size is 32, and train': 652,\n",
       " ', and GloVe .In the vanilla-flavored model, the distributional vector of a word is given by its  PMI with regards to all other w': 653,\n",
       " ', who enable automatic gradient computation in Python code.In addition, the underlying programming and probabilistic paradigm in': 654,\n",
       " 'found that policy optimization algorithms that explored by acting according to the current stochastic policy, including REINFORC': 655,\n",
       " '.ral machine translation  is so successful that for some language pairs it approaches,on average, the quality of human translato': 656,\n",
       " 'and run a 10-fold cross validation.  each dataset, the employed classifiers are trained with individual feature first, and then ': 657,\n",
       " 'for more details.The baseline estimator model is independent from the policy models and theerror is not backpropagated back to t': 658,\n",
       " ', exhibits a gap between actual vstheoretical ratios . GPU speedup ratios are more sensitive to specialized implementation, and ': 659,\n",
       " 'ome hyperlinks appear multiple times in a webpage.All programs were implemented in Python language with the help of scikit-learn': 660,\n",
       " 'words and thus, the entire context.Given the proven effectiveness and the impact of recurrent neural networks in different tasks': 661,\n",
       " 'backend. ADAM optimizer  with learning rate set at 10−4 is used for trainingNetwork is trained to discriminate between all speak': 662,\n",
       " 'ed techniques based on stochastic gradient descent have been proposed  as well as methods based on coordinate descent/ascent see': 663,\n",
       " 'x layer to represent categorical latent variables, and use a uniform distribution for continuous latent variables as proposed in': 664,\n",
       " 'diction,neutral.For this task, we use the Stanford NLI dataset  and modelour approach off of the decomposable attention model of': 665,\n",
       " 'ization of documents. This model is not directly comparable to ours since their framework is extractive while ours and that of ,': 666,\n",
       " 'd with some success to generate text that completely expresses a set of facts: restaurant recommendation text from dialogue acts': 667,\n",
       " '.Inference proceeds by drawing posterior samples from the space of program execution traces.We define an execution trace as the ': 668,\n",
       " '} for both our model and the baseline. Non-recurrent weights were initialized from the uniform distribution with range .milar to': 669,\n",
       " '.We focus here on a task of distilling a policy. We aim to distill a target policy π∗ – a trained neural network which outputs a': 670,\n",
       " 'ental results.In this section we introduce neural mention ranking, the framework which underpins current state-of-the-art models': 671,\n",
       " 'on-making center of a dialogue agent for composing appropriate machine responses.Recent advances in neural variational inference': 672,\n",
       " 'rent network to predict a sequence of frames.The third one shows its use case when there is only one kind of input.We use Theano': 673,\n",
       " 'form any dataset-specific tuning and regularization other than dropout  and early stopping on validation sets.The Adam algorithm': 674,\n",
       " 'nce reduction methods weemploy are simple and model-independent, unlike the more sophisticatedmodel-specific control variates of': 675,\n",
       " 'i-agent settings. Deepreinforcement learning has shown competitive performance in various tasks:arcade games , objectrecognition': 676,\n",
       " '. The MNIST dataset  consists of hand written digits 0-9 which are 28x28 in size. There are 60,000 training images and 10,000 te': 677,\n",
       " 'ns of relationships within the KB  while othersuse a combination of within-KB generalization andinformation extraction from text': 678,\n",
       " 'ge-loss for each task and we compare rescaledexp to five other optimization algorithms: AdaGrad , ScaleInvariant , PiSTOL , Adam': 679,\n",
       " '. The training cost used the constantsαh=0.6,αl=0.4,αt=1.0.We regularize the models using Dropout applied to the reader output ,': 680,\n",
       " 'the approximation of value functions have long been investigated . wever, these methods were previously quite unstable . In DQN,': 681,\n",
       " 'tectures , as well as methods to increaseparallelism while decreasing the computational cost and memoryfootprint .In particular,': 682,\n",
       " '. A DMN consists of an input module, an episodic memory module, and an answering module. DMNs have been used for text based QA, ': 683,\n",
       " 'realized via soft gating mechanisms.Though different extensions and variations to GRUs and LSTMs have been investigated recently': 684,\n",
       " '.We provide pseudocode for synchronous data-parallel SGD inAlgorithm 1.A different approach to SGD consists of each node asynchr': 685,\n",
       " 'to conduct evaluation. The evaluation construct candidate triples combined by entity pairs in testing set and various relations,': 686,\n",
       " 'as optimizer with a learning rate of 0.001. For every experiment, we report the average of 10 runs.As it is easier for the stude': 687,\n",
       " 'ance. The rule shown above iscalled the SGD update, but other variants are also possible. In fact, weuse its variant called ADAM': 688,\n",
       " 'because it addresses important issues encountered in word-level NMT. Word-level NMT systems can suffer from problems with rare w': 689,\n",
       " ', brain activity , and is often considered gold standard in many of these application domains. arning a yesisan model typically ': 690,\n",
       " ' to derive character-based representations.Optimizer. Besides Stochastic Gradient Descent , we evaluate Adagrad chi . , Adadelta': 691,\n",
       " 'd other conditioning variables: y=fx,ξ,where ξ∼ρ⋅, a fixed noise distribution. Rich density models can be expressed in this form': 692,\n",
       " 'work which showed it outperforms other optionse.g., Ratinov and , 2009.Following u and chols 2016, we use the na word embeddings': 693,\n",
       " 'produces query glimpse and document glimpse in each iterations and uses both glimpses to update recurrent state in each iteratio': 694,\n",
       " 'as baseline models in our experiments.We implemented the models used in all experiments with TensorFlow  and used Adam optimizer': 695,\n",
       " ', the authors in  proposed an unsupervised learning method to learn a paragraph vector as a distributed representation of senten': 696,\n",
       " ', however, these approaches still additionally use the automatic POS tags to achieve the best accuracy. Alternatively, joint lea': 697,\n",
       " 'llowing the derivation of , for all W∈W, we haveprovidedbstituting ηt=1/,  becomesThen, Theorem 4 can be proved by replacing  in': 698,\n",
       " 'and unsupervised learning, for instance by learning feature extraction from physics .Object detection and localization with conv': 699,\n",
       " 'to hidden layers. nce different subjects and sessions are likely to have artefactualdifferences, we used a multi-task learning s': 700,\n",
       " '/multi source, respectively. Note that the first 5 models of the Table 1 are pre-trained on AlexNet  instead of the Inception-BN': 701,\n",
       " 'lead researchers to consider methods that are specifically taylored to leverage thelabel sparsity of the chosen classifier e.g.,': 702,\n",
       " 'ure the sequentiality of the text. These methods are typically applied directly to distributed embedding of words  or characters': 703,\n",
       " 'rest neighbor baseline that improves over recent methods.In the past video description has been addressed in controlled settings': 704,\n",
       " ', policy-based RL methods turn out to be more appropriate for our knowledge graph scenario. e reason is that for the path findin': 705,\n",
       " 'hidden layer thatcomputes the conditional probability of each element in theoutput sequence .AsMRI is less complex than machine ': 706,\n",
       " 'ntity in a document.It has been shown beneficial in many natural language processing  applications, including question answering': 707,\n",
       " 'emonstrated to be a powerful alternative sequence-to-sequence transducer, e.g., in machine translation  , and speech recognition': 708,\n",
       " 'layer and add additional RESNET-layers. These architectures are known to be hard to optimize using simultaneous  gradient ascent': 709,\n",
       " 'ngs, we test the algorithms on well-known synthetic functions chosen from  and practical problems. Following previous literature': 710,\n",
       " 'show that it is unable to dissipate excess noise in gradient approximations while maintaining the desired invariant distribution': 711,\n",
       " 'with an additional component: a high-dimensional stochastic latent variable at every dialogue turn.The dialogue context is encod': 712,\n",
       " 'the authors explored several variations of attention mechanism, including different match functions and local attention. We focu': 713,\n",
       " 'etworks need an exponentially large number of neurons when compared to a neural network with many hidden layers. Recently, , and': 714,\n",
       " 'own approach  formulates the problem as inference over the parameters of a symbolic physics engine, while the bottom-up approach': 715,\n",
       " '. wever, when learning sentence embeddings, newcitewieting-17-full showed that PPDB is not as effective as sentential paraphrase': 716,\n",
       " 'ncerning optimization algorithms, parallel synchronisations on clusters w/o GPUs, and stochastic binarization/ternarization, etc': 717,\n",
       " 'teaches the child model by encouraging its logits  close to those generated by the teacher model in terms of square error, and t': 718,\n",
       " 'to perform final classification.  .  proposed another model based on convolutional neural network  where word and entity mention': 719,\n",
       " 'le words.A lot of efforts have been made tackling the Image-QA problem. Some of them have collected their own datasets. linowski': 720,\n",
       " 'ents in accuracy. By appropriately initializing weights and using Composite LSTM that learned representations of video sequences': 721,\n",
       " 'use the same loss function plus a gradient reversal layer to update the feature extractor and domain discriminator simultaneousl': 722,\n",
       " 'proposed a yesian EM framework for continuous-valued labels, which explicitly modelled the precision only of each annotator to a': 723,\n",
       " '.The theoretical understanding of learning in neural networks has lagged the practical successes. It is known that any smooth fu': 724,\n",
       " 'traversal order into a sequence of structural labels has also been widely adopted in recent advances in neural syntactic parsing': 725,\n",
       " '. We do not prune the network in small steps, and instead one-shot prune the network for a given pruning ratio followed by retra': 726,\n",
       " 'ongst non neural net based methods.Very recently, the success of deep neural networks in many natural language processing tasks ': 727,\n",
       " '.Despite promising results, there are still many challenges with this approach. In particular, these models produce short, gener': 728,\n",
       " 'econfigure these networks based on the .prototxt filesavailable at Caffe Zoo  or ones available at the originalauthors’ websites': 729,\n",
       " 'perform afactorization of the output using a predefined and somewhat arbitrary orderingof output dimensions. The resulting model': 730,\n",
       " 'Recurrent ral Network  augmented with a sequence of GS samplers, which when coupled with the straight-through gradient estimator': 731,\n",
       " ' a valid tree over the input sentence. For a given hypothesis, this requirement implies several constraints on the successor set': 732,\n",
       " 'ifferent sign induced by both sx and sy.shedNets  propose a method to compress neural networks using a low-cost hashing function': 733,\n",
       " 'trained on 7.5 million ClueWeb sentences containing entities in Freebase subset of Spades.The network weights were initialized u': 734,\n",
       " 'n that a recurrent network,together with a convolutional network, is able to attend to objects in both an image and a video clip': 735,\n",
       " 'and use Adam as the optimization algorithm.For more detailed hyper-parameters, please refer to the code.In order to quantify the': 736,\n",
       " 'lift minimum risk training Och ;  and Eisner ;  and  ; ille and  ;  and   from linear models for machine translation to NMT. The': 737,\n",
       " '.We further validate our hybrid model on the task of classifying images in the CIFAR10  dataset.The dataset contains 60K images ': 738,\n",
       " 'sification, where there is no previously generated output subsequence to drive the attention, unlike sequence-to-sequence models': 739,\n",
       " 'ks have also been done in exploiting linguistic structures such asparse and dependence trees to improve sentence representations': 740,\n",
       " 'and demonstratingsuper-human performance on the ancient board game Go . earchhas yielded a variety of effective training formula': 741,\n",
       " 'which have been used as simple yeteffective methods for compositionality learningin vector-based semantics , to obtain a documen': 742,\n",
       " 'airs. ch rapid progress stems from the improvement of the recurrent neural network encoder-decoder model, originally proposed in': 743,\n",
       " 'hms with a network that explicitly learned ∂Q/∂a—unfortunately though, their methods were only shown on low-dimensional domains.': 744,\n",
       " 'esulted in superhuman play; however, on games like Montezuma’s Revenge, where rewards are extremely sparse, DQN and its variants': 745,\n",
       " 'coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70% of the time in full': 746,\n",
       " 'ethod Gage , was adapted to learn subword units for NMT nrich . . Other subword units for NMT have also been proposed: character': 747,\n",
       " ' applied on large labeled datasets does not rely on any unsupervised pretraining, semisupervised learning might instead be cruci': 748,\n",
       " '.These results cannot be directly compared to each other because they construct vector representationsfrom different corpora; bu': 749,\n",
       " ' models, which provide state of the art performance in audio and text processing tasks.The dilated convolutional network we cons': 750,\n",
       " 'where the conditioning vector ct is provided by a convolutional attentive encoder,similar to the one described in Section 3.2 of': 751,\n",
       " ', where each instance in the mini-batch is augmented with information about its differences with other instances in the same min': 752,\n",
       " ', the VSE model , and the ral-Image-QA approach .The performances of our proposed CNN model on the DAQUAR-All, DAQUAR-Reduced, a': 753,\n",
       " '.We start by defining some useful notation. Specifically, for any t=1,2,...,n, any k=1,2,...,∣∣At∣∣, we definee key observation ': 754,\n",
       " '.Network Pruning achieves a 9x reduction in model size while maintaining the baseline of 57.2% top-1 and 80.3% top-5 accuracy on': 755,\n",
       " ': it is effective in high dimensional spaces, it is still effective even in cases where the number of dimensions exceeds the num': 756,\n",
       " ' rules  and quasi-synchronous grammar approaches .With the emergence of deep learning as a viable alternative for many NLP tasks': 757,\n",
       " 'and augments it with a number of unsupervised auxiliary tasks that make use of the rich perceptual data available to the agent b': 758,\n",
       " 'mal sub-optimality reached 1e−4, or after 500 iterations.We then fit a linear model to log using using LassoCV from scikit-learn': 759,\n",
       " '. GRU is better in the cross-subject test and LSTM is better in the cross-view test.  the other hand, GRU is faster than LSTM bo': 760,\n",
       " 'show how a monolingual NMT encoder representssentences with similar meaning close in embedding space.They show graphically —with': 761,\n",
       " 'gradient update method for all these models. Figure 2 plots their training set entropy with respect to iteration numbers. To bet': 762,\n",
       " '. Given different sensor types, different data representations and CNN-LSTM structures are required. Our dataset from trauma res': 763,\n",
       " 'uggled as the number of parameters increased. Adaptation to the deep neural network framework has also been done in recent years': 764,\n",
       " ', where Twitter conversation pairs  are used as data. ke our system, for a given query, those authors retrieve the best matching': 765,\n",
       " 'CNN is used to extract fixed-length representation with the help of the popular implementation in Caffe . Features are extracted': 766,\n",
       " '.Another particularly nice property of neural models is that allinternal computations use distributed representations of input d': 767,\n",
       " 'ep learning, and their corresponding distributedversions may have additional considerations e.g. sharing the squared gradientsin': 768,\n",
       " 'e 100 1D filters for CNN char embedding, each with a width of 5. The hidden state size  of the model is 100. We use the AdaDelta': 769,\n",
       " 'and in this section demonstrate that evasion of kernel-based classifiers at test time can be realized with a straightforward gra': 770,\n",
       " 'achieve state-of-the-art results onmany gaming tasks through a novel lightweight, parallel method calledAsynchronous Advantage A': 771,\n",
       " ' paper, we rely on three different supervised classifiers, a labeled LDA classfier , an SVM , and a convolutional neural network': 772,\n",
       " ', which are parametrically expensive, or to employ vocabulary selection strategies . In both cases, we face the issue that words': 773,\n",
       " 'etworks GSN  . 2014, iii the variational auto-encoders introduced above, iv the non-linear independent component estimation NICE': 774,\n",
       " 'ure, a strategy of using a pseudo-parallel corpus to increase the amountof training examples for the decoders of other languages': 775,\n",
       " 'jection sampling, or other complicatedapproaches .VAE’s methodology has been successfully extended to convolutional autoencoders': 776,\n",
       " ', convolutional neural networks  . ;  . , and recursive neural networks  . .re we adopt recurrent neural network with long short': 777,\n",
       " 'ceived an increasing attention , leaving aside classical word representation methods such as LSA. In particular, Word2vec models': 778,\n",
       " 's as paragraphs.A recent branch of research proposes to incorporate visual information to improve the performance of monolingual': 779,\n",
       " 'is an extension of LSTM to multiple dimensions.Another recent model of this type is ral GPU  &  , which can learn to multiply lo': 780,\n",
       " 'and QNN . milar to our basic network quantization method, they also suffer from non-negligible accuracy loss on deep CNNs, espec': 781,\n",
       " 'did explore low-resource NMT, but by assumingthe existence of large amounts of data from related languages.The current de-facto ': 782,\n",
       " 's.All our architectures fall within the neural network embedding based approach.We implemented three different architectures CNN': 783,\n",
       " '. These approaches represent the atomic operations of the network in a differentiable form, which allows for efficient end-to-en': 784,\n",
       " 'ed in newcitepersona16.Despite its demonstrated potential, a major barrier for this line of research is data collection.ny works': 785,\n",
       " 'pical post-processing methods to deal with UNK tokens. A simplest way is to ignore them, and we denote it as Ignore. Another way': 786,\n",
       " ' the original sequence and its label astraining input. ch convolutional baselines include the dynamic CNN with k-maxpooling DCNN': 787,\n",
       " '}, batch size ∈ {32, 64}. The numbers in brackets indicate the distinct values of each hyperparameter we considered.We used Adam': 788,\n",
       " 'is the state-of-the-art model on several datasets. It employs a gating mechanism to represent document which is query-specific i': 789,\n",
       " 'interval of τ=1,i.e. communication occurred every iteration.For gossiping, we used both τ=1 and τ=10 the latter is recommendedin': 790,\n",
       " 'teDBLP:conf/conll/BjorkelundHN09 for nese.The training objective was the categorical cross-entropy, and we optimized itwith Adam': 791,\n",
       " 'at some specific  temperatures in order tosucceed. More work is certainly warranted in that direction.An interesting observation': 792,\n",
       " 'wordpieces when segmented according to the chosen wordpiece model. Ourgreedy algorithm to this optimization problem is similarto': 793,\n",
       " ') solve this problem by proposing a new corpus based on news stories from CNN and ly il that consist of around 280,000 documents': 794,\n",
       " 'have either used strong supervision , or have employed synthetic datasets.We apply NSM to learn a semantic parser with weak supe': 795,\n",
       " 'ere identified correctly.We evaluate two NBT model variants: NBT-DNN and NBT-CNN. To train the models, we use the Adam optimizer': 796,\n",
       " 'or by cross-entropy.Concretely, assume we are learning a multi-class classifier overa data set of examples of the form with poss': 797,\n",
       " 'ch to skill learning through the options framework . Options are Temporally Extended Actions  and are also referred to as skills': 798,\n",
       " 'ine learning approaches in many tasks like part of speechtagging, question answering, sentiment analysis, documentclassification': 799,\n",
       " 'rom using either exploration bonusare observed in games categorized as hard exploration games in the‘taxonomy of exploration’ in': 800,\n",
       " ', ,we need to show that for M, the diagonal entries are nonnegative,  the off-diagonal entries are nonpositive its row sums are ': 801,\n",
       " '.RNNs have been used to model music ,but to our knowledge they always use a symbolicrepresentation. In contrast, our work demons': 802,\n",
       " 'n networks without losing much performance,we need to allocate bit-width carefully and wisely.First we note it has been observed': 803,\n",
       " 'and Theano, two popular GPU implementations.Comparison can be tricky because CPU and GPU implementations bydefinition cannot be ': 804,\n",
       " '.In this paper, we focus on learning how to skim text for fast reading. In particular, we propose a “jumping” model that after r': 805,\n",
       " 's the second highest result on both WN11 and FB13. Note that there are higher results reported for NTN , linear-comp and sE-comp': 806,\n",
       " '-wise product.While a thorough treatment of the LSTM is beyond the scope of this paper, we refer to our review of the literature': 807,\n",
       " 'sing the tokenizer from the Pattern package , and all stop words were removed using the English stop word list from scikit-learn': 808,\n",
       " 'uage modeling task , while Real .  also tested the method on the larger CIFAR-100 dataset. A different approach was presented by': 809,\n",
       " 'with reported decoding speeds of 865 words per second.7 wever, batch decoding with rian-NMT is much faster reaching over 4,500 w': 810,\n",
       " 't achieves the variance reduction effect for SGD, and it does not need to store the n component gradients.As pointed out by    .': 811,\n",
       " '. To do this, they used a modified Dynamic Memory Network  . A DMN consists of an input module, an episodic memory module, and a': 812,\n",
       " 'odels, which support a wider range of decoding procedures. Our approach differs from other recent chart-based neural models e.g.': 813,\n",
       " 'ord embeddings . rthermore, there are studies which have combined language and vision for image caption generation and retrieval': 814,\n",
       " ':We use the class embeddings k in addition to the context vector ^zt in Eqn. 1:The remaining equations for the LSTM recurrence r': 815,\n",
       " ' of the major uses of word embedding models is to learnunsupervised embeddings over large unlabeled datasets such as in Word2Vec': 816,\n",
       " 'with mini-batch to minimize our objective functions and set the initial learning rate to be 0.12.To illustrate the quality of ou': 817,\n",
       " 'networks  have made swift inroads intomany structured prediction tasks in NLP,including machine translation andsyntactic parsing': 818,\n",
       " 'o-End Memory Networks introduced multi-hop training  and do not require strong supervision unlike MemNN. y-Value Memory Networks': 819,\n",
       " '. In their model, eachagent is controlled by a deep network which has access to a communicationchannel through which they receiv': 820,\n",
       " 'as is input, the output of the ith block is recursively defined aswhere fi is some sequence of convolutions, batch normalization': 821,\n",
       " ', and semantic parsing . Also, in defining the layer-by-layer transformations, we can go beyond the read-write operations propos': 822,\n",
       " '.1 The hidden states are then converted to one fixed-length context vector per output index, cj=φj, where φj summarizes all inpu': 823,\n",
       " 're interested in directly optimizing end-to-end neural translation models.The xed Incremental Cross-Entropy Reinforce  algorithm': 824,\n",
       " ' results are slightly better. This is probably due to the different use of dropout and subwords. We also compared our results to': 825,\n",
       " 'introduced the Stack-augmented Parser-Interpreter ral Network  to combine parsing and interpretation within a single tree-sequen': 826,\n",
       " 'is more general and uses a generic form. wever, their task is on text e.g. machine translation while our focus is distant speech': 827,\n",
       " '.In this paper we propose a new framework for extracting relations among problem, treatment and test in clinical discharge summa': 828,\n",
       " ' embedding of the true output and the predicted embedding. This approach is inspired by the distributed representations of words': 829,\n",
       " 'ans that we directly use the sum of previous alignment probabilities without normalization as coverage for each word, as done in': 830,\n",
       " '.This is critical for NLP tasks, especially where additional supervision and external knowledge can be utilized for bootstrappin': 831,\n",
       " 'formance. We perform dropout after each layer, except input and output ones, and the rate usually is set to 0.1 or 0.2. The Adam': 832,\n",
       " 'essful in many application domains, such as image classification Stollenga . , image caption generation  . , machine translation': 833,\n",
       " 'for illustration of the formulation.The use of inverse models has been investigated to learn features for recognition tasks .  .': 834,\n",
       " '. As an importantobservation, we do not quantize layer outputs during fine-tuning. We use floating pointlayer outputs instead, w': 835,\n",
       " ' and possibly  theapproximate nature of our k-best inference procedure.We suspect that a stronger baselinesystem such as that of': 836,\n",
       " '.To find the best CNN to predict social attribute ratings among all six networks, we first find the best-performing feature laye': 837,\n",
       " ', RNN  and others . Those methods cannot be directly applied for recommendation as only a global classification/ranking model is': 838,\n",
       " '. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.We ': 839,\n",
       " 'ed by Wikipedia concepts.Recently, motivated by the simplicity and success of neural network based word embedding ,multi-lingual': 840,\n",
       " 'LSTM/GRU cell and Θv is the parameters of the fv. e to the fact that vanilla RNNs have gradient vanishing and exploding problems': 841,\n",
       " 'in convolutional layers, to  in recurrent layer, and to  in the document modeling. Our point of departure is that unlike these w': 842,\n",
       " 'le the SSVAE-I,II are two implementations of SSVAE using two different conditional LSTMs.The system was implemented using Theano': 843,\n",
       " 'with slight variance. In Algorithm 1, we give out the pseudocodes of merging loss with Variant-3 and considering to relieve the ': 844,\n",
       " ' settings in our proposed model. For our scorer module, we used bilinear and MLP scorers  with 128 hidden units. We used an Adam': 845,\n",
       " ', narized ral Networks  , Net , xout Network , Network in Network  . For each dataset under consideration the best performing LB': 846,\n",
       " '. The second problem is solved via Spearman’s rank correlation coefficient . Higher ρ means better performance. For English word': 847,\n",
       " ', can also be used as an alternative.In the inner  loop of each epoch, we combine the idea of mini-batches  with extra-gradient ': 848,\n",
       " 'and constituency parsing  . ;  . ; Cross and ang .e of the main reasons for the prevalence of bilexical dependencies andtree-bas': 849,\n",
       " 'follow this line and change the encoder to RNN to make it a full RNN based sequence-to-sequence model .All the above works fall ': 850,\n",
       " 'a number of results which differ only slightly from“standard” ones, beginning with a statement of an online MD bound adaptedfrom': 851,\n",
       " 'to obtain multiple topic-sensitive representations per word type using topic distributions.In addition, we do not cluster contex': 852,\n",
       " ' it was empirically showed that hashing in neural nets may achieve drastic reduction in model sizes with no significant loss of ': 853,\n",
       " '. Another problem ofcounting is that the state space may be large or continuous and we want togeneralize between similar states.': 854,\n",
       " ' show that randomized  coordinate descent methods can be accelerated by parallelization for solving Lasso problems. In addition,': 855,\n",
       " '. Another explanation comes from the fact that the contextual information encoded in the word vectors can compensate for the lac': 856,\n",
       " 'to  as they use pairwise comparisons to infer a single ranked list that is neither user-specific nor query-specific. Among them,': 857,\n",
       " 'as an intermediate step in creating sentencerepresentations to solve a wide variety of tasks including classification andranking': 858,\n",
       " 'present explicitly in MRS but not in AMR. Parsers based on RNNs have achieved state-of-the-artperformance for dependency parsing': 859,\n",
       " 'y pruning the search space  an iterative ML training process, where beam search is used to find pseudo-gold programs.Wiseman and': 860,\n",
       " 'and cardinality .While residual networks are powerful models, they still overfit on small datasets. A large number of techniques': 861,\n",
       " 'duce the vector quantization methods.We first consider matrix factorization methods, which have been widely used to speed up CNN': 862,\n",
       " '.It recursively transforming current word vector eqt with the output vector of the previous step ht−1.where zi and ri are update': 863,\n",
       " '.A beam search algorithm was used to generate optimal paraphrases by exploiting the trained models in the testing phase . We use': 864,\n",
       " '.For each pretraining data sample p,c1,⋯,cn,we substitute one symbol  with a random symbol.The distance of the negative sample i': 865,\n",
       " ' the distributionfunction which generates by the i-th row of W. The optimization functioncan get F by using word embedding model': 866,\n",
       " 'is Google’s recently open-sourced framework for the implementation and deployment of large-scale machine learning models. It is ': 867,\n",
       " 'd sense inventory and opt for belfy  to perform WSD on Wikipedia4. Then they train word sense embeddings using CBOW architecture': 868,\n",
       " 'ord in its hidden state, which will otherwise be prone to serious distortion due to the scarcity of training instances for it. .': 869,\n",
       " 'proposed GP-UCB, which entails a specific schedule for κn that yields provable cumulative regret bounds.The second class of acqu': 870,\n",
       " '. The regression network during test time uses the batch normalization parameters estimated on the training data, however, in th': 871,\n",
       " 'is an example of the second category of methods that guarantees monotonic policy improvement and is designed to be scalable to l': 872,\n",
       " 'parsing as a translation task, mapping English expressions to some logical form, under supervision of some deep learning method.': 873,\n",
       " 'is sequence-level training of neural sequence models, specifically, using reinforcement learningto optimize task-specific sequen': 874,\n",
       " 'ength sequences in an end-to-end fashion, it fits many natural language processing tasks and can rapidly achieve great successes': 875,\n",
       " ', a gated-attention reader for text comprehension; DAF  . , a reader with bidirectional attention flow for machine comprehension': 876,\n",
       " 'and the translation. For the first layer of finding relevant sentences, we learn word and phrase embeddings by training word2vec': 877,\n",
       " '. Unlike the CAS er  . , we do not assume any heuristics to our model, such as using merge functions: sum, avg etc. We used a me': 878,\n",
       " '.In particular, different types of attention models have been proposed to address this problem, including hierarchical , stacked': 879,\n",
       " ', and randomly initialized parameters in the interval .As regularizer, we use a dropout of 0.1 on the input representations.rthe': 880,\n",
       " 'as adjusting the parameters θgt of the attention network so that the log-probability of attention location it that has led to a ': 881,\n",
       " 'ested in additional setups, detailed in Sect. 5.Our training corpus is the cleaned and tokenised English Polyglot Wikipedia data': 882,\n",
       " 'for comparison. After training we evaluate each network’s test error for different distortions to the weights . tch norm paramet': 883,\n",
       " 'in order to keep our resultscomparable. These include adding horizontally flipped examples ofall images as well as randomly tran': 884,\n",
       " ', the latter of which we use for our models. To build a probabilistic model for sequence generation with an RNN,one adds a stoch': 885,\n",
       " 'GNC-CNN consistently outperforms baseline models. ral models have recently gained popularity for Natural guage Processing  tasks': 886,\n",
       " ',converting natural language to regular expressions ,and many others.Outlook.Today, the semantic parsing community is a vibrant ': 887,\n",
       " 'or sentiment analysis . Meanwhile, researchers have produced improvements in parsing algorithms and models that have increased t': 888,\n",
       " 'idual recurrent networks for each category hoping that we could get better results. wever, we couldn’t find notable improvement.': 889,\n",
       " '.As an extension, the attention mechanism enables the decoder to revisit the inputsequence’s hidden states and dynamically colle': 890,\n",
       " 'xperiment with span searchingThis resulted in 1.5% improvement in F1 on the development data and that outperformed the DCR model': 891,\n",
       " 'algorithm is an effective on-policy optimization method for large nonlinear policies and tends to give monotonic improvement dur': 892,\n",
       " '. All of these works share the same idea of adding gated linear connections between different layers. The highway networks propo': 893,\n",
       " 'izing the individual filters in the network. In this work, we apply the visualization techniques proposed by Zeiler and   and  .': 894,\n",
       " 'that aim to extract meaningful representations, but often come with an intractable inference step that can hinder their performa': 895,\n",
       " 'terogeneous CQA network along with social network. The input words of our methods are initialized by pre-trained word embeddings': 896,\n",
       " '5. semantic frames that associate with the head word and its p-o-s tag based on FrameNet 6. pre-trained vector for the head word': 897,\n",
       " '.For all the experiments below, we utilize the pre-trained word embeddings word2vec with 300 dimensions from  . shortcite:13 to ': 898,\n",
       " '. Consequently, optimization trajectories can vary significantly based on the chosen parameterizations. This issue can be resolv': 899,\n",
       " 'with a learning rate of 0.001 .5 The batch size is set to 80. Between layers we apply dropout with a probability of 0.2, and in ': 900,\n",
       " ', an adversarial autoencoder is used to learn the cross-domain relations of the vector representations of words from two differe': 901,\n",
       " 'the platform, we provide throughput benchmarks of ELF, and extensive baseline results using state-of-the-art RL methods e.g, A3C': 902,\n",
       " ' . Then we derive a regret bound based on mma 4 derived above.Our construction of confidence set is motivated by the analysis in': 903,\n",
       " '. th OAA and LT areprovided by the Vowpal Wabbit  machine learning tool.The remaining techniques are variants of Algorithm 3 usi': 904,\n",
       " 'r features. Rahimi &   showed that it allows us to effectively learn nonlinear models and still obtain good learning guarantees.': 905,\n",
       " 'mNN models.Our use of an ensembleis an alternative way of replicating the application of dropout  in the previous bestapproaches': 906,\n",
       " 'for a substantial portion of the target domain from the source domains.We introduce pre-trained word embeddings such as word2vec': 907,\n",
       " 'use a full RNN sequence-to-sequence encoder-decoder model and add some features to enhance the encoder, such as POS tag, NER, an': 908,\n",
       " '.wever, despite the aforementioned virtuesof character-level models,no prior work, to our knowledge,has successfully trained the': 909,\n",
       " 'ta as the dev set. Training is done through stochastic gradient descent over shuffled mini-batches with the Adadelta update rule': 910,\n",
       " 'iero. As in the literature, we see large variation in performance over individual NMT systems even with the same vocabulary size': 911,\n",
       " ', regularization zaremba2014_regularization ; krueger2016_zoneout , variable computation jernite2016_variable ; neil2016_phasedl': 912,\n",
       " '.Equivalence between Recurrent Networks and ring chinesDynamical systems  are ring universal . Thus dynamical systems such as th': 913,\n",
       " 'nd W+~W for GloVe following vy . 2015, and W and W+~W for xVec.For evaluation, we use standard word similarity and analogy tasks': 914,\n",
       " 'introduced a Toeplitz-like structured transform, within the framework of displacement operators. nce Toeplitz matrices can be “e': 915,\n",
       " ', are obtained using sequence-level discriminativetraining techniques. For our experiments we minimize the state-basedminimum ye': 916,\n",
       " 'in the data. A similar heuristic has been the de rigueur method for exhibiting disentanglement in the disentanglement literature': 917,\n",
       " '.re, instead of new output weights, we train new input weights.ny common document classification models, like tf-idf logistic re': 918,\n",
       " 're 1 illustrates a ladder structure with two hidden layers.The concept of batch normalization was recently proposed by Ioffe and': 919,\n",
       " 'hedresults without additional labeled data by more then 1% absolute F1.The improvement over the previous best result of 95.77 in': 920,\n",
       " '.Assume that A1 and A2 hold for our stochastic estimate of the true DCCA objective, and assume that the eigenvalues of autocovar': 921,\n",
       " 'limited ability to scaleto many classes because the attribute ontology has to be manually defined. To address this limitation, .': 922,\n",
       " ', restaurant/hotel search  and Belz nonestrong positive NLG, weather forecastStent . weak positive negative paraphrasing of news': 923,\n",
       " 'esentation , which captures word semantics based on context. While word-vector representation is not new, recent algorithms e.g.': 924,\n",
       " 'another natural extension to transfer across tasks with different state-action spaces. The setup from Progressive ral Networks [': 925,\n",
       " 'source language and nese as the target language. The labeled English data consists of balanced labels of 650k lp reviews from  .': 926,\n",
       " 'ieved promising performance on another one-sentence summarization benchmark.Note that, we would like to but fail to take COPYNET': 927,\n",
       " 'assumed fixed graph structure and did not share any weights among neighborhoods. Several works have independently dealt with thi': 928,\n",
       " 'that learns the semantic representation for the question answering task solely based on questions and answers in natural languag': 929,\n",
       " '.An overview of such approaches can be seen in Table IV and is presented in more detail in the following sections.We categorize ': 930,\n",
       " 'ge modelsinclude .Sequence classification.Another recent end-to-end model uses character-level inputs for documentclassification': 931,\n",
       " 'ces in recent years. Combined with the attention mechanism, NMT has reported state-of-the-art results for several language pairs': 932,\n",
       " 'and captioning movshovitz2013natural . We report the log-likelihood of generating the actual code captions based on the learned ': 933,\n",
       " ', unless they are generated with template formats  or simulation  in which case they don’t reflect the free-form of natural conv': 934,\n",
       " ' Convolutional Networks, while still achieving good performance. A soft or differentiable attention mechanisms have been used in': 935,\n",
       " 'silver2016mastering  beat Go human champion13.Recurrent neural networks  have been used successfully for sequence learning tasks': 936,\n",
       " ' driving forces behind the recent success of deep learning in challenging tasks, such as object recognition , speech recognition': 937,\n",
       " 'he noise in the stochastic gradient, or to exploit second order information so as to use Newton-like updates with large stepsize': 938,\n",
       " 's learned distribution, it is straightforward to generate a new sequence by iteratively generating a symbol at each time step. .': 939,\n",
       " 'tor was only fed into the RNN once at the first time step in , while it was used at each time step of the RNN in .Most recently,': 940,\n",
       " ', image captioning  or object detection .re, we will briefly review some of the recent approaches to image classification and ob': 941,\n",
       " ', which is a CNN model on character encoding and is the primary character-based baseline model that we are comparing with.Table ': 942,\n",
       " 'applies a similar idea to improve long-term generation of a recurrent neural language model. Other approaches  . ;  .  extend th': 943,\n",
       " 'ract image representations from a pre-trained CNN  . ;  . .Unrelated to images, CNNs have also been used for text classification': 944,\n",
       " 'have shown that better word representations can be obtained by subsampling of the frequent words. We thus define the following s': 945,\n",
       " '. While benefiting from the strong learning ability, CNNs also have to face the crucial issue of overfilling. Considerable effor': 946,\n",
       " '.    proposed an attention mechanism that learns to weight the multi-scale features at each pixel location. Some previous works ': 947,\n",
       " ' penalty, and training dataset to be identical to the protein homology test case in the recent Stochastic Average Gradient paper': 948,\n",
       " ', and is used to improve the training time and the accuracy of the network.Activation nction are nonlinear functions which are a': 949,\n",
       " '. Table I summarizeseach network’s fully trained top-1/top-5 classification accuracy, theminibatch sizes used for training, and ': 950,\n",
       " 'improved  by feeding the outputs of CNN into a MRF with simple pairwise potentials,but it treated CNN and MRF as separate compon': 951,\n",
       " 'r.See Appendix B for an analysis of a trained model.In this section, we consider two multi-agent tasks using the zeseenvironment': 952,\n",
       " 'o jointly localize highlights in videos and generate their titles.Addressing exposure bias. Recently, the issue of exposure bias': 953,\n",
       " 'rchical  clustering: we use the fastcluster method, , which has a complexity of Θ.lding word vectors: we use the skip-gram model': 954,\n",
       " 'l Networks  and Recurrent ral Networks  utilizing a soft attention mechanism. Our architecture is partially similar to models by': 955,\n",
       " 's competitive on a number of sentence classification tasks.These vectors 300-dimensional were trained by the authors of word2vec': 956,\n",
       " 'and multi-objective optimization hdavi . . wever, these works simply assume the functions involved therein to be pschitz and/or ': 957,\n",
       " ', RMSprop, Adam. The update rules of several common optimization algorithms are listed in Table 1.We are interested in finding a': 958,\n",
       " 'take a perceptron the last layer of “VGG11”  andtrain it on the 1,000 Imagenet classes, with no scale or relighting augmentation': 959,\n",
       " 'is employed as the basic sequence modeling component for the encoder and the decoder.For latent structure modeling,we add histor': 960,\n",
       " '.These encouraging early successes have motivated research interest in training more natural-sounding conversational systemsbase': 961,\n",
       " 'l continuous representations of words that have recently rose to fame following the introduction of the acclaimed word2vec model': 962,\n",
       " '. Andindeed, when we start two models from the same randominitialization and then again train each independently on adifferent s': 963,\n",
       " 'd y-Value Memory Networks.Instead, we use RNN based network, which has been successfully used inmany reading comprehension tasks': 964,\n",
       " 'effectiveness of multimodal perception, we pick two popular continuous action DRL algorithms namely Normalized Advantage nction ': 965,\n",
       " 'that can incorporate labels.Given the labeled set∼DL and the unlabeled setx∼DU, The semi-supervised VAE trains a discriminative ': 966,\n",
       " 'he most salient information over embeddings of a sentence.This approach has been used for semantic sentence level matching by  .': 967,\n",
       " 'as a single “package”. For example, nu .  compare the performance of their SoS n-Norm algorithm to the continuous approach of  .': 968,\n",
       " 'cussed in several papers. Some of the issues raised in previous papers are the absence of turn segmentation in subtitling corpus': 969,\n",
       " '. e computational issue associated with AUCL is that it pairs every positive and negative instance, effectively squaring the tra': 970,\n",
       " 'takes one word from the input question sequentially at a time. In Fig 3 , the hidden layer output of the forward GRU  at time in': 971,\n",
       " ' than an optimization algorithm, it can be combined with KFC; we investigated this in our experiments.Projected Natural Gradient': 972,\n",
       " 'English and Italian with high precision.The Europarl corpus is composed of aligned sentences in a number of European languages .': 973,\n",
       " '. Adaptive spatiotemporal feature representation, with dynamic feature abstraction, involves comparing and evaluating different ': 974,\n",
       " '.In such a framework, comparison of two sequences is not done by comparing two vectors each representing an entire sequence.Inst': 975,\n",
       " '.As said, they may not be appropriate for open-domain dialog systems.We adopt the vector pooling approach that summarizes senten': 976,\n",
       " 'udes replacing sigmoidal with tanh activation functions, as well as the centering transformations discussed in the next section.': 977,\n",
       " ', web navigation  and  , information extraction  . , t popularity prediction and tracking  . , and human-computer dialogue syste': 978,\n",
       " 'ning and to produce better results. We set the learning rate of G to 0.0025, D to 10−5, and use a batch size of 100. We use ADAM': 979,\n",
       " 's in performance w.r.t. the variational bound.Evaluation Accurate evaluation of dialogue system responses is a difficult problem': 980,\n",
       " 'are also reported and compared. GUESS is the model which randomly outputs the answer according to the question type. BOW treats ': 981,\n",
       " 'ared Hinge loss. In particular, using the logistic loss corresponds to the maximum likelihood estimation of the sampling model .': 982,\n",
       " ' GA er is a multi-hop architecture which updates the representation of document tokens through multiple bidirectional GRU layers': 983,\n",
       " 'showed that complimenting the feature loss with an adversarial loss leads to more realistic results. The adversarial loss Ladv i': 984,\n",
       " 'hout any intermediate process, leading to an end-to-end system for ASR. CTC technique has shown promising results in Deep Speech': 985,\n",
       " 'for implementation. The lengths of words, paragraphs, and documents are fixed at 24, 128, and 16 with necessary padding or trunc': 986,\n",
       " '.11For EM  training, the test set’s area under the precision recall curve converges after 96 iterations Fig. 1.We also train a c': 987,\n",
       " 'and N  architectures.Our results are shown in Figure 4 and we note the following:   Ms. Pac-n, Asterix and Zaxxon we achieve bet': 988,\n",
       " ' gradient. Considering that random sampling brings very large variance and sometimes unreasonable results in machine translation': 989,\n",
       " 'rameter ρ=0.95 and ε=1e−6 is used for gradient based optimization.Our neural network based framework is implemented using Theano': 990,\n",
       " 'and attentional models that select words fromdocuments sources . ch approaches generally requirethat answers can be retrieved di': 991,\n",
       " 'other hand, modern RNNsrely on gated nonlinearities such as long short-term memory  chreiter1997  cellsor gated recurrent units ': 992,\n",
       " ' whose correlation structure can exploit both the road segment features and road network topology information.The hyperparameter': 993,\n",
       " 'g cost is very small , we still get a model with better performance than baseline CNN. Also by combination with the maxout units': 994,\n",
       " ', with an encoder to read and encode a source-language sentence into a vector, from which a decoder generates a target-language ': 995,\n",
       " ' fraction of the hidden states based on the current hidden state and input jernite2016_variable , or following periodic patterns': 996,\n",
       " 'm in which the input text in the source language is “encoded” using an encoder network to produce a fixed-length representation ': 997,\n",
       " 'word w. In our task, there are two labels L={NA,DP} corresponding to non-pro-drop or pro-drop pronouns, thus y∈L.Word embeddings': 998,\n",
       " 'iments are listed in Table 6 and Table 7 respectively. We include the state-of-art results of max-margin domain transformations ': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_citated_text  = df['#1 String'].values.tolist()\n",
    "right_citated_text = df['#2 String'].values.tolist()\n",
    "total_citated_text = list(set(left_citated_text + right_citated_text))\n",
    "citated_voca = {}\n",
    "left_citated_id = []\n",
    "right_citated_id = []\n",
    "for i, v in enumerate(total_citated_text):\n",
    "    citated_voca[v] = i\n",
    "citated_voca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l, r in zip(df['#1 String'], df['#2 String']):\n",
    "        left_citated_id.append(citated_voca[l])\n",
    "        right_citated_id.append(citated_voca[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-c43c85095a63>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['#1 ID'] = left_citated_id\n",
      "<ipython-input-107-c43c85095a63>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['#2 ID'] = right_citated_id\n"
     ]
    }
   ],
   "source": [
    "df['#1 ID'] = left_citated_id\n",
    "df['#2 ID'] = right_citated_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_citated_text</th>\n",
       "      <th>right_citated_text</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>target_year</th>\n",
       "      <th>target_author</th>\n",
       "      <th>source_author</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1409.3215v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>ilya sutskever;oriol vinyals;quoc v le</td>\n",
       "      <td>le, recurrent neural networks  have made swift...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>8103</td>\n",
       "      <td>7582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1412.7449v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;lukasz kaiser;terry koo;slav pet...</td>\n",
       "      <td>networks  have made swift inroads intomany str...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>818</td>\n",
       "      <td>21392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reproducibility. All code, data, and experimen...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1506.03134v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;meire fortunato;navdeep jaitly</td>\n",
       "      <td>n-based copying can be seen as acombination of...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>10444</td>\n",
       "      <td>2237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>st like CWS and POS tagging, automatic prosody...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>1511.00360v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>chuang ding;lei xie;jie yan;weini zhang;yang liu</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>sing neural networks from raw text in a fully ...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>5440</td>\n",
       "      <td>14282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We begin by considering a document as the set ...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>s their usefulness for real-world tasks.As a f...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>23427</td>\n",
       "      <td>10153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16662</th>\n",
       "      <td>With human annotation of data, significant int...</td>\n",
       "      <td>proposed a yesian EM framework for continuous-...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>1512.02393v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>tingting zhu;nic dunkley;joachim behar;david a...</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "      <td>ion of each expert annotator and the underlyin...</td>\n",
       "      <td>proposed a yesian EM framework for continuous-...</td>\n",
       "      <td>6892</td>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16663</th>\n",
       "      <td>An effective probabilistic approach to aggrega...</td>\n",
       "      <td>. as is defined as the inverse of accuracy: It...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>1512.02393v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>tingting zhu;nic dunkley;joachim behar;david a...</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "      <td>reduce annotator inter- and intra-variability....</td>\n",
       "      <td>. as is defined as the inverse of accuracy: It...</td>\n",
       "      <td>20408</td>\n",
       "      <td>15313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16665</th>\n",
       "      <td>This approach works reasonably well, but does ...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "      <td>1610.03946v1</td>\n",
       "      <td>1301.3781v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>jessica ficler;yoav goldberg</td>\n",
       "      <td>tomas mikolov;kai chen;greg corrado;jeffrey dean</td>\n",
       "      <td>ell in the CKY chart.3In both approaches,the P...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "      <td>2953</td>\n",
       "      <td>4794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16667</th>\n",
       "      <td>Fig. 10 shows histograms of the execution time...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1603.02199v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>sergey levine;peter pastor;alex krizhevsky;dei...</td>\n",
       "      <td>l concept is shown in Fig. 1.Recent studies ha...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>20485</td>\n",
       "      <td>21074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>As we can see from Eq. , the target of the lea...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1312.5602v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>volodymyr mnih;koray kavukcuoglu;david silver;...</td>\n",
       "      <td>work decision.Algorithm 2 shows the learning t...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1391</td>\n",
       "      <td>22959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12230 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       left_citated_text  \\\n",
       "0      We conducted additional experiments on artific...   \n",
       "1      We conducted additional experiments on artific...   \n",
       "2      Reproducibility. All code, data, and experimen...   \n",
       "3      st like CWS and POS tagging, automatic prosody...   \n",
       "4      We begin by considering a document as the set ...   \n",
       "...                                                  ...   \n",
       "16662  With human annotation of data, significant int...   \n",
       "16663  An effective probabilistic approach to aggrega...   \n",
       "16665  This approach works reasonably well, but does ...   \n",
       "16667  Fig. 10 shows histograms of the execution time...   \n",
       "16668  As we can see from Eq. , the target of the lea...   \n",
       "\n",
       "                                      right_citated_text     target_id  \\\n",
       "0      andsyntactic parsing .Because RNNs make very f...  1606.03622v1   \n",
       "1      .Because RNNs make very few domain-specific as...  1606.03622v1   \n",
       "2      ; in a Pointer Network,the only way to generat...  1606.03622v1   \n",
       "3      . Recently, nsur .  have shown superior perfor...  1511.00360v1   \n",
       "4      model trained on the Google News dataset3.In a...  1705.10900v1   \n",
       "...                                                  ...           ...   \n",
       "16662  proposed a yesian EM framework for continuous-...  1503.06619v1   \n",
       "16663  . as is defined as the inverse of accuracy: It...  1503.06619v1   \n",
       "16665  on the POS sequences in PTB trainingset.The re...  1610.03946v1   \n",
       "16667  , but none of these methods can be applied dir...  1708.04033v1   \n",
       "16668  , we use multiple long short-term memory  laye...  1708.04033v1   \n",
       "\n",
       "          source_id  target_year  \\\n",
       "0       1409.3215v1         2016   \n",
       "1       1412.7449v1         2016   \n",
       "2      1506.03134v1         2016   \n",
       "3       1310.4546v1         2015   \n",
       "4       1310.4546v1         2017   \n",
       "...             ...          ...   \n",
       "16662  1512.02393v1         2015   \n",
       "16663  1512.02393v1         2015   \n",
       "16665   1301.3781v1         2016   \n",
       "16667  1603.02199v1         2017   \n",
       "16668   1312.5602v1         2017   \n",
       "\n",
       "                                           target_author  \\\n",
       "0                                  robin jia;percy liang   \n",
       "1                                  robin jia;percy liang   \n",
       "2                                  robin jia;percy liang   \n",
       "3       chuang ding;lei xie;jie yan;weini zhang;yang liu   \n",
       "4      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "...                                                  ...   \n",
       "16662  tingting zhu;nic dunkley;joachim behar;david a...   \n",
       "16663  tingting zhu;nic dunkley;joachim behar;david a...   \n",
       "16665                       jessica ficler;yoav goldberg   \n",
       "16667  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "16668  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "\n",
       "                                           source_author  \\\n",
       "0                 ilya sutskever;oriol vinyals;quoc v le   \n",
       "1      oriol vinyals;lukasz kaiser;terry koo;slav pet...   \n",
       "2           oriol vinyals;meire fortunato;navdeep jaitly   \n",
       "3      tomas mikolov;ilya sutskever;kai chen 0010;gre...   \n",
       "4      tomas mikolov;ilya sutskever;kai chen 0010;gre...   \n",
       "...                                                  ...   \n",
       "16662                  changbo zhu;huan xu;shuicheng yan   \n",
       "16663                  changbo zhu;huan xu;shuicheng yan   \n",
       "16665   tomas mikolov;kai chen;greg corrado;jeffrey dean   \n",
       "16667  sergey levine;peter pastor;alex krizhevsky;dei...   \n",
       "16668  volodymyr mnih;koray kavukcuoglu;david silver;...   \n",
       "\n",
       "                                               #1 String  \\\n",
       "0      le, recurrent neural networks  have made swift...   \n",
       "1      networks  have made swift inroads intomany str...   \n",
       "2      n-based copying can be seen as acombination of...   \n",
       "3      sing neural networks from raw text in a fully ...   \n",
       "4      s their usefulness for real-world tasks.As a f...   \n",
       "...                                                  ...   \n",
       "16662  ion of each expert annotator and the underlyin...   \n",
       "16663  reduce annotator inter- and intra-variability....   \n",
       "16665  ell in the CKY chart.3In both approaches,the P...   \n",
       "16667  l concept is shown in Fig. 1.Recent studies ha...   \n",
       "16668  work decision.Algorithm 2 shows the learning t...   \n",
       "\n",
       "                                               #2 String  #1 ID  #2 ID  \n",
       "0      andsyntactic parsing .Because RNNs make very f...   8103   7582  \n",
       "1      .Because RNNs make very few domain-specific as...    818  21392  \n",
       "2      ; in a Pointer Network,the only way to generat...  10444   2237  \n",
       "3      . Recently, nsur .  have shown superior perfor...   5440  14282  \n",
       "4      model trained on the Google News dataset3.In a...  23427  10153  \n",
       "...                                                  ...    ...    ...  \n",
       "16662  proposed a yesian EM framework for continuous-...   6892    723  \n",
       "16663  . as is defined as the inverse of accuracy: It...  20408  15313  \n",
       "16665  on the POS sequences in PTB trainingset.The re...   2953   4794  \n",
       "16667  , but none of these methods can be applied dir...  20485  21074  \n",
       "16668  , we use multiple long short-term memory  laye...   1391  22959  \n",
       "\n",
       "[12230 rows x 11 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2017\n",
    "train_idx = df['target_year'][df['target_year'] < year].index\n",
    "test_idx = df['target_year'][df['target_year'] >= year].index\n",
    "train_df = df.loc[train_idx]\n",
    "test_df = df.loc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_citated_text</th>\n",
       "      <th>right_citated_text</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>target_year</th>\n",
       "      <th>target_author</th>\n",
       "      <th>source_author</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We begin by considering a document as the set ...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>s their usefulness for real-world tasks.As a f...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>23427</td>\n",
       "      <td>10153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>With GMM and Aw2v+Ph, the F1-Score of clusteri...</td>\n",
       "      <td>. We observe that by themselves, Ph embeddings...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1408.5882v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>yoon kim</td>\n",
       "      <td>presented in Table 1. For comparison, we provi...</td>\n",
       "      <td>. We observe that by themselves, Ph embeddings...</td>\n",
       "      <td>13225</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We evaluate our method on the CSP dataset8. Th...</td>\n",
       "      <td>. We observe that Ph embeddings perform poorly...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1405.4053v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>quoc v le;tomas mikolov</td>\n",
       "      <td>e word vectors in that sentence. The results a...</td>\n",
       "      <td>. We observe that Ph embeddings perform poorly...</td>\n",
       "      <td>15416</td>\n",
       "      <td>1612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We perform document-level binary sentiment cla...</td>\n",
       "      <td>or purely word vector based methods  . .cument...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1405.4053v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>quoc v le;tomas mikolov</td>\n",
       "      <td>d recent attention owing to their utility in s...</td>\n",
       "      <td>or purely word vector based methods  . .cument...</td>\n",
       "      <td>12091</td>\n",
       "      <td>22025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In the lack of sufficient health status inform...</td>\n",
       "      <td>. Two most popular approaches of DGM rely on v...</td>\n",
       "      <td>1709.00845v1</td>\n",
       "      <td>1406.5298v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>andre s yoon;taehoon lee;yongsub lim;deokwoo j...</td>\n",
       "      <td>diederik p kingma;shakir mohamed;danilo jimene...</td>\n",
       "      <td>enerative models  have recently achieved state...</td>\n",
       "      <td>. Two most popular approaches of DGM rely on v...</td>\n",
       "      <td>9400</td>\n",
       "      <td>16385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16649</th>\n",
       "      <td>We now demonstrate cascading modern CNN archit...</td>\n",
       "      <td>, as a reference and construct a similar archi...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>1605.07146v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>edouard oyallon;eugene belilovsky;sergey zagor...</td>\n",
       "      <td>sergey zagoruyko;nikos komodakis</td>\n",
       "      <td>ication task. re we consider cascading the sca...</td>\n",
       "      <td>, as a reference and construct a similar archi...</td>\n",
       "      <td>11129</td>\n",
       "      <td>1356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16650</th>\n",
       "      <td>For the scattering transform we used J=2 which...</td>\n",
       "      <td>. Specifically we modify the WRN of 16 layers ...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>1605.07146v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>edouard oyallon;eugene belilovsky;sergey zagor...</td>\n",
       "      <td>sergey zagoruyko;nikos komodakis</td>\n",
       "      <td>on CIFAR-10, all based on end-to-end learned C...</td>\n",
       "      <td>. Specifically we modify the WRN of 16 layers ...</td>\n",
       "      <td>13604</td>\n",
       "      <td>7143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16651</th>\n",
       "      <td>We now consider the popular CIFAR-10 dataset c...</td>\n",
       "      <td>. Table 3 reports the accuracy in the unsuperv...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>1502.03167v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>edouard oyallon;eugene belilovsky;sergey zagor...</td>\n",
       "      <td>sergey ioffe;christian szegedy</td>\n",
       "      <td>on in both case. We utilize batch normalizatio...</td>\n",
       "      <td>. Table 3 reports the accuracy in the unsuperv...</td>\n",
       "      <td>2229</td>\n",
       "      <td>1423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16667</th>\n",
       "      <td>Fig. 10 shows histograms of the execution time...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1603.02199v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>sergey levine;peter pastor;alex krizhevsky;dei...</td>\n",
       "      <td>l concept is shown in Fig. 1.Recent studies ha...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>20485</td>\n",
       "      <td>21074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>As we can see from Eq. , the target of the lea...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1312.5602v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>volodymyr mnih;koray kavukcuoglu;david silver;...</td>\n",
       "      <td>work decision.Algorithm 2 shows the learning t...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1391</td>\n",
       "      <td>22959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4783 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       left_citated_text  \\\n",
       "4      We begin by considering a document as the set ...   \n",
       "5      With GMM and Aw2v+Ph, the F1-Score of clusteri...   \n",
       "6      We evaluate our method on the CSP dataset8. Th...   \n",
       "7      We perform document-level binary sentiment cla...   \n",
       "8      In the lack of sufficient health status inform...   \n",
       "...                                                  ...   \n",
       "16649  We now demonstrate cascading modern CNN archit...   \n",
       "16650  For the scattering transform we used J=2 which...   \n",
       "16651  We now consider the popular CIFAR-10 dataset c...   \n",
       "16667  Fig. 10 shows histograms of the execution time...   \n",
       "16668  As we can see from Eq. , the target of the lea...   \n",
       "\n",
       "                                      right_citated_text     target_id  \\\n",
       "4      model trained on the Google News dataset3.In a...  1705.10900v1   \n",
       "5      . We observe that by themselves, Ph embeddings...  1705.10900v1   \n",
       "6      . We observe that Ph embeddings perform poorly...  1705.10900v1   \n",
       "7      or purely word vector based methods  . .cument...  1705.10900v1   \n",
       "8      . Two most popular approaches of DGM rely on v...  1709.00845v1   \n",
       "...                                                  ...           ...   \n",
       "16649  , as a reference and construct a similar archi...  1703.08961v1   \n",
       "16650  . Specifically we modify the WRN of 16 layers ...  1703.08961v1   \n",
       "16651  . Table 3 reports the accuracy in the unsuperv...  1703.08961v1   \n",
       "16667  , but none of these methods can be applied dir...  1708.04033v1   \n",
       "16668  , we use multiple long short-term memory  laye...  1708.04033v1   \n",
       "\n",
       "          source_id  target_year  \\\n",
       "4       1310.4546v1         2017   \n",
       "5       1408.5882v1         2017   \n",
       "6       1405.4053v1         2017   \n",
       "7       1405.4053v1         2017   \n",
       "8       1406.5298v1         2017   \n",
       "...             ...          ...   \n",
       "16649  1605.07146v1         2017   \n",
       "16650  1605.07146v1         2017   \n",
       "16651  1502.03167v1         2017   \n",
       "16667  1603.02199v1         2017   \n",
       "16668   1312.5602v1         2017   \n",
       "\n",
       "                                           target_author  \\\n",
       "4      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "5      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "6      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "7      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "8      andre s yoon;taehoon lee;yongsub lim;deokwoo j...   \n",
       "...                                                  ...   \n",
       "16649  edouard oyallon;eugene belilovsky;sergey zagor...   \n",
       "16650  edouard oyallon;eugene belilovsky;sergey zagor...   \n",
       "16651  edouard oyallon;eugene belilovsky;sergey zagor...   \n",
       "16667  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "16668  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "\n",
       "                                           source_author  \\\n",
       "4      tomas mikolov;ilya sutskever;kai chen 0010;gre...   \n",
       "5                                               yoon kim   \n",
       "6                                quoc v le;tomas mikolov   \n",
       "7                                quoc v le;tomas mikolov   \n",
       "8      diederik p kingma;shakir mohamed;danilo jimene...   \n",
       "...                                                  ...   \n",
       "16649                   sergey zagoruyko;nikos komodakis   \n",
       "16650                   sergey zagoruyko;nikos komodakis   \n",
       "16651                     sergey ioffe;christian szegedy   \n",
       "16667  sergey levine;peter pastor;alex krizhevsky;dei...   \n",
       "16668  volodymyr mnih;koray kavukcuoglu;david silver;...   \n",
       "\n",
       "                                               #1 String  \\\n",
       "4      s their usefulness for real-world tasks.As a f...   \n",
       "5      presented in Table 1. For comparison, we provi...   \n",
       "6      e word vectors in that sentence. The results a...   \n",
       "7      d recent attention owing to their utility in s...   \n",
       "8      enerative models  have recently achieved state...   \n",
       "...                                                  ...   \n",
       "16649  ication task. re we consider cascading the sca...   \n",
       "16650  on CIFAR-10, all based on end-to-end learned C...   \n",
       "16651  on in both case. We utilize batch normalizatio...   \n",
       "16667  l concept is shown in Fig. 1.Recent studies ha...   \n",
       "16668  work decision.Algorithm 2 shows the learning t...   \n",
       "\n",
       "                                               #2 String  #1 ID  #2 ID  \n",
       "4      model trained on the Google News dataset3.In a...  23427  10153  \n",
       "5      . We observe that by themselves, Ph embeddings...  13225    377  \n",
       "6      . We observe that Ph embeddings perform poorly...  15416   1612  \n",
       "7      or purely word vector based methods  . .cument...  12091  22025  \n",
       "8      . Two most popular approaches of DGM rely on v...   9400  16385  \n",
       "...                                                  ...    ...    ...  \n",
       "16649  , as a reference and construct a similar archi...  11129   1356  \n",
       "16650  . Specifically we modify the WRN of 16 layers ...  13604   7143  \n",
       "16651  . Table 3 reports the accuracy in the unsuperv...   2229   1423  \n",
       "16667  , but none of these methods can be applied dir...  20485  21074  \n",
       "16668  , we use multiple long short-term memory  laye...   1391  22959  \n",
       "\n",
       "[4783 rows x 11 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_column = ['Quality', '#1 ID', '#2 ID', '#1 String', '#2 String', 'target_id', 'source_author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit_transform(df['source_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg_label': 0, 'pos_label': 1, 'sparse_output': False}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_argmax(df, lb):\n",
    "\n",
    "    y = df['source_id'].values\n",
    "    y = lb.transform(y)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    df['Quality'] = y\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1 = convert_argmax(train_df, lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_citated_text</th>\n",
       "      <th>right_citated_text</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>target_year</th>\n",
       "      <th>target_author</th>\n",
       "      <th>source_author</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "      <th>Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1409.3215v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>ilya sutskever;oriol vinyals;quoc v le</td>\n",
       "      <td>le, recurrent neural networks  have made swift...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>8103</td>\n",
       "      <td>7582</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1412.7449v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;lukasz kaiser;terry koo;slav pet...</td>\n",
       "      <td>networks  have made swift inroads intomany str...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>818</td>\n",
       "      <td>21392</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reproducibility. All code, data, and experimen...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>1506.03134v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>oriol vinyals;meire fortunato;navdeep jaitly</td>\n",
       "      <td>n-based copying can be seen as acombination of...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>10444</td>\n",
       "      <td>2237</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>st like CWS and POS tagging, automatic prosody...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>1511.00360v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>chuang ding;lei xie;jie yan;weini zhang;yang liu</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>sing neural networks from raw text in a fully ...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>5440</td>\n",
       "      <td>14282</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>stillation has been traditionally applied to n...</td>\n",
       "      <td>usedcompression to transfer knowledge from a d...</td>\n",
       "      <td>1511.06295v1</td>\n",
       "      <td>1312.6184v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>andrei a rusu;sergio gomez colmenarejo;caglar ...</td>\n",
       "      <td>jimmy ba;rich caruana</td>\n",
       "      <td>by cila . , whoproposed it as a means of comp...</td>\n",
       "      <td>usedcompression to transfer knowledge from a d...</td>\n",
       "      <td>7301</td>\n",
       "      <td>3444</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16660</th>\n",
       "      <td>An important inspiration for the proposed fram...</td>\n",
       "      <td>. Two important differenceswith the approach p...</td>\n",
       "      <td>1502.04156v1</td>\n",
       "      <td>1406.2751v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>yoshua bengio;dong-hyun lee;jorg bornschein;th...</td>\n",
       "      <td>j\\\"org bornschein;yoshua bengio</td>\n",
       "      <td>lgorithm  and which finds very interesting ins...</td>\n",
       "      <td>. Two important differenceswith the approach p...</td>\n",
       "      <td>19965</td>\n",
       "      <td>15181</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16661</th>\n",
       "      <td>The proposal made here also owes a lot to the ...</td>\n",
       "      <td>and generativeadversarial networks ( ., 2014)...</td>\n",
       "      <td>1502.04156v1</td>\n",
       "      <td>1306.1091v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>yoshua bengio;dong-hyun lee;jorg bornschein;th...</td>\n",
       "      <td>yoshua bengio;eric laufer;guillaume alain;jaso...</td>\n",
       "      <td>about the same or better as was obtained for c...</td>\n",
       "      <td>and generativeadversarial networks ( ., 2014)...</td>\n",
       "      <td>1787</td>\n",
       "      <td>20657</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16662</th>\n",
       "      <td>With human annotation of data, significant int...</td>\n",
       "      <td>proposed a yesian EM framework for continuous-...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>1512.02393v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>tingting zhu;nic dunkley;joachim behar;david a...</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "      <td>ion of each expert annotator and the underlyin...</td>\n",
       "      <td>proposed a yesian EM framework for continuous-...</td>\n",
       "      <td>6892</td>\n",
       "      <td>723</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16663</th>\n",
       "      <td>An effective probabilistic approach to aggrega...</td>\n",
       "      <td>. as is defined as the inverse of accuracy: It...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>1512.02393v1</td>\n",
       "      <td>2015</td>\n",
       "      <td>tingting zhu;nic dunkley;joachim behar;david a...</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "      <td>reduce annotator inter- and intra-variability....</td>\n",
       "      <td>. as is defined as the inverse of accuracy: It...</td>\n",
       "      <td>20408</td>\n",
       "      <td>15313</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16665</th>\n",
       "      <td>This approach works reasonably well, but does ...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "      <td>1610.03946v1</td>\n",
       "      <td>1301.3781v1</td>\n",
       "      <td>2016</td>\n",
       "      <td>jessica ficler;yoav goldberg</td>\n",
       "      <td>tomas mikolov;kai chen;greg corrado;jeffrey dean</td>\n",
       "      <td>ell in the CKY chart.3In both approaches,the P...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "      <td>2953</td>\n",
       "      <td>4794</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7447 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       left_citated_text  \\\n",
       "0      We conducted additional experiments on artific...   \n",
       "1      We conducted additional experiments on artific...   \n",
       "2      Reproducibility. All code, data, and experimen...   \n",
       "3      st like CWS and POS tagging, automatic prosody...   \n",
       "13     stillation has been traditionally applied to n...   \n",
       "...                                                  ...   \n",
       "16660  An important inspiration for the proposed fram...   \n",
       "16661  The proposal made here also owes a lot to the ...   \n",
       "16662  With human annotation of data, significant int...   \n",
       "16663  An effective probabilistic approach to aggrega...   \n",
       "16665  This approach works reasonably well, but does ...   \n",
       "\n",
       "                                      right_citated_text     target_id  \\\n",
       "0      andsyntactic parsing .Because RNNs make very f...  1606.03622v1   \n",
       "1      .Because RNNs make very few domain-specific as...  1606.03622v1   \n",
       "2      ; in a Pointer Network,the only way to generat...  1606.03622v1   \n",
       "3      . Recently, nsur .  have shown superior perfor...  1511.00360v1   \n",
       "13     usedcompression to transfer knowledge from a d...  1511.06295v1   \n",
       "...                                                  ...           ...   \n",
       "16660  . Two important differenceswith the approach p...  1502.04156v1   \n",
       "16661   and generativeadversarial networks ( ., 2014)...  1502.04156v1   \n",
       "16662  proposed a yesian EM framework for continuous-...  1503.06619v1   \n",
       "16663  . as is defined as the inverse of accuracy: It...  1503.06619v1   \n",
       "16665  on the POS sequences in PTB trainingset.The re...  1610.03946v1   \n",
       "\n",
       "          source_id  target_year  \\\n",
       "0       1409.3215v1         2016   \n",
       "1       1412.7449v1         2016   \n",
       "2      1506.03134v1         2016   \n",
       "3       1310.4546v1         2015   \n",
       "13      1312.6184v1         2015   \n",
       "...             ...          ...   \n",
       "16660   1406.2751v1         2015   \n",
       "16661   1306.1091v1         2015   \n",
       "16662  1512.02393v1         2015   \n",
       "16663  1512.02393v1         2015   \n",
       "16665   1301.3781v1         2016   \n",
       "\n",
       "                                           target_author  \\\n",
       "0                                  robin jia;percy liang   \n",
       "1                                  robin jia;percy liang   \n",
       "2                                  robin jia;percy liang   \n",
       "3       chuang ding;lei xie;jie yan;weini zhang;yang liu   \n",
       "13     andrei a rusu;sergio gomez colmenarejo;caglar ...   \n",
       "...                                                  ...   \n",
       "16660  yoshua bengio;dong-hyun lee;jorg bornschein;th...   \n",
       "16661  yoshua bengio;dong-hyun lee;jorg bornschein;th...   \n",
       "16662  tingting zhu;nic dunkley;joachim behar;david a...   \n",
       "16663  tingting zhu;nic dunkley;joachim behar;david a...   \n",
       "16665                       jessica ficler;yoav goldberg   \n",
       "\n",
       "                                           source_author  \\\n",
       "0                 ilya sutskever;oriol vinyals;quoc v le   \n",
       "1      oriol vinyals;lukasz kaiser;terry koo;slav pet...   \n",
       "2           oriol vinyals;meire fortunato;navdeep jaitly   \n",
       "3      tomas mikolov;ilya sutskever;kai chen 0010;gre...   \n",
       "13                                 jimmy ba;rich caruana   \n",
       "...                                                  ...   \n",
       "16660                    j\\\"org bornschein;yoshua bengio   \n",
       "16661  yoshua bengio;eric laufer;guillaume alain;jaso...   \n",
       "16662                  changbo zhu;huan xu;shuicheng yan   \n",
       "16663                  changbo zhu;huan xu;shuicheng yan   \n",
       "16665   tomas mikolov;kai chen;greg corrado;jeffrey dean   \n",
       "\n",
       "                                               #1 String  \\\n",
       "0      le, recurrent neural networks  have made swift...   \n",
       "1      networks  have made swift inroads intomany str...   \n",
       "2      n-based copying can be seen as acombination of...   \n",
       "3      sing neural networks from raw text in a fully ...   \n",
       "13      by cila . , whoproposed it as a means of comp...   \n",
       "...                                                  ...   \n",
       "16660  lgorithm  and which finds very interesting ins...   \n",
       "16661  about the same or better as was obtained for c...   \n",
       "16662  ion of each expert annotator and the underlyin...   \n",
       "16663  reduce annotator inter- and intra-variability....   \n",
       "16665  ell in the CKY chart.3In both approaches,the P...   \n",
       "\n",
       "                                               #2 String  #1 ID  #2 ID  \\\n",
       "0      andsyntactic parsing .Because RNNs make very f...   8103   7582   \n",
       "1      .Because RNNs make very few domain-specific as...    818  21392   \n",
       "2      ; in a Pointer Network,the only way to generat...  10444   2237   \n",
       "3      . Recently, nsur .  have shown superior perfor...   5440  14282   \n",
       "13     usedcompression to transfer knowledge from a d...   7301   3444   \n",
       "...                                                  ...    ...    ...   \n",
       "16660  . Two important differenceswith the approach p...  19965  15181   \n",
       "16661   and generativeadversarial networks ( ., 2014)...   1787  20657   \n",
       "16662  proposed a yesian EM framework for continuous-...   6892    723   \n",
       "16663  . as is defined as the inverse of accuracy: It...  20408  15313   \n",
       "16665  on the POS sequences in PTB trainingset.The re...   2953   4794   \n",
       "\n",
       "       Quality  \n",
       "0          139  \n",
       "1          174  \n",
       "2          232  \n",
       "3           90  \n",
       "13         102  \n",
       "...        ...  \n",
       "16660      127  \n",
       "16661       80  \n",
       "16662      319  \n",
       "16663      319  \n",
       "16665       69  \n",
       "\n",
       "[7447 rows x 12 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1 = convert_argmax(test_df, lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_citated_text</th>\n",
       "      <th>right_citated_text</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>target_year</th>\n",
       "      <th>target_author</th>\n",
       "      <th>source_author</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "      <th>Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We begin by considering a document as the set ...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>s their usefulness for real-world tasks.As a f...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>23427</td>\n",
       "      <td>10153</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>With GMM and Aw2v+Ph, the F1-Score of clusteri...</td>\n",
       "      <td>. We observe that by themselves, Ph embeddings...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1408.5882v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>yoon kim</td>\n",
       "      <td>presented in Table 1. For comparison, we provi...</td>\n",
       "      <td>. We observe that by themselves, Ph embeddings...</td>\n",
       "      <td>13225</td>\n",
       "      <td>377</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We evaluate our method on the CSP dataset8. Th...</td>\n",
       "      <td>. We observe that Ph embeddings perform poorly...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1405.4053v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>quoc v le;tomas mikolov</td>\n",
       "      <td>e word vectors in that sentence. The results a...</td>\n",
       "      <td>. We observe that Ph embeddings perform poorly...</td>\n",
       "      <td>15416</td>\n",
       "      <td>1612</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We perform document-level binary sentiment cla...</td>\n",
       "      <td>or purely word vector based methods  . .cument...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>1405.4053v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>quoc v le;tomas mikolov</td>\n",
       "      <td>d recent attention owing to their utility in s...</td>\n",
       "      <td>or purely word vector based methods  . .cument...</td>\n",
       "      <td>12091</td>\n",
       "      <td>22025</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In the lack of sufficient health status inform...</td>\n",
       "      <td>. Two most popular approaches of DGM rely on v...</td>\n",
       "      <td>1709.00845v1</td>\n",
       "      <td>1406.5298v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>andre s yoon;taehoon lee;yongsub lim;deokwoo j...</td>\n",
       "      <td>diederik p kingma;shakir mohamed;danilo jimene...</td>\n",
       "      <td>enerative models  have recently achieved state...</td>\n",
       "      <td>. Two most popular approaches of DGM rely on v...</td>\n",
       "      <td>9400</td>\n",
       "      <td>16385</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16649</th>\n",
       "      <td>We now demonstrate cascading modern CNN archit...</td>\n",
       "      <td>, as a reference and construct a similar archi...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>1605.07146v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>edouard oyallon;eugene belilovsky;sergey zagor...</td>\n",
       "      <td>sergey zagoruyko;nikos komodakis</td>\n",
       "      <td>ication task. re we consider cascading the sca...</td>\n",
       "      <td>, as a reference and construct a similar archi...</td>\n",
       "      <td>11129</td>\n",
       "      <td>1356</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16650</th>\n",
       "      <td>For the scattering transform we used J=2 which...</td>\n",
       "      <td>. Specifically we modify the WRN of 16 layers ...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>1605.07146v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>edouard oyallon;eugene belilovsky;sergey zagor...</td>\n",
       "      <td>sergey zagoruyko;nikos komodakis</td>\n",
       "      <td>on CIFAR-10, all based on end-to-end learned C...</td>\n",
       "      <td>. Specifically we modify the WRN of 16 layers ...</td>\n",
       "      <td>13604</td>\n",
       "      <td>7143</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16651</th>\n",
       "      <td>We now consider the popular CIFAR-10 dataset c...</td>\n",
       "      <td>. Table 3 reports the accuracy in the unsuperv...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>1502.03167v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>edouard oyallon;eugene belilovsky;sergey zagor...</td>\n",
       "      <td>sergey ioffe;christian szegedy</td>\n",
       "      <td>on in both case. We utilize batch normalizatio...</td>\n",
       "      <td>. Table 3 reports the accuracy in the unsuperv...</td>\n",
       "      <td>2229</td>\n",
       "      <td>1423</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16667</th>\n",
       "      <td>Fig. 10 shows histograms of the execution time...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1603.02199v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>sergey levine;peter pastor;alex krizhevsky;dei...</td>\n",
       "      <td>l concept is shown in Fig. 1.Recent studies ha...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>20485</td>\n",
       "      <td>21074</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>As we can see from Eq. , the target of the lea...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>1312.5602v1</td>\n",
       "      <td>2017</td>\n",
       "      <td>tadanobu inoue;giovanni de magistris;asim muna...</td>\n",
       "      <td>volodymyr mnih;koray kavukcuoglu;david silver;...</td>\n",
       "      <td>work decision.Algorithm 2 shows the learning t...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1391</td>\n",
       "      <td>22959</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4783 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       left_citated_text  \\\n",
       "4      We begin by considering a document as the set ...   \n",
       "5      With GMM and Aw2v+Ph, the F1-Score of clusteri...   \n",
       "6      We evaluate our method on the CSP dataset8. Th...   \n",
       "7      We perform document-level binary sentiment cla...   \n",
       "8      In the lack of sufficient health status inform...   \n",
       "...                                                  ...   \n",
       "16649  We now demonstrate cascading modern CNN archit...   \n",
       "16650  For the scattering transform we used J=2 which...   \n",
       "16651  We now consider the popular CIFAR-10 dataset c...   \n",
       "16667  Fig. 10 shows histograms of the execution time...   \n",
       "16668  As we can see from Eq. , the target of the lea...   \n",
       "\n",
       "                                      right_citated_text     target_id  \\\n",
       "4      model trained on the Google News dataset3.In a...  1705.10900v1   \n",
       "5      . We observe that by themselves, Ph embeddings...  1705.10900v1   \n",
       "6      . We observe that Ph embeddings perform poorly...  1705.10900v1   \n",
       "7      or purely word vector based methods  . .cument...  1705.10900v1   \n",
       "8      . Two most popular approaches of DGM rely on v...  1709.00845v1   \n",
       "...                                                  ...           ...   \n",
       "16649  , as a reference and construct a similar archi...  1703.08961v1   \n",
       "16650  . Specifically we modify the WRN of 16 layers ...  1703.08961v1   \n",
       "16651  . Table 3 reports the accuracy in the unsuperv...  1703.08961v1   \n",
       "16667  , but none of these methods can be applied dir...  1708.04033v1   \n",
       "16668  , we use multiple long short-term memory  laye...  1708.04033v1   \n",
       "\n",
       "          source_id  target_year  \\\n",
       "4       1310.4546v1         2017   \n",
       "5       1408.5882v1         2017   \n",
       "6       1405.4053v1         2017   \n",
       "7       1405.4053v1         2017   \n",
       "8       1406.5298v1         2017   \n",
       "...             ...          ...   \n",
       "16649  1605.07146v1         2017   \n",
       "16650  1605.07146v1         2017   \n",
       "16651  1502.03167v1         2017   \n",
       "16667  1603.02199v1         2017   \n",
       "16668   1312.5602v1         2017   \n",
       "\n",
       "                                           target_author  \\\n",
       "4      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "5      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "6      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "7      paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "8      andre s yoon;taehoon lee;yongsub lim;deokwoo j...   \n",
       "...                                                  ...   \n",
       "16649  edouard oyallon;eugene belilovsky;sergey zagor...   \n",
       "16650  edouard oyallon;eugene belilovsky;sergey zagor...   \n",
       "16651  edouard oyallon;eugene belilovsky;sergey zagor...   \n",
       "16667  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "16668  tadanobu inoue;giovanni de magistris;asim muna...   \n",
       "\n",
       "                                           source_author  \\\n",
       "4      tomas mikolov;ilya sutskever;kai chen 0010;gre...   \n",
       "5                                               yoon kim   \n",
       "6                                quoc v le;tomas mikolov   \n",
       "7                                quoc v le;tomas mikolov   \n",
       "8      diederik p kingma;shakir mohamed;danilo jimene...   \n",
       "...                                                  ...   \n",
       "16649                   sergey zagoruyko;nikos komodakis   \n",
       "16650                   sergey zagoruyko;nikos komodakis   \n",
       "16651                     sergey ioffe;christian szegedy   \n",
       "16667  sergey levine;peter pastor;alex krizhevsky;dei...   \n",
       "16668  volodymyr mnih;koray kavukcuoglu;david silver;...   \n",
       "\n",
       "                                               #1 String  \\\n",
       "4      s their usefulness for real-world tasks.As a f...   \n",
       "5      presented in Table 1. For comparison, we provi...   \n",
       "6      e word vectors in that sentence. The results a...   \n",
       "7      d recent attention owing to their utility in s...   \n",
       "8      enerative models  have recently achieved state...   \n",
       "...                                                  ...   \n",
       "16649  ication task. re we consider cascading the sca...   \n",
       "16650  on CIFAR-10, all based on end-to-end learned C...   \n",
       "16651  on in both case. We utilize batch normalizatio...   \n",
       "16667  l concept is shown in Fig. 1.Recent studies ha...   \n",
       "16668  work decision.Algorithm 2 shows the learning t...   \n",
       "\n",
       "                                               #2 String  #1 ID  #2 ID  \\\n",
       "4      model trained on the Google News dataset3.In a...  23427  10153   \n",
       "5      . We observe that by themselves, Ph embeddings...  13225    377   \n",
       "6      . We observe that Ph embeddings perform poorly...  15416   1612   \n",
       "7      or purely word vector based methods  . .cument...  12091  22025   \n",
       "8      . Two most popular approaches of DGM rely on v...   9400  16385   \n",
       "...                                                  ...    ...    ...   \n",
       "16649  , as a reference and construct a similar archi...  11129   1356   \n",
       "16650  . Specifically we modify the WRN of 16 layers ...  13604   7143   \n",
       "16651  . Table 3 reports the accuracy in the unsuperv...   2229   1423   \n",
       "16667  , but none of these methods can be applied dir...  20485  21074   \n",
       "16668  , we use multiple long short-term memory  laye...   1391  22959   \n",
       "\n",
       "       Quality  \n",
       "4           90  \n",
       "5          136  \n",
       "6          120  \n",
       "7          120  \n",
       "8          130  \n",
       "...        ...  \n",
       "16649      397  \n",
       "16650      397  \n",
       "16651      185  \n",
       "16667      357  \n",
       "16668       99  \n",
       "\n",
       "[4783 rows x 12 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_df1[bert_column], test_df1[bert_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quality</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>23427</td>\n",
       "      <td>10153</td>\n",
       "      <td>s their usefulness for real-world tasks.As a f...</td>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>136</td>\n",
       "      <td>13225</td>\n",
       "      <td>377</td>\n",
       "      <td>presented in Table 1. For comparison, we provi...</td>\n",
       "      <td>. We observe that by themselves, Ph embeddings...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>yoon kim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>120</td>\n",
       "      <td>15416</td>\n",
       "      <td>1612</td>\n",
       "      <td>e word vectors in that sentence. The results a...</td>\n",
       "      <td>. We observe that Ph embeddings perform poorly...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>quoc v le;tomas mikolov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>120</td>\n",
       "      <td>12091</td>\n",
       "      <td>22025</td>\n",
       "      <td>d recent attention owing to their utility in s...</td>\n",
       "      <td>or purely word vector based methods  . .cument...</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>quoc v le;tomas mikolov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>130</td>\n",
       "      <td>9400</td>\n",
       "      <td>16385</td>\n",
       "      <td>enerative models  have recently achieved state...</td>\n",
       "      <td>. Two most popular approaches of DGM rely on v...</td>\n",
       "      <td>1709.00845v1</td>\n",
       "      <td>diederik p kingma;shakir mohamed;danilo jimene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16649</th>\n",
       "      <td>397</td>\n",
       "      <td>11129</td>\n",
       "      <td>1356</td>\n",
       "      <td>ication task. re we consider cascading the sca...</td>\n",
       "      <td>, as a reference and construct a similar archi...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>sergey zagoruyko;nikos komodakis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16650</th>\n",
       "      <td>397</td>\n",
       "      <td>13604</td>\n",
       "      <td>7143</td>\n",
       "      <td>on CIFAR-10, all based on end-to-end learned C...</td>\n",
       "      <td>. Specifically we modify the WRN of 16 layers ...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>sergey zagoruyko;nikos komodakis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16651</th>\n",
       "      <td>185</td>\n",
       "      <td>2229</td>\n",
       "      <td>1423</td>\n",
       "      <td>on in both case. We utilize batch normalizatio...</td>\n",
       "      <td>. Table 3 reports the accuracy in the unsuperv...</td>\n",
       "      <td>1703.08961v1</td>\n",
       "      <td>sergey ioffe;christian szegedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16667</th>\n",
       "      <td>357</td>\n",
       "      <td>20485</td>\n",
       "      <td>21074</td>\n",
       "      <td>l concept is shown in Fig. 1.Recent studies ha...</td>\n",
       "      <td>, but none of these methods can be applied dir...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>sergey levine;peter pastor;alex krizhevsky;dei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>99</td>\n",
       "      <td>1391</td>\n",
       "      <td>22959</td>\n",
       "      <td>work decision.Algorithm 2 shows the learning t...</td>\n",
       "      <td>, we use multiple long short-term memory  laye...</td>\n",
       "      <td>1708.04033v1</td>\n",
       "      <td>volodymyr mnih;koray kavukcuoglu;david silver;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4783 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Quality  #1 ID  #2 ID  \\\n",
       "4           90  23427  10153   \n",
       "5          136  13225    377   \n",
       "6          120  15416   1612   \n",
       "7          120  12091  22025   \n",
       "8          130   9400  16385   \n",
       "...        ...    ...    ...   \n",
       "16649      397  11129   1356   \n",
       "16650      397  13604   7143   \n",
       "16651      185   2229   1423   \n",
       "16667      357  20485  21074   \n",
       "16668       99   1391  22959   \n",
       "\n",
       "                                               #1 String  \\\n",
       "4      s their usefulness for real-world tasks.As a f...   \n",
       "5      presented in Table 1. For comparison, we provi...   \n",
       "6      e word vectors in that sentence. The results a...   \n",
       "7      d recent attention owing to their utility in s...   \n",
       "8      enerative models  have recently achieved state...   \n",
       "...                                                  ...   \n",
       "16649  ication task. re we consider cascading the sca...   \n",
       "16650  on CIFAR-10, all based on end-to-end learned C...   \n",
       "16651  on in both case. We utilize batch normalizatio...   \n",
       "16667  l concept is shown in Fig. 1.Recent studies ha...   \n",
       "16668  work decision.Algorithm 2 shows the learning t...   \n",
       "\n",
       "                                               #2 String     target_id  \\\n",
       "4      model trained on the Google News dataset3.In a...  1705.10900v1   \n",
       "5      . We observe that by themselves, Ph embeddings...  1705.10900v1   \n",
       "6      . We observe that Ph embeddings perform poorly...  1705.10900v1   \n",
       "7      or purely word vector based methods  . .cument...  1705.10900v1   \n",
       "8      . Two most popular approaches of DGM rely on v...  1709.00845v1   \n",
       "...                                                  ...           ...   \n",
       "16649  , as a reference and construct a similar archi...  1703.08961v1   \n",
       "16650  . Specifically we modify the WRN of 16 layers ...  1703.08961v1   \n",
       "16651  . Table 3 reports the accuracy in the unsuperv...  1703.08961v1   \n",
       "16667  , but none of these methods can be applied dir...  1708.04033v1   \n",
       "16668  , we use multiple long short-term memory  laye...  1708.04033v1   \n",
       "\n",
       "                                           source_author  \n",
       "4      tomas mikolov;ilya sutskever;kai chen 0010;gre...  \n",
       "5                                               yoon kim  \n",
       "6                                quoc v le;tomas mikolov  \n",
       "7                                quoc v le;tomas mikolov  \n",
       "8      diederik p kingma;shakir mohamed;danilo jimene...  \n",
       "...                                                  ...  \n",
       "16649                   sergey zagoruyko;nikos komodakis  \n",
       "16650                   sergey zagoruyko;nikos komodakis  \n",
       "16651                     sergey ioffe;christian szegedy  \n",
       "16667  sergey levine;peter pastor;alex krizhevsky;dei...  \n",
       "16668  volodymyr mnih;koray kavukcuoglu;david silver;...  \n",
       "\n",
       "[4783 rows x 7 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quality</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "      <th>target_id</th>\n",
       "      <th>source_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139</td>\n",
       "      <td>8103</td>\n",
       "      <td>7582</td>\n",
       "      <td>le, recurrent neural networks  have made swift...</td>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>ilya sutskever;oriol vinyals;quoc v le</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174</td>\n",
       "      <td>818</td>\n",
       "      <td>21392</td>\n",
       "      <td>networks  have made swift inroads intomany str...</td>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>oriol vinyals;lukasz kaiser;terry koo;slav pet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>10444</td>\n",
       "      <td>2237</td>\n",
       "      <td>n-based copying can be seen as acombination of...</td>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>oriol vinyals;meire fortunato;navdeep jaitly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90</td>\n",
       "      <td>5440</td>\n",
       "      <td>14282</td>\n",
       "      <td>sing neural networks from raw text in a fully ...</td>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>1511.00360v1</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>102</td>\n",
       "      <td>7301</td>\n",
       "      <td>3444</td>\n",
       "      <td>by cila . , whoproposed it as a means of comp...</td>\n",
       "      <td>usedcompression to transfer knowledge from a d...</td>\n",
       "      <td>1511.06295v1</td>\n",
       "      <td>jimmy ba;rich caruana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16660</th>\n",
       "      <td>127</td>\n",
       "      <td>19965</td>\n",
       "      <td>15181</td>\n",
       "      <td>lgorithm  and which finds very interesting ins...</td>\n",
       "      <td>. Two important differenceswith the approach p...</td>\n",
       "      <td>1502.04156v1</td>\n",
       "      <td>j\\\"org bornschein;yoshua bengio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16661</th>\n",
       "      <td>80</td>\n",
       "      <td>1787</td>\n",
       "      <td>20657</td>\n",
       "      <td>about the same or better as was obtained for c...</td>\n",
       "      <td>and generativeadversarial networks ( ., 2014)...</td>\n",
       "      <td>1502.04156v1</td>\n",
       "      <td>yoshua bengio;eric laufer;guillaume alain;jaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16662</th>\n",
       "      <td>319</td>\n",
       "      <td>6892</td>\n",
       "      <td>723</td>\n",
       "      <td>ion of each expert annotator and the underlyin...</td>\n",
       "      <td>proposed a yesian EM framework for continuous-...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16663</th>\n",
       "      <td>319</td>\n",
       "      <td>20408</td>\n",
       "      <td>15313</td>\n",
       "      <td>reduce annotator inter- and intra-variability....</td>\n",
       "      <td>. as is defined as the inverse of accuracy: It...</td>\n",
       "      <td>1503.06619v1</td>\n",
       "      <td>changbo zhu;huan xu;shuicheng yan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16665</th>\n",
       "      <td>69</td>\n",
       "      <td>2953</td>\n",
       "      <td>4794</td>\n",
       "      <td>ell in the CKY chart.3In both approaches,the P...</td>\n",
       "      <td>on the POS sequences in PTB trainingset.The re...</td>\n",
       "      <td>1610.03946v1</td>\n",
       "      <td>tomas mikolov;kai chen;greg corrado;jeffrey dean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7447 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Quality  #1 ID  #2 ID  \\\n",
       "0          139   8103   7582   \n",
       "1          174    818  21392   \n",
       "2          232  10444   2237   \n",
       "3           90   5440  14282   \n",
       "13         102   7301   3444   \n",
       "...        ...    ...    ...   \n",
       "16660      127  19965  15181   \n",
       "16661       80   1787  20657   \n",
       "16662      319   6892    723   \n",
       "16663      319  20408  15313   \n",
       "16665       69   2953   4794   \n",
       "\n",
       "                                               #1 String  \\\n",
       "0      le, recurrent neural networks  have made swift...   \n",
       "1      networks  have made swift inroads intomany str...   \n",
       "2      n-based copying can be seen as acombination of...   \n",
       "3      sing neural networks from raw text in a fully ...   \n",
       "13      by cila . , whoproposed it as a means of comp...   \n",
       "...                                                  ...   \n",
       "16660  lgorithm  and which finds very interesting ins...   \n",
       "16661  about the same or better as was obtained for c...   \n",
       "16662  ion of each expert annotator and the underlyin...   \n",
       "16663  reduce annotator inter- and intra-variability....   \n",
       "16665  ell in the CKY chart.3In both approaches,the P...   \n",
       "\n",
       "                                               #2 String     target_id  \\\n",
       "0      andsyntactic parsing .Because RNNs make very f...  1606.03622v1   \n",
       "1      .Because RNNs make very few domain-specific as...  1606.03622v1   \n",
       "2      ; in a Pointer Network,the only way to generat...  1606.03622v1   \n",
       "3      . Recently, nsur .  have shown superior perfor...  1511.00360v1   \n",
       "13     usedcompression to transfer knowledge from a d...  1511.06295v1   \n",
       "...                                                  ...           ...   \n",
       "16660  . Two important differenceswith the approach p...  1502.04156v1   \n",
       "16661   and generativeadversarial networks ( ., 2014)...  1502.04156v1   \n",
       "16662  proposed a yesian EM framework for continuous-...  1503.06619v1   \n",
       "16663  . as is defined as the inverse of accuracy: It...  1503.06619v1   \n",
       "16665  on the POS sequences in PTB trainingset.The re...  1610.03946v1   \n",
       "\n",
       "                                           source_author  \n",
       "0                 ilya sutskever;oriol vinyals;quoc v le  \n",
       "1      oriol vinyals;lukasz kaiser;terry koo;slav pet...  \n",
       "2           oriol vinyals;meire fortunato;navdeep jaitly  \n",
       "3      tomas mikolov;ilya sutskever;kai chen 0010;gre...  \n",
       "13                                 jimmy ba;rich caruana  \n",
       "...                                                  ...  \n",
       "16660                    j\\\"org bornschein;yoshua bengio  \n",
       "16661  yoshua bengio;eric laufer;guillaume alain;jaso...  \n",
       "16662                  changbo zhu;huan xu;shuicheng yan  \n",
       "16663                  changbo zhu;huan xu;shuicheng yan  \n",
       "16665   tomas mikolov;kai chen;greg corrado;jeffrey dean  \n",
       "\n",
       "[7447 rows x 7 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df['#1 ID'] == 14050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_label_info = []\n",
    "for i in temp_df.groupby(['#1 ID', '#2 ID']):\n",
    "    instance_label = {}\n",
    "    instance_label['Quality'] = i[1]['Quality'].values\n",
    "    instance_label['index'] = i[1]['Quality'].index.values\n",
    "    multi_label_info.append(instance_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,len(multi_label_info)):\n",
    "    if multi_label_info[i]['index'].any() == 0:\n",
    "        print(multi_label_info[i])\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_label_info[0]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: HI \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenizer.encode_plus(\"Hi. How are you?\",\"Hi. I am fine and how are you?\", add_special_tokens = True, max_length = 100, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tokenizer.encode_plus(\"Hi. I am fine and how are you?\", None, add_special_tokens = True, max_length = 100, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 7632,\n",
       " 1012,\n",
       " 2129,\n",
       " 2024,\n",
       " 2017,\n",
       " 1029,\n",
       " 102,\n",
       " 7632,\n",
       " 1012,\n",
       " 1045,\n",
       " 2572,\n",
       " 2986,\n",
       " 1998,\n",
       " 2129,\n",
       " 2024,\n",
       " 2017,\n",
       " 1029,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS]', 101)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token, tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using pip version.Just like a watched pot never boils\n"
     ]
    }
   ],
   "source": [
    "context = \"You are using pip version.Just like a watched pot never boils\"\n",
    "print(\" \".join(context.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"andsyntactic parsing .Because RNNs make very few domain-specific assumptions,they have the potential to succeed at a wide variety of taskswith minimal feature engineering.wever, this flexibility also puts RNNs at a disadvantage compared tostandard semantic parsers, which can generalize naturallyby leveraging their built-in awareness of logical compositionality.In this paper, we introduce data recombination,a generic framework for declaratively injecting prior knowledgeinto a domain-general structured prediction model.In data recombination, prior knowledge about a taskis used to build a high-precision generative modelthat expands the empirical distributionby allowing fragments of different examples to be combined inparticular ways.mples from this generative model are then used to train a domain-general model.In the case of semantic parsing, we construct a generative modelby inducing a synchronous context-free grammar ,creating new examples such as those shown in Figure 1;our domain-general model is a sequence-to-sequence RNNwith a novel attention-based copying mechanism.Data recombination boosts the accuracyof our RNN model on three semantic parsing datasets. the o dataset, data recombinationimproves test accuracy by 4.3 percentage pointsover our baseline RNN, leading to newstate-of-the-art results for models that do not use aseed lexicon for predicates.ox: “what is the population of iowa ?”y: _answer  , _const    ATISx: “can you list all flights from chicago to milwaukee”y:  _lambda $0 e  _and  _flight $0  _from $0 chicago : _ci  _to $0 milwaukee : _ci   Overnightx: “when is the weekly standup”y:  call listValue  callgetProperty meeting.weekly_standup string start_time   We cast semantic parsing as a sequence-to-sequence task.The input utterance x is a sequence of words x1,...,xm∈Vin, the input vocabulary;similarly, the output logical form y isa sequence of tokens y1,...,yn∈Vout, the output vocabulary.A linear sequence of tokens might appear tolose the hierarchical structure of a logical form,but there is precedent for this choice:newcitevinyals2015grammarshowed that an RNN can reliably predict tree-structured outputsin a linear fashion.We evaluate our system on three existing semantic parsing datasets.Figure 2 shows sample input-output pairs from each of these datasets.oery o containsnatural language questions about US geographypaired with corresponding Prolog database queries.We use the standard split of 600 training examples and 280 test examplesintroduced by newcitezettlemoyer05ccg.We preprocess the logical forms to De Brujin index notationto standardize variable naming.ATIS ATIS containsnatural language queries for a flights databasepaired with corresponding database querieswritten in lambda calculus.We train on 4473 examples and evaluate on the 448test examples used bynewcitezettlemoyer07relaxed.Overnight Overnight containslogical forms paired with natural languageparaphrases across eight varied subdomains.newcitewang2015overnight constructed the datasetby generating all possible logicalforms up to some depth threshold,then getting multiple natural language paraphrasesfor each logical form from workers on Amazon Mechanical rk.We evaluate on the same train/test splits asnewcitewang2015overnight.In this paper, we only explore learning from logical forms.In the last few years, there has an emergence ofsemantic parsers learned from denotations.While our system cannot directly learn from denotations,it could be used to rerank candidate derivationsgenerated by one of these other systems.Our sequence-to-sequence RNN model is based on existingattention-based neural machine translation models,but also includes a novel attention-based copying mechanism.milar copying mechanisms have been exploredin parallel by newcitegu2016copying and newcitegulcehre2016pointing.Encoder. The encoder converts the input sequence x1,...,xminto a sequence of context-sensitive embeddingsb1,...,bm using a bidirectional RNN .First, a word embedding function φinmaps each word xi to a fixed-dimensional vector.These vectors are fed as input to two RNNs: a forward RNN and a backward RNN.The forward RNN starts with an initial hidden state hF0,and generates a sequence of hidden states hF1,...,hFm byrepeatedly applying the recurrenceThe recurrence takes the form of an LSTM .The backward RNN similarly generates hidden states hBm,...,hB1by processing the input sequence in reverse order.Finally, for each input position i, we definethe context-sensitive embeddingbi to be the concatenation of hFi and hDecoder. The decoder is an attention-based modelthat generates the output sequence y1,...,ynone token at a time. At each time step j,it writes yj based on thecurrent hidden state sj, then updates the hiddenstate to sj+1 based on sj and yj.Formally, the decoder is defined by the following equations:When not specified, i ranges over {1,...,m}and j ranges over {1,...,n}.Intuitively, the αji’s define a probabilitydistribution over the input words,describing what words in the input the decoder is focusing on at time j.They are computed from the unnormalizedattention scores eji.The matrices Ws, Wa, and U,as well as the embedding function φout, are parameters of the model.In the basic model of the previous section,the next output word yj is chosenvia a simple softmax over all words in the output vocabulary.wever, this model has difficulty generalizing to the long tail ofentity names commonly found in semantic parsing datasets.Conveniently, entity names in the input often corresponddirectly to tokens in the outpute.g., “iowa” becomes iowa in Figure 2.1To capture this intuition, we introducea new attention-based copying mechanism.At each time step j, the decodergenerates one of two types of actions.As before, it can write any word in the output vocabulary.In addition, it can copy any input word xi directly to the output,where the probability with which we copy xi is determined by theattention score on xi.Formally, we define a latent action ajthat is either Write for some w∈Voutor Copy for some i∈{1,...,m}.We then haveThe decoder chooses aj with a softmax over all these possible actions;yj is then a deterministic function of aj and x.ring training, we maximize the log-likelihood of y,marginalizing out a.Attention-based copying can be seen as acombination of a standard softmax output layer of an attention-based model and a Pointer Network ; in a Pointer Network,the only way to generate output is to copy a symbol from the input.Examples“what states border texas ?”,answerNV, stateV0, next_toV0, NV, constV0, stateidtexas“what is the highest mountain in ohio ?”,answerNV, highestV0, mountainV0, locV0, NV, constV0, stateidohioRules created by AbsEntitiesRoot →⟨ “what states border Id ?”,answerNV, stateV0, next_toV0, NV, constV0, stateidId ⟩Id →⟨ “texas”, texas ⟩Root →⟨ “what is the highest mountain in Id ?”,answerNV, highestV0, mountainV0, locV0, NV, constV0, stateidId ⟩Id →⟨“ohio”, ohio⟩Rules created by AbsWholePhrasesRoot →⟨ “what states border  ?”, answerNV, stateV0, next_toV0, NV,  ⟩ →⟨ “states border texas”, stateV0, next_toV0, NV, constV0, stateidtexas⟩Root →⟨ “what is the highest mountain in  ?”,answerNV, highestV0, mountainV0, locV0, NV,  ⟩Rules created by Concat-2Root →⟨textsct1 </s> textsct2,textsct1 </s> textsct2⟩t →⟨ “what states border texas ?”,answerNV, stateV0, next_toV0, NV, constV0, stateidtexas ⟩t →⟨ “what is the highest mountain in ohio ?”,answerNV, highestV0, mountainV0, locV0, NV, constV0, stateidohio⟩The main contribution of this paper is a novel data recombination frameworkthat injects important prior knowledge into our oblivious sequence-to-sequence RNN.In this framework, we induce a high-precisiongenerative model from the training data,then sample from it to generate new training examples.The process of inducing this generative modelcan leverage any available prior knowledge,which is transmitted through the generated examplesto the RNN model.A key advantage of our two-stage approach is that it allows us todeclare desired properties of the task which might be hard to capturein the model architecture.Our approach generalizes data augmentation,which is commonly employed to inject prior knowledge into a model.Data augmentation techniques focus on modelinginvariances—transformations liketranslating an image or adding noisethat alter the inputs x,but do not change the output y.These techniques have proven effective in areas likecomputer vision and speech recognition .In semantic parsing, however,we would like to capture more than just invariance properties.Consider an example with the utterance “what states border texas ?”.Given this example, it should be easy togeneralize to questions where “texas”is replaced by the name of any other state:simply replace the mention of Texas in the logical formwith the name of the new state.Underlying this phenomenon is a strong conditional independence principle:the meaning of the rest of the sentence is independent of thename of the state in question.Standard data augmentation is not sufficient to model such phenomena:instead of holding y fixed,we would like to apply simultaneous transformations to x and ysuch that the new x still maps to the new y.Data recombination addresses this need.In the general setting of data recombination,we start with a training set D of x,y pairs,which defines the empirical distribution ^px,y.We then fit a generative model ~px,y to ^pwhich generalizes beyond the support of ^p,for example by splicing together fragments of different examples.We refer to examples in the support of ~p as recombinant examples.Finally, to train our actual model pθy∣x,we maximize the expected value oflogpθy∣x, where x,y is drawn from ~p.For semantic parsing, we induce a synchronous context-free grammar to serve as the backbone of our generative model ~p.An SCFG consists of a set of production rulesX→⟨α,β⟩, where X is a category non-terminal,and α and β are sequences of terminal and non-terminal symbols.Any non-terminal symbols in α mustbe aligned to the same non-terminal symbol in β,and vice versa.Therefore, an SCFG defines a set of joint derivations ofaligned pairs of strings.In our case, we use an SCFG to represent joint derivationsof utterances x and logical forms y which for us is just a sequence of tokens.After we induce an SCFG G from D,the corresponding generative model ~px,yis the distribution over pairs x,ydefined by sampling from G,where we choose production rules to apply uniformly at random.It is instructive to compare our SCFG-based data recombination withWasp ,which uses an SCFG as the actual semantic parsing model.The grammar induced by Waspmust have good coverage in order to generalize to new inputsat test time.Wasp also requires the implementation of anefficient algorithm for computing the conditionalprobability py∣x.In contrast, our SCFG is only used to conveyprior knowledge about conditional independence structure,so it only needs to have high precision;our RNN model is responsible for boosting recallover the entire input space.We also only need to forward sample from the SCFG, which isconsiderably easier to implement than conditional inference.Below, we examine various strategies for inducinga grammar G from a dataset D.We first encode D as an initial grammarwith rules Root →⟨x,y⟩for each x,y∈D.Next, we will define each grammar induction strategyas a mapping from an input grammar Gin to a new grammar Gout.This formulation allows us to compose grammar induction strategiesSection 4.3.4.Our first grammar induction strategy, AbsEntities, simply abstracts entitieswith their types.We assume that each entity e e.g., texashas a corresponding type e.t e.g., state,which we infer based on the presence of certain predicates in the logical forme.g. stateid.For each grammar rule X→⟨α,β⟩ in Gin,where α contains a token e.g., “texas” thatstring matches an entity e.g., texas in β,we add two rules to Gout:i a rule where both occurrences are replaced with the type of the entitye.g., state,and ii a new rule that maps the type to the entity e.g.,textscId→⟨``{texas}'',texas⟩;we reserve the category name  for the next section.Thus, Gout generates recombinant examplesthat fuse most of one example with an entity found in a second example.A concrete example from the o domain is given inFigure 3.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFile[\"right_citated_text\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcn_data(train_df, test_df, save_dir):\n",
    "    with open('{}/gcn_pretrain.pkl'.format(save_dir), 'rb') as f:\n",
    "        embedding = pickle.load(f)\n",
    "        node2id = pickle.load(f)\n",
    "    gcn_train = np.array([[node2id[i]] for i in train_df['query_id'].values])\n",
    "    gcn_test = np.array([[node2id[i]] for i in test_df['query_id'].values])\n",
    "    return gcn_train, gcn_test, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = None\n",
    "node2id = None\n",
    "with open('/mnt/e/MS/2-1B/AP-FIR/bert-gcn/pre_train/gcn/PeerRead/PeerRead_gcn_pretrain_author.pkl', 'rb') as f:\n",
    "        embedding = pickle.load(f)\n",
    "        node2id = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quoc v le'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['source_author'].values[0].split(';')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = train_df['source_author'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teri maa ki  1111\n",
      "Teri maa ki  1112\n",
      "Teri maa ki  1264\n",
      "Teri maa ki  1265\n",
      "Teri maa ki  1266\n",
      "Teri maa ki  1267\n",
      "Teri maa ki  1358\n",
      "Teri maa ki  1359\n",
      "Teri maa ki  1360\n",
      "Teri maa ki  1361\n",
      "Teri maa ki  2168\n",
      "Teri maa ki  2169\n",
      "Teri maa ki  2698\n",
      "Teri maa ki  2840\n",
      "Teri maa ki  2850\n",
      "Teri maa ki  3943\n",
      "Teri maa ki  4738\n",
      "Teri maa ki  5316\n",
      "Teri maa ki  5317\n",
      "Teri maa ki  5318\n",
      "Teri maa ki  5430\n",
      "Teri maa ki  6831\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in arr:\n",
    "    cnt += 1\n",
    "    if i.split(';')[-1] == 'afroze ibrahim baqapuri':\n",
    "        print('Teri maa ki ', cnt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'afroze ibrahim baqapuri'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-b35a31d285c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnode2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'afroze ibrahim baqapuri'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'afroze ibrahim baqapuri'"
     ]
    }
   ],
   "source": [
    "node2id['afroze ibrahim baqapuri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.75661084e-02, -2.11767226e-01, -7.69936517e-02, -1.34367093e-01,\n",
       "       -1.32247269e-01,  9.25423503e-02, -1.23237289e-01, -2.50031084e-01,\n",
       "        1.83598384e-01, -1.87942892e-01,  1.05373263e-02, -3.16464901e-03,\n",
       "        2.89688170e-01,  1.16193756e-01,  3.08152884e-02, -1.64103135e-01,\n",
       "        3.85200605e-02,  1.93548799e-02,  1.04115993e-01,  1.59260869e-01,\n",
       "        3.75042222e-02, -4.13777083e-01, -1.40298188e-01, -7.97704756e-02,\n",
       "        1.69954151e-02,  9.29089710e-02,  5.71189970e-02, -9.63231102e-02,\n",
       "        2.62236208e-01,  2.40637809e-02,  1.57377809e-01, -1.58616498e-01,\n",
       "       -1.79514438e-01,  1.06614847e-02, -4.85762879e-02,  8.04453343e-02,\n",
       "       -7.72103742e-02, -1.38636418e-02, -5.83091676e-02,  3.78895551e-03,\n",
       "       -1.35671839e-01,  8.73243362e-02, -8.78408253e-02,  7.90649429e-02,\n",
       "        3.46747875e-01,  2.61593997e-01,  1.78148653e-02, -1.58512473e-01,\n",
       "        2.26705939e-01, -1.97524950e-01,  2.39470914e-01, -1.20129056e-01,\n",
       "        3.78371663e-02,  1.43525712e-02,  3.07499677e-01,  3.93205136e-02,\n",
       "        3.26521963e-01,  1.33485287e-01,  7.21615106e-02,  5.97223006e-02,\n",
       "       -1.21679194e-01, -1.88882463e-04,  8.32585096e-02,  2.37187371e-03,\n",
       "       -3.79623994e-02,  3.63164917e-02,  1.57881349e-01, -1.06942460e-01,\n",
       "        2.39275128e-01, -1.29011154e-01, -9.83724967e-02,  1.77602649e-01,\n",
       "        1.74250811e-01, -2.16639966e-01, -1.07145943e-02,  1.90985620e-01,\n",
       "       -8.94674510e-02, -1.25081211e-01, -7.06397817e-02, -8.83093253e-02,\n",
       "       -1.58514634e-01, -2.91686535e-01, -1.53919961e-02,  1.23052299e-03,\n",
       "       -8.97953957e-02, -1.59408286e-01,  4.09115627e-02,  5.01033664e-02,\n",
       "       -1.36052649e-02, -2.93312222e-02, -1.08291216e-01,  4.32716981e-02,\n",
       "        2.18032673e-02, -3.35435867e-02,  2.18881100e-01, -3.69883701e-02,\n",
       "       -1.82689846e-01, -2.95824502e-02,  2.63306294e-02,  7.04644844e-02,\n",
       "        2.15215653e-01,  8.89781713e-02, -1.25944898e-01, -1.45684928e-01,\n",
       "        1.77671283e-01,  3.18231195e-01, -3.87550116e-01,  1.38486981e-01,\n",
       "       -1.05099469e-01,  1.35701388e-01,  2.03291714e-01, -1.70869976e-01,\n",
       "        1.24309823e-01, -1.39292642e-01,  9.14501101e-02, -2.23273978e-01,\n",
       "        2.57606447e-01,  2.65174806e-01,  6.02744222e-02, -4.30236384e-02,\n",
       "       -1.11364447e-01,  5.96083365e-02, -8.72399881e-02,  1.57256201e-01,\n",
       "        1.98825896e-02,  5.03073931e-02, -3.17308068e-01,  5.25218621e-02,\n",
       "       -1.85600176e-01,  9.44966003e-02, -1.70663714e-01,  5.25478981e-02,\n",
       "        2.96930932e-02,  3.90870264e-03,  7.86724240e-02, -1.05689093e-03,\n",
       "       -6.82214051e-02,  8.38978961e-02, -3.35178711e-02,  2.84304589e-01,\n",
       "        2.36499161e-01,  1.47517145e-01,  8.66151601e-02, -1.81072459e-01,\n",
       "       -1.96698904e-02,  1.57579109e-01, -1.92496926e-03, -2.09096923e-01,\n",
       "       -1.58170134e-01,  5.24469987e-02, -1.35565549e-03,  1.33539885e-01,\n",
       "        1.78045243e-01, -1.06197000e-02, -4.36103493e-02,  3.63907933e-01,\n",
       "        3.28459620e-01, -1.43535599e-01,  1.16977558e-01,  1.54407874e-01,\n",
       "        2.84364820e-03, -3.21803659e-01, -1.41427889e-01, -1.41040727e-01,\n",
       "        1.16746262e-01, -7.78113678e-02, -3.88078317e-02, -2.05975980e-01,\n",
       "       -1.06396735e-01,  8.20268393e-02, -9.63085964e-02, -1.68748811e-01,\n",
       "        1.53609887e-01, -8.22425932e-02, -1.61294565e-02, -1.21433571e-01,\n",
       "       -1.49345025e-01, -2.24692188e-02,  2.30898291e-01,  9.56578739e-03,\n",
       "       -1.14168301e-01, -8.27038288e-03,  1.00167826e-01,  2.50400811e-01,\n",
       "       -7.10021630e-02, -5.84701374e-02, -9.67250019e-02, -1.63447440e-01,\n",
       "       -2.67274618e-01,  4.18512300e-02,  8.01496953e-02, -3.89249250e-02,\n",
       "       -1.09632500e-01,  1.33213878e-01, -1.98018625e-02,  1.20433904e-02,\n",
       "        1.79826140e-01,  2.21650973e-01,  5.44003583e-02, -1.59740448e-05,\n",
       "       -9.93075520e-02,  4.88293450e-03,  2.38708407e-03,  3.62814218e-01,\n",
       "        2.82487795e-02,  7.17111006e-02,  2.49472111e-02, -4.52988818e-02,\n",
       "       -3.81362140e-02, -5.15649468e-02,  2.57485330e-01, -9.33613256e-03,\n",
       "       -6.18604161e-02, -2.81135384e-02,  1.32598415e-01,  1.72094434e-01,\n",
       "        9.54440534e-02, -2.37655520e-01, -1.80550635e-01,  2.51775891e-01,\n",
       "       -4.10698615e-02, -1.87859088e-01, -1.35056712e-02, -2.08016485e-02,\n",
       "        2.80811697e-01, -1.64679930e-01,  2.17366546e-01,  1.58944771e-01,\n",
       "       -4.29793969e-02,  2.54787683e-01, -1.91771209e-01,  1.81889355e-01,\n",
       "        1.62545964e-01, -1.09660417e-01,  8.10352564e-02,  1.40040651e-01,\n",
       "        4.12505120e-03,  4.69538271e-02, -7.55614787e-02, -5.12436211e-01,\n",
       "        3.23276758e-01,  1.48093641e-01, -1.14473954e-01,  1.21638924e-02,\n",
       "       -1.14913434e-01,  2.81020492e-01,  6.87991604e-02, -2.10273132e-01,\n",
       "        2.14323848e-01, -2.72452205e-01,  6.00721240e-02, -5.16775064e-03,\n",
       "        2.71047294e-01,  2.09529966e-01,  1.19500108e-01, -2.06246078e-02,\n",
       "       -2.81132162e-01,  7.01486245e-02, -3.71465348e-02,  2.43571073e-01,\n",
       "        5.05249128e-02,  2.78368384e-01,  2.23756909e-01,  1.20734997e-01,\n",
       "        9.57269073e-02,  2.14078724e-02,  6.03707433e-02, -1.50995702e-01,\n",
       "        8.34118575e-02,  2.72654951e-01, -5.52794524e-02,  8.17072019e-02,\n",
       "       -2.57179439e-02,  1.21474534e-01,  3.06730419e-01, -1.83703601e-01,\n",
       "        1.31079406e-01,  2.59748459e-01, -1.71105772e-01,  1.12223893e-01,\n",
       "        1.75267428e-01, -1.28129721e-02,  1.92294538e-01,  5.61143309e-02,\n",
       "        4.48231325e-02,  2.87542194e-01,  1.11761123e-01,  9.83986109e-02,\n",
       "        2.07739323e-01, -1.20437980e-01,  1.17418468e-01,  2.90116742e-02,\n",
       "       -2.01227993e-01, -7.90366679e-02, -1.51033159e-02, -1.30126625e-02,\n",
       "       -9.51851085e-02,  1.25735700e-02, -2.08578393e-01,  4.85891476e-03,\n",
       "        8.88678581e-02,  2.06115097e-01,  4.28253375e-02,  4.29691859e-02,\n",
       "        2.26800963e-02,  2.23484248e-01,  1.05923101e-01, -5.47512621e-02,\n",
       "        2.77482904e-02, -9.09721851e-02,  2.01319069e-01,  7.00249895e-03,\n",
       "        1.93106294e-01,  4.41445373e-02, -1.37465615e-02, -2.96661705e-01,\n",
       "       -7.21250549e-02, -3.57294753e-02, -9.09278542e-02,  1.20054029e-01,\n",
       "        1.70667395e-01, -2.54131138e-01,  2.14243934e-01, -1.17644757e-01,\n",
       "        9.83435512e-02,  6.58015981e-02, -9.94444713e-02, -3.19661409e-01,\n",
       "        2.33328789e-01,  1.33826762e-01, -1.06701434e-01,  4.28169146e-02,\n",
       "        6.82872683e-02,  1.73110873e-01,  5.72999045e-02, -1.90228209e-01,\n",
       "       -1.47427678e-01,  2.27702737e-01,  3.44721600e-02, -5.07902578e-02,\n",
       "        7.02113509e-02, -1.72802687e-01, -9.85324234e-02,  3.26666459e-02,\n",
       "       -1.96010351e-01, -6.25237152e-02, -4.27084491e-02,  1.48135096e-01,\n",
       "       -2.82395899e-01,  1.12219669e-01, -9.71483141e-02,  1.08037397e-01,\n",
       "       -1.74854577e-01, -2.42618099e-02,  1.07621491e-01,  9.60434228e-02,\n",
       "        4.22468670e-02, -3.28735784e-02,  1.18553653e-01,  2.32916832e-01,\n",
       "        1.90050691e-01, -9.10579190e-02,  4.14975733e-01,  1.43490255e-01,\n",
       "        5.55760451e-02,  1.32838160e-01,  3.34881037e-01,  5.85687235e-02,\n",
       "        6.84086159e-02,  7.21669793e-02, -1.34318113e-01,  9.60158855e-02,\n",
       "       -1.59656391e-01, -6.69039786e-03, -7.15199411e-02, -1.07623562e-01,\n",
       "        1.28388375e-01, -1.67945713e-01,  3.35906208e-01,  1.98486745e-01,\n",
       "       -4.16123345e-02, -3.06370765e-01, -1.48574904e-01,  3.02260481e-02,\n",
       "       -7.15667605e-02,  1.02613159e-02, -2.19856814e-01,  1.21370010e-01,\n",
       "        2.09813535e-01, -1.49144068e-01, -2.27242149e-02, -9.19120014e-02,\n",
       "       -7.85933435e-02,  1.49443716e-01,  1.63351864e-01, -1.35934874e-01,\n",
       "       -1.27314702e-01, -1.66492537e-02, -7.07650930e-02, -2.32006237e-02,\n",
       "        1.56630874e-01, -1.38354942e-01,  1.09988973e-01, -8.46767649e-02,\n",
       "        2.33088240e-01, -2.66831875e-01,  1.18204169e-02,  1.00317568e-01,\n",
       "        7.27736205e-02, -2.43472397e-01,  1.15221672e-01,  1.30485862e-01,\n",
       "        1.80570439e-01,  8.97386856e-03,  5.50009869e-02,  5.39868735e-02,\n",
       "        1.56628177e-01,  1.84021026e-01,  9.18419361e-02,  2.07420737e-01,\n",
       "       -1.21091433e-01, -1.04261078e-01,  8.02391022e-02,  6.83686957e-02,\n",
       "        3.73614520e-01,  3.14177983e-02, -4.27574329e-02,  1.66034430e-01,\n",
       "        1.76892597e-02, -3.21981050e-02,  2.58408785e-02, -2.65410393e-02,\n",
       "        8.25869758e-03, -1.69332355e-01,  1.71659887e-01,  4.45934525e-03,\n",
       "       -3.14371511e-02, -5.14396466e-02, -1.17563799e-01, -6.15183190e-02,\n",
       "        9.24739242e-03,  4.38381843e-02, -1.71670377e-01, -6.54966459e-02,\n",
       "        1.89694911e-02, -1.13118708e-01, -1.68198377e-01, -7.04121590e-02,\n",
       "       -1.97749138e-02, -5.03671691e-02, -7.89410248e-03,  9.27044079e-03,\n",
       "       -1.23837747e-01,  1.49337232e-01, -1.34346187e-02, -4.64165173e-02,\n",
       "       -7.88813643e-03, -1.69540405e-01,  1.42077491e-01, -6.11776412e-02,\n",
       "       -3.19144815e-01,  2.58426294e-02, -1.78591400e-01, -2.39100158e-01,\n",
       "        6.44768327e-02,  2.07931176e-03, -4.72212769e-02, -3.38750303e-01,\n",
       "       -3.00750136e-04, -2.00204179e-02,  1.50527582e-01,  1.74028486e-01,\n",
       "       -3.14174712e-01, -3.18715461e-02,  2.35564619e-01,  8.39074627e-02,\n",
       "        1.25215784e-01,  9.07751173e-03,  1.37846142e-01,  1.86413303e-01,\n",
       "        1.10946596e-01,  4.42354605e-02, -1.68076336e-01, -6.87485486e-02,\n",
       "        1.69376642e-01,  3.30896676e-03, -2.31722221e-02, -7.85727799e-03,\n",
       "        1.52144386e-02,  7.43778199e-02, -3.22849602e-02,  1.03080317e-01,\n",
       "       -6.16932958e-02,  1.13012530e-02,  6.06167167e-02, -8.11293721e-03,\n",
       "       -1.51319169e-02, -3.40532660e-02, -2.31781863e-02, -1.44042656e-01,\n",
       "        1.76639855e-02, -2.34837145e-01,  5.16671427e-02, -3.83913703e-02,\n",
       "       -2.43441910e-02,  1.29225492e-01,  1.99231476e-01,  1.18984036e-01,\n",
       "        8.07112157e-02,  1.39053985e-01, -1.69619685e-04, -4.31291433e-03,\n",
       "       -1.62510380e-01,  8.47315788e-02,  2.22810134e-02, -9.79485139e-02,\n",
       "        2.25980237e-01, -1.89049676e-01,  1.21856987e-01, -1.45658189e-02,\n",
       "        5.72173446e-02,  1.22110918e-01, -3.43536027e-02,  2.31859237e-01,\n",
       "        3.58117148e-02, -4.16780338e-02,  2.19577476e-02, -1.33153200e-01,\n",
       "        1.55553222e-01,  6.58908337e-02,  1.78108886e-01,  4.52630334e-02,\n",
       "        1.06615096e-01,  3.19849774e-02,  3.79055858e-01,  3.68325599e-02,\n",
       "       -8.48008767e-02, -2.17905611e-01, -5.90476915e-02, -1.79626018e-01,\n",
       "       -3.06966364e-01, -4.69530523e-01, -2.45361984e-01,  2.30832234e-01,\n",
       "        1.02443472e-01,  1.54213667e-01,  3.66463028e-02, -2.70364881e-01,\n",
       "       -1.42036870e-01,  1.90208375e-01,  8.16864520e-02, -1.68174267e-01,\n",
       "       -1.18704155e-01,  7.30824247e-02, -1.08038515e-01, -1.45595461e-01,\n",
       "        1.35603249e-02,  8.77831578e-02, -2.90948063e-01, -4.36806977e-02,\n",
       "       -2.18599755e-02, -9.13985744e-02, -5.23272902e-02,  2.01560892e-02,\n",
       "       -5.90383112e-02, -1.54629424e-01, -7.28187710e-02,  3.56769562e-02,\n",
       "        4.71678227e-02, -1.65586919e-03,  7.41321817e-02, -3.38459045e-01,\n",
       "       -1.64768115e-01, -8.14933628e-02,  8.28793421e-02,  2.04840720e-01,\n",
       "        4.12821546e-02, -1.37941986e-02,  1.43288374e-01, -5.12973554e-02,\n",
       "        6.10718802e-02, -1.36533584e-02, -4.51214731e-01, -2.39280798e-02,\n",
       "        1.18384078e-01, -5.40161014e-01, -3.35807055e-01, -1.08482741e-01,\n",
       "       -6.57397509e-03,  8.17316771e-02, -9.08377394e-02,  9.48181897e-02,\n",
       "        3.49377543e-01, -1.25080690e-01, -2.96466947e-01,  7.89749324e-02,\n",
       "       -6.33802190e-02, -1.50898054e-01, -2.55509913e-01,  4.84517664e-02,\n",
       "        1.35174558e-01,  7.29764700e-02, -3.04877814e-02, -8.19886476e-02,\n",
       "       -1.70084462e-01,  1.94895193e-01,  2.32917480e-02,  7.96707049e-02,\n",
       "       -1.13909818e-01,  1.68204740e-01, -8.66214838e-03,  4.22080606e-03,\n",
       "        6.95541501e-03,  1.48016125e-01,  5.01022711e-02, -1.20396756e-01,\n",
       "        2.83337414e-01, -3.47635373e-02,  1.17587984e-01,  1.66253045e-01,\n",
       "       -8.45528916e-02,  3.55906785e-01, -2.99423970e-02,  4.71518934e-03,\n",
       "        2.27763653e-02,  1.52365386e-01, -4.28475849e-02, -7.15086088e-02,\n",
       "       -1.66595832e-01, -1.78473853e-02,  1.20108522e-01, -9.40418392e-02,\n",
       "        6.06597625e-02, -3.49631794e-02,  1.01526640e-02,  7.33784586e-02,\n",
       "        9.48322490e-02, -1.28458768e-01, -1.96678489e-01, -7.37394765e-02,\n",
       "       -1.53580993e-01, -5.91962188e-02, -1.78037360e-02,  2.01527268e-01,\n",
       "       -1.59936011e-01,  1.51845664e-02, -4.81478386e-02,  4.50078547e-02,\n",
       "        7.19841644e-02,  4.60779369e-02, -2.57471144e-01, -3.34817544e-02,\n",
       "       -3.13381314e-01,  5.87949604e-02,  7.55175203e-02,  1.17862493e-01,\n",
       "       -1.77163124e-01,  5.87680414e-02,  2.39789039e-02, -5.87395988e-02,\n",
       "        2.38105118e-01, -2.95949161e-01, -1.65956974e-01,  1.13459872e-02,\n",
       "       -9.43712741e-02, -2.59541497e-02, -3.91002744e-01,  9.69361514e-03,\n",
       "        4.58048247e-02,  8.27248245e-02, -1.60008237e-01, -1.59729600e-01,\n",
       "       -1.03350013e-01, -1.41877964e-01,  1.41779348e-01, -2.58843243e-01,\n",
       "        7.28308707e-02, -1.43969640e-01, -1.96585685e-01,  3.88807543e-02,\n",
       "        5.34303784e-02,  1.40571699e-01, -2.88314670e-02,  3.62033173e-02,\n",
       "       -1.43949851e-01,  1.16582915e-01,  3.25497389e-01, -7.86455423e-02,\n",
       "       -2.04120964e-01,  8.60311687e-02, -5.51437549e-02, -8.77598152e-02,\n",
       "        1.98504124e-02, -1.50903672e-01,  1.64051414e-01, -2.04029121e-02,\n",
       "        1.45213991e-01, -1.85651377e-01,  1.40388921e-01,  1.82447419e-01,\n",
       "       -3.02394062e-01,  1.02255300e-01,  1.59040838e-01, -7.98771977e-02,\n",
       "        9.97654274e-02,  1.10547230e-01, -1.68365628e-01, -1.83388889e-01,\n",
       "       -1.53088830e-02,  1.81984037e-01,  2.77941674e-01,  2.52358258e-01,\n",
       "       -2.49769241e-01,  1.99677587e-01,  2.17406541e-01, -3.97141650e-02,\n",
       "        1.67970523e-01, -3.47817317e-02, -1.90912649e-01,  3.32801700e-01,\n",
       "       -3.36467475e-03,  7.11534321e-02,  1.42974854e-01, -1.13277346e-01,\n",
       "        2.89390415e-01, -1.22241210e-02, -1.13263994e-01,  4.34265852e-01,\n",
       "        4.11671430e-01,  1.62717894e-01, -6.59641773e-02,  5.96385300e-02,\n",
       "        1.05811179e-01,  1.46182239e-01,  7.41886422e-02, -8.74630362e-02,\n",
       "       -8.36880654e-02, -1.28679082e-01,  2.60367811e-01,  1.08746782e-01,\n",
       "       -8.60203803e-02,  1.11941762e-01,  1.57412291e-02, -1.65263433e-02,\n",
       "        2.92397160e-02, -1.50352996e-02, -3.36176082e-02,  2.68480420e-01,\n",
       "       -9.78345275e-02,  2.07967963e-02,  1.27875507e-01, -4.08427790e-03,\n",
       "       -2.45908499e-02,  1.78359702e-01,  1.38917118e-02,  2.28043646e-05,\n",
       "        2.26909593e-02, -2.22606644e-01, -4.00838330e-02,  2.32521053e-02,\n",
       "       -2.22429603e-01, -3.04735936e-02, -1.33497760e-01,  2.69895270e-02,\n",
       "        1.00307256e-01, -7.80656189e-03,  1.66559637e-01,  7.01473653e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[3564]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_train = np.zeros(shape=(1,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.append(gcn_train, [embedding[node2id[arr[0].split(';')[-1]]]], axis=0)\n",
    "b = np.append(b, [embedding[node2id[arr[1].split(';')[-1]]]], axis=0)\n",
    "b = np.append(b, [embedding[node2id[arr[2].split(';')[-1]]]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'afroze ibrahim baqapuri'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-b35a31d285c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnode2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'afroze ibrahim baqapuri'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'afroze ibrahim baqapuri'"
     ]
    }
   ],
   "source": [
    "node2id['afroze ibrahim baqapuri'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = zip([1,2], [9,10], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2013, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetMatrix = np.zeros(shape=(10, 3))\n",
    "targets = np.array([0,2,2,1,0,2,1,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0]\n",
      "1 [2]\n",
      "2 [2]\n",
      "3 [1]\n",
      "4 [0]\n",
      "5 [2]\n",
      "6 [1]\n",
      "7 [1]\n",
      "8 [0]\n",
      "9 [0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,targets.shape[0]):\n",
    "    print(i, targets[[i]])\n",
    "    targetMatrix[i,targets[[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 2, 1, 0, 2, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ('MM', ['hi', 'bye'], 'MM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MM', ['hi', 'bye'], 'MM')"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 5)\n",
    "y = torch.randint(5, (10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6558, -0.6311, -1.1523, -0.4526, -0.1082],\n",
       "        [-0.0242,  0.2569,  1.0637, -1.3420,  0.7725],\n",
       "        [-1.3376,  0.0537, -0.2791, -1.9397,  0.3444],\n",
       "        [-0.7768, -0.5065,  0.4023,  0.4000,  1.2417],\n",
       "        [ 0.8871,  0.8222,  1.1341,  0.0854, -0.6681],\n",
       "        [ 1.4926, -1.0424, -0.1974, -1.4365,  2.7164],\n",
       "        [ 0.4590,  1.7561, -0.7182, -0.6271, -0.7176],\n",
       "        [-0.5132,  1.5448,  1.0711,  0.4586,  0.7220],\n",
       "        [-0.3532,  0.9306,  0.5386, -1.3265, -1.6723],\n",
       "        [ 1.5191,  0.5804,  0.7835,  1.4071,  0.4309]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 1, 2, 3, 0, 3, 4, 3, 1])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8631)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
